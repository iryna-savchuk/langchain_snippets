{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1790f12",
   "metadata": {},
   "source": [
    "# Using open-source 'GPT4All' model locally \n",
    "\n",
    "* [1. Backgorund](#background)\n",
    "* [2. Downloading and Converting the Model](#convert)\n",
    "* [3. Using the Local Model](#using)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281d7d6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"background\">\n",
    "    \n",
    "## 1. Background\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f028db0",
   "metadata": {},
   "source": [
    "There are several limitations that can restrict the ongoing research on Large Language Models (LLMs). \n",
    "\n",
    "First, the access to the weights and architecture of the trained models from GPT-family is usually restricted, and even if one does have access, it requires significant resources to perform any task. For example, though Facebook has released its LLaMA model weights under a non-commercial license, running this model on a local PC is practically impossible due to the large number of parameters (7 billion).\n",
    "\n",
    "Second, the available APIs to the pre-trained LLMs are usually not free to build on top of. \n",
    "\n",
    "The alternative open-source models (like `GPT4All` which is trained on top of Facebookâ€™s LLaMA model) aim to overcome these obstacles and make the LLMs more accessible to everyone. They can be loaded to a local PC and used to ask questions though prompts using the local computer's CPU. The authors of GPT4All incorporated several tricks to do efficient fine-tuning and inference. It is true that we are sacrificing quality by a small margin when using this approach. However, it is a trade-off between no access at all and accessing a slightly underpowered model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccda4b0",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"convert\">\n",
    "    \n",
    "## 2. Downloading  and Converting the Model\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d2ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df38a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = './models/gpt4all-lora-quantized-ggml.bin'\n",
    "Path(local_path).parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d973f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "514266it [12:01, 713.10it/s] \n"
     ]
    }
   ],
   "source": [
    "# Download the model from URL - \n",
    "# this process might take a while since the file size is 4GB\n",
    "\n",
    "url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'\n",
    "\n",
    "# Send a GET request to the URL to download the file\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# Open the file in binary mode and write the contents of the response in chunks\n",
    "with open(local_path, 'wb') as f:\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
    "        if chunk:\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aaf9a1",
   "metadata": {},
   "source": [
    "#### Transform the downloaded file to the latest format\n",
    "\n",
    "- Start by downloading the codes in the LLaMAcpp repository or simply fork it using the following command \n",
    "- Pass the downloaded file to the `convert.py` script and run it with a Python interpreter\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/ggerganov/llama.cpp.git\n",
    "cd llama.cpp && git checkout 2b26469\n",
    "python3 llama.cpp/convert.py ./models/gpt4all-lora-quantized-ggml.bin\n",
    "```\n",
    "\n",
    "Running the script \"convert.py\" will create a new file in the same directory as the original model with the following name: ggml-model-q4_0.bin. It basically is a converted version of the pre-trained model weights to **4-bit precision using the GGML format**. So, it uses fewer bits to represent the numbers and hence, reduces memory usage and allows faster inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc3cba",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"using\">\n",
    "    \n",
    "## 3. Using the Local Model\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47949e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LangChain library uses PyLLaMAcpp module to load the converted GPT4All weights\n",
    "# !pip install pyllamacpp\n",
    "# !pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242afc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "#from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72fec8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the prompt\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfdf75c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./models/ggml-model-q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[72777]: Class GGMLMetalClass is implemented in both /Users/iryna/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x169300208) and /Users/iryna/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x16954c208). One of the two will be used. Which one is undefined.\n",
      "llama.cpp: loading model from ./models/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1024.00 MB\n"
     ]
    }
   ],
   "source": [
    "callback_manager = AsyncCallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = GPT4All(model=\"./models/ggml-model-q4_0.bin\", \n",
    "              callback_manager=callback_manager, \n",
    "              verbose=True)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8bc8950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Earthquake occurs due to movement of tectonic plates, which are huge slabs that make up the outer layer of our planet. These plates move slowly over time and can cause sudden movements when they collide or slide past each other. The reason for this is not fully understood but it could be related to changes in temperature or pressure deep within the earth's crust, which causes stress on the tectonic plates."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Earthquake occurs due to movement of tectonic plates, which are huge slabs that make up the outer layer of our planet. These plates move slowly over time and can cause sudden movements when they collide or slide past each other. The reason for this is not fully understood but it could be related to changes in temperature or pressure deep within the earth's crust, which causes stress on the tectonic plates.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the reason for the earthquakes?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532f110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
