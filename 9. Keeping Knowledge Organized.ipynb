{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1790f12",
   "metadata": {},
   "source": [
    "# Keeping Knowledge Organized\n",
    "\n",
    "* [1. Utilizing Deep Lake](#deeplake)\n",
    "    * [1.1. Using Text Loaders and Text Splitters](#loaders_splitters) \n",
    "    * [1.2. Exploring DeepLake - adding and retrieving data](#deeplake_explore) \n",
    "    * [1.3. Question-Answering Example](#q_a) \n",
    "    * [1.4. Using Document Compressors](#compressors) \n",
    "* [2. Streamlined Data Ingestion](#ingestion)\n",
    "    * [2.1. TextLoader](#TextLoader)\n",
    "    * [2.2. PyPDFLoader](#PyPDFLoader) \n",
    "    * [2.3. SeleniumURLLoader](#SeleniumURLLoader) \n",
    "    * [2.4. GoogleDriveLoader](#GoogleDriveLoader) \n",
    "* [3. Text Splitters](#splitters)\n",
    "    * [3.1. Character Text Splitter](#Character_Splitter)\n",
    "    * [3.2. Recursive Character Text Splitter](#Recursive_Splitter) \n",
    "    * [3.3. NLTK Text Splitter](#NLTK_Splitter) \n",
    "    * [3.4. Spacy Text Splitter](#Spacy_Splitter) \n",
    "    * [3.5. Markdown Text Splitter](#Markdown_Splitter) \n",
    "    * [3.6. Token Text Splitter](#Token_Splitter) \n",
    "* [4. Embeddings](#embeddings)\n",
    "* [5. Customer Support Question Answering Chatbot](#cs)\n",
    "* [6. Gong.io Open-Source Alternative AI Sales Assistant](#gong_io)\n",
    "* [7. Creating Picture Books with OpenAI, Replicate, and Deep Lake](#picture_books)\n",
    "* [8. Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98da2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d6c3b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"deeplake\">\n",
    "    \n",
    "## 1. Utilizing Deep Lake\n",
    "    \n",
    "</a>\n",
    "\n",
    "In LangChain, a crucial role in structuring documents and fetching relevant data for LLMs belongs to **indexes and retrievers**. An `index` is a data structure that organizes and stores documents to enable efficient searching, while a `retriever` uses the index to find and return relevant documents in response to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ea52e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"loaders_splitters\">\n",
    "    \n",
    "### 1.1. Using Text Loaders and Text Splitters\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ae1e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ffae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of text, taken from https://www.theverge.com/2023/3/14/23639313/google-ai-language-model-palm-api-challenge-openai\n",
    "text = \"\"\"Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\n",
    "Google is offering developers access to one of its most advanced AI language models: PaLM.\n",
    "The search giant is launching an API for PaLM alongside a number of AI enterprise tools\n",
    "it says will help businesses “generate text, images, code, videos, audio, and more from\n",
    "simple natural language prompts.”\n",
    "\n",
    "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\n",
    "Meta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\n",
    "PaLM is a flexible system that can potentially carry out all sorts of text generation and\n",
    "editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\n",
    "example, or you could use it for tasks like summarizing text or even writing code.\n",
    "(It’s similar to features Google also announced today for its Workspace apps like Google\n",
    "Docs and Gmail.)\n",
    "\"\"\"\n",
    "\n",
    "# Write text to local file\n",
    "with open(\"output/my_file.txt\", \"w\") as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a34d5c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Use TextLoader to load text from the local file\n",
    "loader = TextLoader(\"output/my_file.txt\")\n",
    "docs_from_file = loader.load()\n",
    "\n",
    "print(len(docs_from_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9bfe85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses “generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.”\\n\\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summarizing text or even writing code.\\n(It’s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)\\n', metadata={'source': 'output/my_file.txt'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71dc35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 373, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Use CharacterTextSplitter to split the docs into texts\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20) # create a text splitter\n",
    "docs = text_splitter.split_documents(docs_from_file) # split documents into chunks\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae18793",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"deeplake_explore\">\n",
    "    \n",
    "### 1.2. Exploring DeepLake - adding and retrieving data\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8df8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an embedder model\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4dcb1",
   "metadata": {},
   "source": [
    "Let's explore Deep Lake and how it can be utilized to retrieve pertinent documents for contextual use. \n",
    "\n",
    "**Deep Lake** is a vector store that provides several advantages:\n",
    "\n",
    "- It’s **multimodal**, which means that it can be used to store items of diverse modalities (texts, images, audio, and video, along with their vector representations).\n",
    "- It’s **serverless**, which means that we can create and manage cloud datasets without the need to create and manage a database instance. \n",
    "- It’s possible to create a *streaming data loader* out of the data loaded into a Deep Lake dataset, which is convenient for fine-tuning machine learning models using common frameworks like PyTorch and TensorFlow.\n",
    "- Data can be **queried and visualized** from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cdd3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Activeloop key \n",
    "from keys import ACTIVELOOP_TOKEN\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN\n",
    "\n",
    "# Import DeepLake\n",
    "from langchain.vectorstores import DeepLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bfa1a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://iryna/langchain_course_retrievers already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "# Create DeepLake dataset\n",
    "my_activeloop_org_id = \"iryna\"\n",
    "my_activeloop_dataset_name = \"langchain_course_knowledge_organized\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9032c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add documents to the DeepLake dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/langchain/vectorstores/base.py:104\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    103\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/langchain/vectorstores/deeplake.py:219\u001b[0m, in \u001b[0;36mDeepLake.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`texts` parameter shouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/deeplake/core/vectorstore/deeplake_vectorstore.py:282\u001b[0m, in \u001b[0;36mVectorStore.add\u001b[0;34m(self, embedding_function, embedding_data, embedding_tensor, total_samples_processed, return_ids, num_workers, ingestion_batch_size, **tensors)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m id_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    280\u001b[0m utils\u001b[38;5;241m.\u001b[39mcheck_length_of_each_tensor(processed_tensors)\n\u001b[0;32m--> 282\u001b[0m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend_or_ingest_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mingestion_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mingestion_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mingestion_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_samples_processed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_samples_processed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcommit(allow_empty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/deeplake/core/vectorstore/vector_search/dataset/dataset.py:419\u001b[0m, in \u001b[0;36mextend_or_ingest_dataset\u001b[0;34m(processed_tensors, dataset, embedding_function, embedding_tensor, embedding_data, ingestion_batch_size, num_workers, total_samples_processed, logger)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding function returned empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    417\u001b[0m             processed_tensors[tensor] \u001b[38;5;241m=\u001b[39m embedded_data\n\u001b[0;32m--> 419\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     elements \u001b[38;5;241m=\u001b[39m create_elements(processed_tensors)\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/deeplake/core/dataset/dataset.py:3066\u001b[0m, in \u001b[0;36mDataset.extend\u001b[0;34m(self, samples, skip_ok, append_empty, ignore_errors, progressbar)\u001b[0m\n\u001b[1;32m   3060\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3061\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`ignore_errors` argument will be ignored while extending with numpy arrays or tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3062\u001b[0m         )\n\u001b[1;32m   3063\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_or_extend(\n\u001b[1;32m   3064\u001b[0m         samples, extend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, skip_ok\u001b[38;5;241m=\u001b[39mskip_ok, append_empty\u001b[38;5;241m=\u001b[39mappend_empty\n\u001b[1;32m   3065\u001b[0m     )\n\u001b[0;32m-> 3066\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   3067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m progressbar:\n\u001b[1;32m   3068\u001b[0m         indices \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(n))\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/deeplake/core/dataset/dataset.py:296\u001b[0m, in \u001b[0;36mDataset.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vc_info_updated:\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush_vc_info()\n\u001b[0;32m--> 296\u001b[0m     \u001b[43mspinner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mautoflush \u001b[38;5;241m=\u001b[39m autoflush\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/deeplake/util/spinner.py:143\u001b[0m, in \u001b[0;36mspinner.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     spinner \u001b[38;5;241m=\u001b[39m Spinner()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m run_spinner(spinner):\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/deeplake/core/storage/lru_cache.py:111\u001b[0m, in \u001b[0;36mLRUCache.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirty_keys\u001b[38;5;241m.\u001b[39mcopy():\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_storage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_storage\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/deeplake/core/storage/lru_cache.py:373\u001b[0m, in \u001b[0;36mLRUCache._forward\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward the value at a given path to the next storage, and un-marks its key.\"\"\"\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_storage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_storage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/deeplake/core/storage/lru_cache.py:386\u001b[0m, in \u001b[0;36mLRUCache._forward_value\u001b[0;34m(self, path, value)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirty_keys\u001b[38;5;241m.\u001b[39mpop(path, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DeepLakeMemoryObject):\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_storage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_storage[path] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/deeplake/core/storage/s3.py:176\u001b[0m, in \u001b[0;36mS3Provider.__setitem__\u001b[0;34m(self, path, content)\u001b[0m\n\u001b[1;32m    174\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(\u001b[38;5;28mmemoryview\u001b[39m(content))\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m botocore\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mClientError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m S3ResetReloadCredentialsManager(\u001b[38;5;28mself\u001b[39m, S3SetError):\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/deeplake/core/storage/s3.py:153\u001b[0m, in \u001b[0;36mS3Provider._set\u001b[0;34m(self, path, content)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, content):\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/octet-stream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# signifies binary data\u001b[39;49;00m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/botocore/client.py:943\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    942\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m--> 943\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{operation_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    949\u001b[0m         service_id\u001b[38;5;241m=\u001b[39mservice_id, operation_name\u001b[38;5;241m=\u001b[39moperation_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m    955\u001b[0m )\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/botocore/client.py:966\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    968\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m    969\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{operation_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    970\u001b[0m                 service_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    974\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m    975\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/botocore/endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m         operation_model,\n\u001b[1;32m    117\u001b[0m         request_dict,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/botocore/endpoint.py:199\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[1;32m    198\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_request(request_dict, operation_model)\n\u001b[0;32m--> 199\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[1;32m    203\u001b[0m     attempts,\n\u001b[1;32m    204\u001b[0m     operation_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m     exception,\n\u001b[1;32m    208\u001b[0m ):\n\u001b[1;32m    209\u001b[0m     attempts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/botocore/endpoint.py:241\u001b[0m, in \u001b[0;36mEndpoint._get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, operation_model, context):\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# This will return a tuple of (success_response, exception)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# and success_response is itself a tuple of\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# (http_response, parsed_dict).\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# If an exception occurs then the success_response is None.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# If no exception occurs then exception is None.\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     kwargs_to_emit \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_response\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: context,\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception\u001b[39m\u001b[38;5;124m'\u001b[39m: exception,\n\u001b[1;32m    249\u001b[0m     }\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/botocore/endpoint.py:281\u001b[0m, in \u001b[0;36mEndpoint._do_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    279\u001b[0m     http_response \u001b[38;5;241m=\u001b[39m first_non_none_response(responses)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m http_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m         http_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, e)\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/botocore/endpoint.py:377\u001b[0m, in \u001b[0;36mEndpoint._send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/botocore/httpsession.py:455\u001b[0m, in \u001b[0;36mURLLib3Session.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    452\u001b[0m     conn\u001b[38;5;241m.\u001b[39mproxy_headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m host\n\u001b[1;32m    454\u001b[0m request_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_target(request\u001b[38;5;241m.\u001b[39murl, proxy_url)\n\u001b[0;32m--> 455\u001b[0m urllib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m http_response \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mawsrequest\u001b[38;5;241m.\u001b[39mAWSResponse(\n\u001b[1;32m    468\u001b[0m     request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    469\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    470\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    471\u001b[0m     urllib_response,\n\u001b[1;32m    472\u001b[0m )\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m request\u001b[38;5;241m.\u001b[39mstream_output:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Cause the raw stream to be exhausted immediately. We do it\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# this way instead of using preload_content because\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# preload_content will never buffer chunked responses\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Add documents to the DeepLake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from db\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a20448",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"q_a\">\n",
    "    \n",
    "### 1.3. Question-Answering Example\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb9c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Create a retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(model=\"text-davinci-003\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29fb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How Google plans to challenge OpenAI?\"\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e4269",
   "metadata": {},
   "source": [
    "What happened under the hood in the question-answering example above is a similarity search. It was conducted using the embeddings to identify matching documents to be used as context for the LLM. Preselecting the most suitable documents based on semantic similarity enables us to provide the model with meaningful knowledge through the prompt while remaining within the allowed context size.\n",
    "\n",
    "Also, \"stuff chain\" was used to supply information to the LLM. In this technique, we \"stuff\" all the information into the LLM's prompt. \n",
    "\n",
    "**Note:** Stuffing is effective only with shorter documents because of context length limit that most LLMs have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecea0a9c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"compressors\">\n",
    "    \n",
    "### 1.4. Using Document Compressors\n",
    "    \n",
    "</a>\n",
    "\n",
    "Including unrelated information in the LLM prompt is detrimental, because it can divert the LLM's focus from important details and occupies valuable prompt space.\n",
    "\n",
    "To address this issue and improve the retrieval process, let's use a wrapper named `ContextualCompressionRetriever` that will wrap the base retriever with an `LLMChainExtractor`. The `LLMChainExtractor` iterates over the initially returned documents and extracts only the content relevant to the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7784388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how to use ContextualCompressionRetriever with LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Create GPT3 wrapper\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Create compressor for the retriever\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc909c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve compressed (relevant) documents \n",
    "retrieved_docs = compression_retriever.get_relevant_documents(\"How Google plans to challenge OpenAI?\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a06b13",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"ingestion\">\n",
    "    \n",
    "## 2. Streamlined Data Ingestion\n",
    "    \n",
    "</a>\n",
    "\n",
    "The LangChain library offers a variety of helpers designed to facilitate data loading and extraction from diverse sources: \n",
    "- TextLoader (handling plain text files);\n",
    "- PyPDFLoader (dealing with PDF files);\n",
    "- SeleniumURLLoaders (loading HTML documents from URLs that require JavaScript rendering);\n",
    "- GoogleDriveLoader (importing data from Google Drive docs or folders).\n",
    "\n",
    "Regardless of whether the information originates from a PDF file or website content, these classes streamline the process of handling different data formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb2f84",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"TextLoader\">\n",
    "    \n",
    "### 2.1. TextLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "file_path = 'data/my_file.txt'\n",
    "loader = TextLoader(file_path) # optional argument: encoding=\"ISO-8859-1\"\n",
    "documents = loader.load()\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e587185",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"PyPDFLoader\">\n",
    "    \n",
    "### 2.2. PyPDFLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f66fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF file has been taken here:\n",
    "# https://arxiv.org/pdf/2304.04858.pdf\n",
    "loader = PyPDFLoader(\"data/article.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ced444",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"SeleniumURLLoader\">\n",
    "    \n",
    "### 2.3. SeleniumURLLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff19cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q unstructured selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.boost.ai/blog/llms-large-language-models\",\n",
    "    \"https://www.youtube.com/watch?v=6Zv6A_9urh4&t=112s\"\n",
    "]\n",
    "\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "data = loader.load() # .load() returns the list of document instances containing 'page_content' and 'metadata'\n",
    "\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633931d0",
   "metadata": {},
   "source": [
    "The `SeleniumURLLoader` class has the following attributes:\n",
    "- `urls` (List[str]): List of URLs to load from;\n",
    "- `continue_on_failure` (bool, default=True): If set to True, continues loading other URLs on failure;\n",
    "- `browser` (str, default=\"chrome\"): Browser selection, either 'Chrome' or 'Firefox';\n",
    "- `executable_path` (Optional[str], default=None): Browser executable path;\n",
    "- `headless` (bool, default=True): Browser runs in headless mode if True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68865921",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"GoogleDriveLoader\">\n",
    "    \n",
    "### 2.4. GoogleDriveLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08155393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GoogleDriveLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a620efb",
   "metadata": {},
   "source": [
    "By default, the GoogleDriveLoader searches for the \"credentials.json\" file in \"~/.credentials/credentials.json\". Use the `credentials_file` keyword argument to modify this path.\n",
    "The \"token.json\" file follows the same principle and will be created automatically upon the loader's first use.\n",
    "\n",
    "Steps to set up the `credentials_file`:\n",
    "\n",
    "1. Create a new Google Cloud Platform project (or use an existing one) by visiting the Google Cloud Console. Ensure that billing is enabled for your project.\n",
    "2. Enable the Google Drive API by navigating to its dashboard in the Google Cloud Console and clicking \"Enable.\"\n",
    "3. Create a service account by going to the Service Accounts page in the Google Cloud Console. Follow the prompts to set up a new service account.\n",
    "4. Assign necessary roles to the service account, such as \"Google Drive API - Drive File Access\" and \"Google Drive API - Drive Metadata Read/Write Access,\" depending on your needs.\n",
    "5. After creating the service account, access the \"Actions\" menu next to it, select \"Manage keys,\" click \"Add Key,\" and choose \"JSON\" as the key type. This generates a JSON key file and downloads it to your computer, which serves as your credentials_file.\n",
    "\n",
    "To retrieve the folder or document ID from the URL:\n",
    "- Folder: https://drive.google.com/drive/u/0/folders/{folder_id}\n",
    "- Document: https://docs.google.com/document/d/{document_id}/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GoogleDriveLoader(\n",
    "    folder_id=\"the_folder_id\",\n",
    "    recursive=False  # default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2456367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45728d03",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"splitters\">\n",
    "    \n",
    "## 3. Text Splitters\n",
    "    \n",
    "</a>\n",
    "\n",
    "\n",
    "The length of the contents may vary depending on their source and may exceed the input window size of the model. Splitting the large text into smaller segments allows to use the most relevant chunk as the context instead of expecting the model to comprehend entire documents. \n",
    "\n",
    "Using a Text Splitter can also improve vector store search results: smaller segments might be more likely to match a query. That is why experimenting with different chunk sizes and overlaps can be beneficial.\n",
    "\n",
    "Two primary dimensions to consider:\n",
    "- the way the text is split\n",
    "- the way the chunk size (and overlap size) is measured\n",
    "\n",
    "No universal approach for chunking text will fit all scenarios - what's effective for one case might not be suitable for another. \n",
    "\n",
    "Main steps when using text splitters:\n",
    "\n",
    "1. Clean up data, get rid of anything unnecessary, like HTML tags.\n",
    "2. Divide the text into small, semantically meaningful chunks (often sentences).\n",
    "3. Combine these small chunks into a larger one until a specific size is reached (determined by a particular function).\n",
    "4. Once the desired size is attained, separate that chunk as an individual piece of text, then start forming a new chunk with some overlap to maintain context between segments.\n",
    "5. Run some queries to test the chosen chunk size.\n",
    "5. Repeat steps 2-5 to test a few different chunk sizes\n",
    "6. Compare results (the best size will depend on what kind of data and model are used). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d61cb",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"Character_Splitter\">\n",
    "    \n",
    "### 3.1. Character Text Splitter\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23729cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF file has been taken here: \n",
    "# https://assets.super.so/0d43acc6-3340-4118-9744-d03a54c41788/files/d3bbdb7c-5122-4066-800c-f44e043ecf43.pdf\n",
    "loader = PyPDFLoader(\"data/linux_manual.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "#print(texts[0])\n",
    "\n",
    "print (f\"You have {len(texts)} documents\")\n",
    "\n",
    "#print (\"Preview:\", texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19259776",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"Recursive_Splitter\">\n",
    "    \n",
    "### 3.2. Recursive Character Text Splitter\n",
    "    \n",
    "</a>\n",
    "\n",
    "The Recursive Splitter allows to split the text into chunks based on a list of characters provided. It attempts to split text using the characters from a list in that particular order until the resulting chunks are small enough. \n",
    "\n",
    "The default list of characters is **`[\"\\n\\n\", \"\\n\", \" \", \"]`**, meaning the splitter tries to keep paragraphs, sentences, and words together as long as possible, as they are generally the most semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e34792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"data/article.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb40b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the RecursiveCharacterTextSplitter class with the desired parameters,\n",
    "# the default list of characters to split by is [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3154bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([pages[0].page_content])\n",
    "\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "print(texts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a785cb1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"NLTK_Splitter\">\n",
    "    \n",
    "### 3.3. NLTK Text Splitter\n",
    "    \n",
    "</a>\n",
    "\n",
    "The NLTKTextSplitter in LangChain uses the Natural Language Toolkit (NLTK) library to split text based on tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71865316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500)\n",
    "texts = text_splitter.split_text(pages[0].page_content)\n",
    "\n",
    "\n",
    "# Print the length of the first chunk and the chunk\n",
    "print(len(texts[0]))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb21edd",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"Spacy_Splitter\">\n",
    "    \n",
    "### 3.4. Spacy Text Splitter\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f13df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import en_core_web_sm\n",
    "#en_core_web_sm.load(disable=[\"tagger\", \"ner\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db13e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "# Instantiate the SpacyTextSplitter with the desired chunk size\n",
    "text_splitter = SpacyTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "# Split the text using SpacyTextSplitter\n",
    "texts = text_splitter.split_text(pages[0].page_content)\n",
    "\n",
    "# Print the length of the first chunk and the chunk\n",
    "print(len(texts[0]))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8075c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"Markdown_Splitter\">\n",
    "    \n",
    "### 3.5. Markdown Text Splitter\n",
    "    \n",
    "</a>\n",
    "\n",
    "The `MarkdownTextSplitter` is used to split text written using Markdown languages like headers, code blocks, or dividers. It is implemented as a simple subclass of `RecursiveCharacterSplitter` with Markdown-specific separators and helps divide text while preserving the structure and meaning provided by Markdown formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3968e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# \n",
    "\n",
    "# Welcome to My Blog!\n",
    "\n",
    "## Introduction\n",
    "Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python, Java, and JavaScript.\n",
    "\n",
    "Here's a list of my favorite programming languages:\n",
    "\n",
    "1. Python\n",
    "2. JavaScript\n",
    "3. Java\n",
    "\n",
    "You can check out some of my projects on [GitHub](https://github.com).\n",
    "\n",
    "## About this Blog\n",
    "In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n",
    "\n",
    "Here's a small piece of Python code to say hello:\n",
    "\n",
    "\\``` python\n",
    "def say_hello(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "say_hello(\"John\")\n",
    "\\```\n",
    "\n",
    "Stay tuned for more updates!\n",
    "\n",
    "## Contact Me\n",
    "Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at johndoe@email.com.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8639f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = markdown_splitter.create_documents([markdown_text])\n",
    "\n",
    "print(docs[0])\n",
    "print(docs[1])\n",
    "print(docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72adba1c",
   "metadata": {},
   "source": [
    "**Note**: The default separators that are determined by the Markdown syntax can be customized when MarkdownTextSplitter instance is initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0f3ac",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"Token_Splitter\">\n",
    "    \n",
    "### 3.6. Token Text Splitter\n",
    "    \n",
    "</a>\n",
    "\n",
    "TokenTextSplitter respects the token boundaries, ensuring that the chunks do not split tokens in the middle. This type of splitter breaks down raw text strings into smaller pieces by initially converting the text into BPE (Byte Pair Encoding) tokens, and subsequently dividing these tokens into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3279a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Initialize the TokenTextSplitter with desired chunk size and overlap\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "\n",
    "# Split into smaller chunks\n",
    "texts = text_splitter.split_text(pages[0].page_content)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afce34e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"embeddings\">\n",
    "    \n",
    "## 4. Embeddings\n",
    "    \n",
    "</a>\n",
    "\n",
    "\n",
    "LLMs can transform textual data into embedding space, allowing for versatile representations across languages.  \n",
    "\n",
    "Embeddings are high-dimensional vectors that capture semantic information. Embeddings also serve to identify relevant information by quantifying the distance between data points (by indicating closer semantic meaning for points being closer together).\n",
    "\n",
    "The LangChain integration provides necessary functions for both transforming and calculating similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fee4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336958e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea10b3ca",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"cs\">\n",
    "    \n",
    "## 5. Customer Support Question Answering Chatbot\n",
    "    \n",
    "</a>\n",
    "\n",
    "Let's demonstrate how to use a website's content as supplementary context for a chatbot to respond to user queries effectively. \n",
    "\n",
    "The code implementation below involves:\n",
    "- employing data loaders, \n",
    "- storing the corresponding embeddings in the Deep Lake dataset, \n",
    "- and retrieving the most relevant documents corresponding to the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0dd70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668847e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f12e6c89",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"gong_io\">\n",
    "    \n",
    "## 6. Gong.io Open-Source Alternative AI Sales Assistant\n",
    "    \n",
    "</a>\n",
    "\n",
    "Let's explore how LangChain, Deep Lake, and GPT-4 can be used to develop a sales assistant able to give advice to salesman, taking into considerations internal guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503bfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3bb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b832b37",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"picture_books\">\n",
    "    \n",
    "## 7. Creating Picture Books with OpenAI, Replicate, and Deep Lake\n",
    "</a>\n",
    "\n",
    "Having as look at the use case of AI technology in the creative domain of children's picture book creation, using both OpenAI GPT-3.5 LLM for writing the story and Stable Diffusion for generating images for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de079a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ce6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fac692d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"resources\">\n",
    "    \n",
    "## 8. Additional Resources\n",
    "</a>\n",
    "\n",
    "- [Improving Document Retrieval with Contextual Compression](https://blog.langchain.dev/improving-document-retrieval-with-contextual-compression/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8dddd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
