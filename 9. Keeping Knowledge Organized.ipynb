{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1790f12",
   "metadata": {},
   "source": [
    "# Keeping Knowledge Organized\n",
    "\n",
    "* [1. Utilizing Deep Lake](#deeplake)\n",
    "    * [1.1. Using Text Loaders and Text Splitters](#loaders_splitters) \n",
    "    * [1.2. Exploring DeepLake - adding and retrieving data](#deeplake_explore) \n",
    "    * [1.3. Question-Answering Example](#q_a) \n",
    "    * [1.4. Using Document Compressors](#compressors) \n",
    "* [2. Streamlined Data Ingestion](#ingestion)\n",
    "* [3. Text Splitters](#splitters)\n",
    "* [4. Embeddings](#embeddings)\n",
    "* [5. Customer Support Question Answering Chatbot](#cs)\n",
    "* [6. Gong.io Open-Source Alternative AI Sales Assistant](#gong_io)\n",
    "* [7. Creating Picture Books with OpenAI, Replicate, and Deep Lake](#picture_books)\n",
    "* [8. Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98da2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d6c3b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"deeplake\">\n",
    "    \n",
    "## 1. Utilizing Deep Lake\n",
    "    \n",
    "</a>\n",
    "\n",
    "In LangChain, a crucial role in structuring documents and fetching relevant data for LLMs belongs to **indexes and retrievers**. An `index` is a data structure that organizes and stores documents to enable efficient searching, while a `retriever` uses the index to find and return relevant documents in response to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d9984",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"loaders_splitters\">\n",
    "    \n",
    "### 1.1. Using Text Loaders and Text Splitters\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed9c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7d6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of text,taken from https://www.theverge.com/2023/3/14/23639313/google-ai-language-model-palm-api-challenge-openai\n",
    "text = \"\"\"Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\n",
    "Google is offering developers access to one of its most advanced AI language models: PaLM.\n",
    "The search giant is launching an API for PaLM alongside a number of AI enterprise tools\n",
    "it says will help businesses “generate text, images, code, videos, audio, and more from\n",
    "simple natural language prompts.”\n",
    "\n",
    "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\n",
    "Meta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\n",
    "PaLM is a flexible system that can potentially carry out all sorts of text generation and\n",
    "editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\n",
    "example, or you could use it for tasks like summarizing text or even writing code.\n",
    "(It’s similar to features Google also announced today for its Workspace apps like Google\n",
    "Docs and Gmail.)\n",
    "\"\"\"\n",
    "\n",
    "# Write text to local file\n",
    "with open(\"output/my_file.txt\", \"w\") as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8a0093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Use TextLoader to load text from the local file\n",
    "loader = TextLoader(\"output/my_file.txt\")\n",
    "docs_from_file = loader.load()\n",
    "\n",
    "print(len(docs_from_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39610ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses “generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.”\\n\\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summarizing text or even writing code.\\n(It’s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)\\n', metadata={'source': 'output/my_file.txt'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71dc35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 373, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Use CharacterTextSplitter to split the docs into texts\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20) # create a text splitter\n",
    "docs = text_splitter.split_documents(docs_from_file) # split documents into chunks\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b50355",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"deeplake_explore\">\n",
    "    \n",
    "### 1.2. Exploring DeepLake - adding and retrieving data\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "128dd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an embedder model\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681ae18",
   "metadata": {},
   "source": [
    "Let's explore Deep Lake and how it can be utilized to retrieve pertinent documents for contextual use. \n",
    "\n",
    "**Deep Lake** is a vector store that provides several advantages:\n",
    "\n",
    "- It’s **multimodal**, which means that it can be used to store items of diverse modalities (texts, images, audio, and video, along with their vector representations).\n",
    "- It’s **serverless**, which means that we can create and manage cloud datasets without the need to create and manage a database instance. \n",
    "- It’s possible to create a *streaming data loader* out of the data loaded into a Deep Lake dataset, which is convenient for fine-tuning machine learning models using common frameworks like PyTorch and TensorFlow.\n",
    "- Data can be **queried and visualized** from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cecb929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Activeloop key \n",
    "from keys import ACTIVELOOP_TOKEN\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN\n",
    "\n",
    "# Import DeepLake\n",
    "from langchain.vectorstores import DeepLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "550384f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create DeepLake dataset\n",
    "my_activeloop_org_id = \"iryna\"\n",
    "my_activeloop_dataset_name = \"langchain_course_indexers_retrievers\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07dde95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://iryna/langchain_course_indexers_retrievers', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (2, 1536)  float32   None   \n",
      "    id        text      (2, 1)      str     None   \n",
      " metadata     json      (2, 1)      str     None   \n",
      "   text       text      (2, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['e7da2916-385d-11ee-9111-12ee7aa5dbdc',\n",
       " 'e7da2a10-385d-11ee-9111-12ee7aa5dbdc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add documents to the DeepLake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b596a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from db\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649a4fd",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"q_a\">\n",
    "    \n",
    "### 1.3. Question-Answering Example\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee55d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Create a retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(model=\"text-davinci-003\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9544aa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Google is offering access to its AI language model, PaLM, to developers. It is launching an API for PaLM which will help businesses generate text, images, code, videos, audio, and more from natural language prompts. PaLM is a large language model, similar to the GPT series created by OpenAI, which can be used for tasks like summarizing text or writing code.\n"
     ]
    }
   ],
   "source": [
    "query = \"How Google plans to challenge OpenAI?\"\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e1f5ce",
   "metadata": {},
   "source": [
    "What happened under the hood in the question-answering example above is a similarity search. It was conducted using the embeddings to identify matching documents to be used as context for the LLM. Preselecting the most suitable documents based on semantic similarity enables us to provide the model with meaningful knowledge through the prompt while remaining within the allowed context size.\n",
    "\n",
    "Also, \"stuff chain\" was used to supply information to the LLM. In this technique, we \"stuff\" all the information into the LLM's prompt. \n",
    "\n",
    "**Note:** Stuffing is effective only with shorter documents because of context length limit that most LLMs have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4fb709",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"compressors\">\n",
    "    \n",
    "### 1.4. Using Document Compressors\n",
    "    \n",
    "</a>\n",
    "\n",
    "Including unrelated information in the LLM prompt is detrimental, because it can divert the LLM's focus from important details and occupies valuable prompt space.\n",
    "\n",
    "To address this issue and improve the retrieval process, let's use a wrapper named `ContextualCompressionRetriever` that will wrap the base retriever with an `LLMChainExtractor`. The `LLMChainExtractor` iterates over the initially returned documents and extracts only the content relevant to the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbabad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how to use ContextualCompressionRetriever with LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Create GPT3 wrapper\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Create compressor for the retriever\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a84bda3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google is offering developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses “generate text, images, code, videos, audio, and more from simple natural language prompts.”\n"
     ]
    }
   ],
   "source": [
    "# Retrieve compressed (relevant) documents \n",
    "retrieved_docs = compression_retriever.get_relevant_documents(\"How Google plans to challenge OpenAI?\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a06b13",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"ingestion\">\n",
    "    \n",
    "## 2. Streamlined Data Ingestion\n",
    "    \n",
    "</a>\n",
    "\n",
    "The LangChain library offers a variety of helpers designed to facilitate data loading and extraction from diverse sources: \n",
    "- Text\n",
    "- PyPDF \n",
    "- Selenium URL Loaders\n",
    "- Google Drive Sync\n",
    "\n",
    "Regardless of whether the information originates from a PDF file or website content, these classes streamline the process of handling different data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72d06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f35daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45728d03",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"splitters\">\n",
    "    \n",
    "## 3. Text Splitters\n",
    "    \n",
    "</a>\n",
    "\n",
    "\n",
    "The length of the contents may vary depending on their source and may exceed the input window size of the model. Splitting the large text into smaller segments allows to use the most relevant chunk as the context instead of expecting the model to comprehend the textual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ed626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40e0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7afce34e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"embeddings\">\n",
    "    \n",
    "## 4. Embeddings\n",
    "    \n",
    "</a>\n",
    "\n",
    "\n",
    "LLMs can transform textual data into embedding space, allowing for versatile representations across languages.  \n",
    "\n",
    "Embeddings are high-dimensional vectors that capture semantic information. Embeddings also serve to identify relevant information by quantifying the distance between data points (by indicating closer semantic meaning for points being closer together).\n",
    "\n",
    "The LangChain integration provides necessary functions for both transforming and calculating similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fee4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336958e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea10b3ca",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"cs\">\n",
    "    \n",
    "## 5. Customer Support Question Answering Chatbot\n",
    "    \n",
    "</a>\n",
    "\n",
    "Let's demonstrate how to use a website's content as supplementary context for a chatbot to respond to user queries effectively. \n",
    "\n",
    "The code implementation below involves:\n",
    "- employing data loaders, \n",
    "- storing the corresponding embeddings in the Deep Lake dataset, \n",
    "- and retrieving the most relevant documents corresponding to the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0dd70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668847e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f12e6c89",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"gong_io\">\n",
    "    \n",
    "## 6. Gong.io Open-Source Alternative AI Sales Assistant\n",
    "    \n",
    "</a>\n",
    "\n",
    "Let's explore how LangChain, Deep Lake, and GPT-4 can be used to develop a sales assistant able to give advice to salesman, taking into considerations internal guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503bfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3bb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b832b37",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"picture_books\">\n",
    "    \n",
    "## 7. Creating Picture Books with OpenAI, Replicate, and Deep Lake\n",
    "</a>\n",
    "\n",
    "Having as look at the use case of AI technology in the creative domain of children's picture book creation, using both OpenAI GPT-3.5 LLM for writing the story and Stable Diffusion for generating images for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de079a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ce6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2779642",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"resources\">\n",
    "    \n",
    "## 8. Additional Resources\n",
    "</a>\n",
    "\n",
    "- [Improving Document Retrieval with Contextual Compression](https://blog.langchain.dev/improving-document-retrieval-with-contextual-compression/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d94880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
