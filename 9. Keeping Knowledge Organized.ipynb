{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1790f12",
   "metadata": {},
   "source": [
    "# Keeping Knowledge Organized\n",
    "\n",
    "* [1. Utilizing Deep Lake](#deeplake)\n",
    "    * [1.1. Using Text Loaders and Text Splitters](#loaders_splitters) \n",
    "    * [1.2. Exploring DeepLake - adding and retrieving data](#deeplake_explore) \n",
    "    * [1.3. Question-Answering Example](#q_a) \n",
    "    * [1.4. Using Document Compressors](#compressors) \n",
    "* [2. Streamlined Data Ingestion](#ingestion)\n",
    "    * [2.1. TextLoader](#TextLoader)\n",
    "    * [2.2. PyPDFLoader](#PyPDFLoader) \n",
    "    * [2.3. SeleniumURLLoader](#SeleniumURLLoader) \n",
    "    * [2.4. GoogleDriveLoader](#GoogleDriveLoader) \n",
    "* [3. Text Splitters](#splitters)\n",
    "* [4. Embeddings](#embeddings)\n",
    "* [5. Customer Support Question Answering Chatbot](#cs)\n",
    "* [6. Gong.io Open-Source Alternative AI Sales Assistant](#gong_io)\n",
    "* [7. Creating Picture Books with OpenAI, Replicate, and Deep Lake](#picture_books)\n",
    "* [8. Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98da2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d6c3b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"deeplake\">\n",
    "    \n",
    "## 1. Utilizing Deep Lake\n",
    "    \n",
    "</a>\n",
    "\n",
    "In LangChain, a crucial role in structuring documents and fetching relevant data for LLMs belongs to **indexes and retrievers**. An `index` is a data structure that organizes and stores documents to enable efficient searching, while a `retriever` uses the index to find and return relevant documents in response to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425b249",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"loaders_splitters\">\n",
    "    \n",
    "### 1.1. Using Text Loaders and Text Splitters\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59fe2e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0ac9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of text, taken from https://www.theverge.com/2023/3/14/23639313/google-ai-language-model-palm-api-challenge-openai\n",
    "text = \"\"\"Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\n",
    "Google is offering developers access to one of its most advanced AI language models: PaLM.\n",
    "The search giant is launching an API for PaLM alongside a number of AI enterprise tools\n",
    "it says will help businesses “generate text, images, code, videos, audio, and more from\n",
    "simple natural language prompts.”\n",
    "\n",
    "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\n",
    "Meta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\n",
    "PaLM is a flexible system that can potentially carry out all sorts of text generation and\n",
    "editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\n",
    "example, or you could use it for tasks like summarizing text or even writing code.\n",
    "(It’s similar to features Google also announced today for its Workspace apps like Google\n",
    "Docs and Gmail.)\n",
    "\"\"\"\n",
    "\n",
    "# Write text to local file\n",
    "with open(\"output/my_file.txt\", \"w\") as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792cb1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Use TextLoader to load text from the local file\n",
    "loader = TextLoader(\"output/my_file.txt\")\n",
    "docs_from_file = loader.load()\n",
    "\n",
    "print(len(docs_from_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d74640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses “generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.”\\n\\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summarizing text or even writing code.\\n(It’s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)\\n', metadata={'source': 'output/my_file.txt'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71dc35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 373, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Use CharacterTextSplitter to split the docs into texts\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20) # create a text splitter\n",
    "docs = text_splitter.split_documents(docs_from_file) # split documents into chunks\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e86a5e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"deeplake_explore\">\n",
    "    \n",
    "### 1.2. Exploring DeepLake - adding and retrieving data\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d857b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an embedder model\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98701e47",
   "metadata": {},
   "source": [
    "Let's explore Deep Lake and how it can be utilized to retrieve pertinent documents for contextual use. \n",
    "\n",
    "**Deep Lake** is a vector store that provides several advantages:\n",
    "\n",
    "- It’s **multimodal**, which means that it can be used to store items of diverse modalities (texts, images, audio, and video, along with their vector representations).\n",
    "- It’s **serverless**, which means that we can create and manage cloud datasets without the need to create and manage a database instance. \n",
    "- It’s possible to create a *streaming data loader* out of the data loaded into a Deep Lake dataset, which is convenient for fine-tuning machine learning models using common frameworks like PyTorch and TensorFlow.\n",
    "- Data can be **queried and visualized** from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ce6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Activeloop key \n",
    "from keys import ACTIVELOOP_TOKEN\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN\n",
    "\n",
    "# Import DeepLake\n",
    "from langchain.vectorstores import DeepLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c8601fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://iryna/langchain_course_indexers_retrievers already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "# Create DeepLake dataset\n",
    "my_activeloop_org_id = \"iryna\"\n",
    "my_activeloop_dataset_name = \"langchain_course_indexers_retrievers\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9e06825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://iryna/langchain_course_indexers_retrievers', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['15cdcbe2-3882-11ee-a6a7-12ee7aa5dbdc',\n",
       " '15cdccd2-3882-11ee-a6a7-12ee7aa5dbdc']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add documents to the DeepLake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1077f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from db\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b80c30",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"q_a\">\n",
    "    \n",
    "### 1.3. Question-Answering Example\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3994adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Create a retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(model=\"text-davinci-003\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a16f211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Google plans to challenge OpenAI by offering developers access to their most advanced AI language model, PaLM, and launching an API for PaLM alongside a number of AI enterprise tools. PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or Meta's LLaMA family of models.\n"
     ]
    }
   ],
   "source": [
    "query = \"How Google plans to challenge OpenAI?\"\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045de01",
   "metadata": {},
   "source": [
    "What happened under the hood in the question-answering example above is a similarity search. It was conducted using the embeddings to identify matching documents to be used as context for the LLM. Preselecting the most suitable documents based on semantic similarity enables us to provide the model with meaningful knowledge through the prompt while remaining within the allowed context size.\n",
    "\n",
    "Also, \"stuff chain\" was used to supply information to the LLM. In this technique, we \"stuff\" all the information into the LLM's prompt. \n",
    "\n",
    "**Note:** Stuffing is effective only with shorter documents because of context length limit that most LLMs have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a24fb5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"compressors\">\n",
    "    \n",
    "### 1.4. Using Document Compressors\n",
    "    \n",
    "</a>\n",
    "\n",
    "Including unrelated information in the LLM prompt is detrimental, because it can divert the LLM's focus from important details and occupies valuable prompt space.\n",
    "\n",
    "To address this issue and improve the retrieval process, let's use a wrapper named `ContextualCompressionRetriever` that will wrap the base retriever with an `LLMChainExtractor`. The `LLMChainExtractor` iterates over the initially returned documents and extracts only the content relevant to the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1eacdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how to use ContextualCompressionRetriever with LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Create GPT3 wrapper\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Create compressor for the retriever\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3111004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iryna/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/langchain/chains/llm.py:279: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google is offering developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses “generate text, images, code, videos, audio, and more from simple natural language prompts.”\n"
     ]
    }
   ],
   "source": [
    "# Retrieve compressed (relevant) documents \n",
    "retrieved_docs = compression_retriever.get_relevant_documents(\"How Google plans to challenge OpenAI?\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a06b13",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"ingestion\">\n",
    "    \n",
    "## 2. Streamlined Data Ingestion\n",
    "    \n",
    "</a>\n",
    "\n",
    "The LangChain library offers a variety of helpers designed to facilitate data loading and extraction from diverse sources: \n",
    "- TextLoader (handling plain text files);\n",
    "- PyPDFLoader (dealing with PDF files);\n",
    "- SeleniumURLLoaders (loading HTML documents from URLs that require JavaScript rendering);\n",
    "- GoogleDriveLoader (importing data from Google Drive docs or folders).\n",
    "\n",
    "Regardless of whether the information originates from a PDF file or website content, these classes streamline the process of handling different data formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc607ee",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"TextLoader\">\n",
    "    \n",
    "### 2.1. TextLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea72d06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses “generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.”\\n\\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summarizing text or even writing code.\\n(It’s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)\\n', metadata={'source': 'data/my_file.txt'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "file_path = 'data/my_file.txt'\n",
    "loader = TextLoader(file_path) # optional argument: encoding=\"ISO-8859-1\"\n",
    "documents = loader.load()\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49e05b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"PyPDFLoader\">\n",
    "    \n",
    "### 2.2. PyPDFLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43663cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af7ce932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Simulated Annealing in Early Layers Leads to Better Generalization\\nAmir M. Sarfi1,2Zahra Karimpour1Muawiz Chaudhary1,2Nasir M. Khalid1,2\\nMirco Ravanelli1,2Sudhir Mudur1Eugene Belilovsky1,2\\n1Concordia University2Mila – Quebec AI Institute\\nAbstract\\nRecently, a number of iterative learning methods have\\nbeen introduced to improve generalization. These typically\\nrely on training for longer periods of time in exchange for\\nimproved generalization. LLF (later-layer-forgetting) is a\\nstate-of-the-art method in this category. It strengthens learn-\\ning in early layers by periodically re-initializing the last\\nfew layers of the network. Our principal innovation in this\\nwork is to use Simulated annealing in EArly Layers (SEAL)\\nof the network in place of re-initialization of later layers.\\nEssentially, later layers go through the normal gradient de-\\nscent process, while the early layers go through short stints\\nof gradient ascent followed by gradient descent. Extensive\\nexperiments on the popular Tiny-ImageNet dataset bench-\\nmark and a series of transfer learning and few-shot learning\\ntasks show that we outperform LLF by a significant margin.\\nWe further show that, compared to normal training, LLF\\nfeatures, although improving on the target task, degrade\\nthe transfer learning performance across all datasets we ex-\\nplored. In comparison, our method outperforms LLF across\\nthe same target datasets by a large margin. We also show\\nthat the prediction depth of our method is significantly lower\\nthan that of LLF and normal training, indicating on average\\nbetter prediction performance.1\\n1. Introduction\\nOverfitting is a crucial challenge in supervised deep\\nlearning, which prevents a neural network from performing\\nwell on unseen data. This problem is particularly perva-\\nsive in smaller dataset regimes. Classical machine learn-\\ning approaches to address this problem typically rely on\\nexplicit regularization added as part of the optimization ob-\\nThis research was partially funded by NSERC Discovery\\nGrant RGPIN2021-04104 and RGPIN-2019-05729. We ac-\\nknowledge the resources provided by Compute Canada and Cal-\\ncul Quebec. Correspondence to: a.m.sarfi@gmail.com,\\neugene.belilovsky@concordia.ca,\\nsudhir.mudur@concordia.ca\\n1The code to reproduce our results is publicly available at:\\nhttps://github.com/amiiir-sarfi/SEALjective [39]. A number of implicit approaches are often\\napplied in training deep networks. These implicit methods\\nare usually easier to design than explicit terms added to the\\nobjective function. For example, early stopping [44] and\\ndropout [34] are classical examples of implicit regulariza-\\ntion methods. It can be shown that these techniques can be\\nshown to be linked directly to explicit regularization terms.\\nConsider the case of linear regression, early stopping can be\\ndirectly seen as Tikhonov regularization [44]. Indeed the use\\nof modified optimization strategies to improve generalization\\nis becoming pervasive in deep learning [7].\\nRecently, iterative training methods have been introduced\\nto improve generalization in deep networks. These allow\\nneural networks to be trained for many epochs while pro-\\ngressively improving generalization [9, 10,37, 43,48]. These\\nworks typically rely on a notion of a generation , in which\\na model is optimized towards a local minimum in a single\\ngeneration. Subsequently, in a new generation, the objective\\nfunction is modified, or the network is perturbed towards\\na high loss, requiring a new generation of optimization to-\\nwards a minimum. The earliest work on this topic focused\\non self-distillation [10, 43], where, in each successive gen-\\neration, a student network with the same architecture was\\ninitialized and trained to mimic the softmax output distribu-\\ntion of the previous model. This procedure was theoretically\\nstudied by [23] and provided the formal link to its explicit\\nregularization effects.\\nA number of other techniques were subsequently pro-\\nposed. These have been characterized by Zhou et al. [48] as' metadata={'source': 'data/article.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"data/article.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9311031b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"SeleniumURLLoader\">\n",
    "    \n",
    "### 2.3. SeleniumURLLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e90d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q unstructured selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e7bf60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"With your permission, we use cookies to personalize content and ads, provide social media features and analyze our traffic. Learn more about our cookies policy.\\n\\nBy clicking ‘Accept’, you give your consent to the aforementioned and accept that we share this information with third parties. If you do not give us your consent, we will continue to use only essential cookies to enable core functionality of the website.\\n\\nAccept\\n\\nDecline\\n\\nProduct\\n\\nConversational AI\\n\\nChat Automation\\n\\nVoice Call Automation\\n\\nIntegrations\\n\\nLarge Language Models\\n\\nSolutions\\n\\nCustomer self-service\\n\\nInternal virtual agent\\n\\nFinancial Services\\n\\ninsurance\\n\\ntelecom\\n\\nPublic Sector\\n\\nResources\\n\\nCase studies\\n\\nWebinars\\n\\nguides\\n\\nblog\\n\\nAnnouncements\\n\\nAcademy\\n\\nPartners\\n\\nCompany\\n\\nAbout us\\n\\nCareers\\n\\nSuppliers\\n\\nSecurity\\n\\nAccessibility\\n\\nPrivacy Policy\\n\\nCookies Policy\\n\\nContact\\n\\nSecurity\\n\\nAbout\\n\\nCareer\\n\\nSuppliers\\n\\nProduct\\n\\nConversational AI\\n\\nChat Automation\\n\\nVoice Call Automation\\n\\nIntegrations\\n\\nLarge Language Models\\n\\nSolutions\\n\\nCustomer self-service\\n\\nInternal virtual agent\\n\\nFinancial Services\\n\\ninsurance\\n\\ntelecom\\n\\nPublic Sector\\n\\nResources\\n\\nCase studies\\n\\nWebinars\\n\\nguides\\n\\nblog\\n\\nAnnouncements\\n\\nAcademy\\n\\nPartner\\n\\nSecurity\\n\\nAbout us\\n\\nCareer\\n\\nContact\\n\\nWhat are large language models and how do they work?\\n\\nMike Priest\\n\\nUpdated on\\n\\nJuly 19, 2023\\n\\nArtificial intelligence\\n\\nLarge language models, or LLMs, are a type of AI that can mimic human intelligence. They use statistical models to analyze vast amounts of data, learning the patterns and connections between words and phrases. This allows them to generate new content, such as essays or articles, that are similar in style to a specific author or genre.\\n\\nAs technology advances, we are constantly discovering new ways to push the boundaries of what we thought was possible. Large language models are just one example of how we are utilizing technology to create more intelligent and sophisticated software. In this blog, we will take a closer look at what LLMs are, and how they work.\\n\\nTo understand how large language models work, it's helpful to first look at how they are trained. Training a large language model involves feeding it large amounts of data, such as books, articles, or web pages, so that it can learn the patterns and connections between words. The more data it is trained on, the better it will be at generating new content.\\n\\nOnce the large language model has been trained, it can be used to generate new content based on the parameters set by the user. For example, if you wanted to generate a new article in the style of Shakespeare, you would provide the Large language model with a prompt, such as a sentence or paragraph, and it would generate the rest of the article based on the patterns and connections it has learned from analyzing Shakespeare's works.\\n\\nLarge language models can also be used for a wide range of other applications, such as chatbots and virtual agents. By analyzing natural language patterns, they can generate responses that are similar to how a human might respond. This can be incredibly useful for companies looking to provide customer service through a chatbot or virtual agent, as it allows them to provide personalized responses without requiring a human to be present.\\n\\nOf course, like any technology, large language models have their limitations. One of the biggest challenges is ensuring that the content they generate is accurate and reliable. While LLMs can generate content that is similar in style to a particular author or genre, they can also generate content that is inaccurate or misleading. This is particularly true when it comes to generating news articles or other types of content that require a high degree of accuracy.\\n\\nOne way of mitigating this flaw in LLMs is to use conversational AI to connect the model to a reliable data source, such as a company’s website. This makes it possible to harness an large language model’s generative properties to create a host of useful content for a virtual agent, including training data and responses that are aligned with that company’s brand identity.\\n\\nNevertheless, due to the inherent stochastic nature of such models, achieving absolute 100% accuracy is presently unattainable. Consequently, it remains imperative to incorporate a human-in-the-loop approach to validate any content generated by large language models before conveying it to end-users. This practice becomes particularly crucial when applying such technology in enterprise settings, where potential liability concerns may arise.\\n\\nAt boost.ai, we have developed proprietary algorithms that are able to accurately scan websites and other data sources, and interface with LLMs to essentially tame them for use in enterprise use cases.\\n\\nWant to learn more about how boost.ai is using large language models to create better automated customer experiences today? Check out our latest webinar demoing a variety of LLM-enriched features coming soon to our virtual agents.\\xa0[WATCH HERE]\\n\\nResources\\n\\nCase studies\\n\\nWebinars\\n\\nGuides\\n\\nBlog\\n\\nAnnouncements\\n\\nRelated blogs\\n\\nNo blogs with same category\\n\\nJoin the conversation\\n\\nSign up for our newsletter and receive a free 4-part masterclass on conversational AI\\n\\nI have read the privacy policy\\n\\nPlease Enter Business Email Address\\n\\nThank you! Your submission has been received!\\n\\nOops! Something went wrong while submitting the form.\\n\\nBoost.ai uses the contact information you provide to us to contact you about our products and services. You may unsubscribe from these communications at anytime. For more information check out our privacy policy\\n\\nMain menu\\n\\nHome\\n\\nProduct\\n\\nSolutions\\n\\nContact\\n\\nProduct\\n\\nConversational AI\\n\\nChat Automation\\n\\nVoice Call Automation\\n\\nIntegrations\\n\\nLarge Language Models\\n\\nSolutions\\n\\nCustomer Self-Service\\n\\nInternal Virtual Agent\\n\\nFinancial Services\\n\\nInsurance\\n\\nTelecom\\n\\nPublic Sector\\n\\nContact Centers\\n\\nResources\\n\\nCase studies\\n\\nWebinars\\n\\nGuides\\n\\nBlog\\n\\nAnnouncements\\n\\nAcademy\\n\\nCompany\\n\\nAbout us\\n\\nCareers\\n\\nContact us\\n\\nSuppliers\\n\\nSecurity\\n\\nAccessibility\\n\\nPrivacy policy\\n\\nCookies policy\\n\\nPartners\\n\\nWhy partner\\n\\nBecome a partner\\n\\n© 2023 Boost.ai All rights reserved.\" metadata={'source': 'https://www.boost.ai/blog/llms-large-language-models'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.boost.ai/blog/llms-large-language-models\",\n",
    "    \"https://www.youtube.com/watch?v=6Zv6A_9urh4&t=112s\"\n",
    "]\n",
    "\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "data = loader.load() # .load() returns the list of document instances containing 'page_content' and 'metadata'\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060dcf90",
   "metadata": {},
   "source": [
    "The `SeleniumURLLoader` class has the following attributes:\n",
    "- `urls` (List[str]): List of URLs to load from;\n",
    "- `continue_on_failure` (bool, default=True): If set to True, continues loading other URLs on failure;\n",
    "- `browser` (str, default=\"chrome\"): Browser selection, either 'Chrome' or 'Firefox';\n",
    "- `executable_path` (Optional[str], default=None): Browser executable path;\n",
    "- `headless` (bool, default=True): Browser runs in headless mode if True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82157a9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"GoogleDriveLoader\">\n",
    "    \n",
    "### 2.4. GoogleDriveLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ff75186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GoogleDriveLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4603b046",
   "metadata": {},
   "source": [
    "By default, the GoogleDriveLoader searches for the \"credentials.json\" file in \"~/.credentials/credentials.json\". Use the `credentials_file` keyword argument to modify this path.\n",
    "The \"token.json\" file follows the same principle and will be created automatically upon the loader's first use.\n",
    "\n",
    "Steps to set up the `credentials_file`:\n",
    "\n",
    "1. Create a new Google Cloud Platform project (or use an existing one) by visiting the Google Cloud Console. Ensure that billing is enabled for your project.\n",
    "2. Enable the Google Drive API by navigating to its dashboard in the Google Cloud Console and clicking \"Enable.\"\n",
    "3. Create a service account by going to the Service Accounts page in the Google Cloud Console. Follow the prompts to set up a new service account.\n",
    "4. Assign necessary roles to the service account, such as \"Google Drive API - Drive File Access\" and \"Google Drive API - Drive Metadata Read/Write Access,\" depending on your needs.\n",
    "5. After creating the service account, access the \"Actions\" menu next to it, select \"Manage keys,\" click \"Add Key,\" and choose \"JSON\" as the key type. This generates a JSON key file and downloads it to your computer, which serves as your credentials_file.\n",
    "\n",
    "To retrieve the folder or document ID from the URL:\n",
    "- Folder: https://drive.google.com/drive/u/0/folders/{folder_id}\n",
    "- Document: https://docs.google.com/document/d/{document_id}/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13a172f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GoogleDriveLoader(\n",
    "    folder_id=\"your_folder_id\",\n",
    "    recursive=False  # Optional: Fetch files from subfolders recursively. Defaults to False.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45728d03",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"splitters\">\n",
    "    \n",
    "## 3. Text Splitters\n",
    "    \n",
    "</a>\n",
    "\n",
    "\n",
    "The length of the contents may vary depending on their source and may exceed the input window size of the model. Splitting the large text into smaller segments allows to use the most relevant chunk as the context instead of expecting the model to comprehend the textual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073fb8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ed626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40e0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7afce34e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"embeddings\">\n",
    "    \n",
    "## 4. Embeddings\n",
    "    \n",
    "</a>\n",
    "\n",
    "\n",
    "LLMs can transform textual data into embedding space, allowing for versatile representations across languages.  \n",
    "\n",
    "Embeddings are high-dimensional vectors that capture semantic information. Embeddings also serve to identify relevant information by quantifying the distance between data points (by indicating closer semantic meaning for points being closer together).\n",
    "\n",
    "The LangChain integration provides necessary functions for both transforming and calculating similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fee4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336958e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea10b3ca",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"cs\">\n",
    "    \n",
    "## 5. Customer Support Question Answering Chatbot\n",
    "    \n",
    "</a>\n",
    "\n",
    "Let's demonstrate how to use a website's content as supplementary context for a chatbot to respond to user queries effectively. \n",
    "\n",
    "The code implementation below involves:\n",
    "- employing data loaders, \n",
    "- storing the corresponding embeddings in the Deep Lake dataset, \n",
    "- and retrieving the most relevant documents corresponding to the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0dd70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668847e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f12e6c89",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"gong_io\">\n",
    "    \n",
    "## 6. Gong.io Open-Source Alternative AI Sales Assistant\n",
    "    \n",
    "</a>\n",
    "\n",
    "Let's explore how LangChain, Deep Lake, and GPT-4 can be used to develop a sales assistant able to give advice to salesman, taking into considerations internal guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503bfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3bb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b832b37",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"picture_books\">\n",
    "    \n",
    "## 7. Creating Picture Books with OpenAI, Replicate, and Deep Lake\n",
    "</a>\n",
    "\n",
    "Having as look at the use case of AI technology in the creative domain of children's picture book creation, using both OpenAI GPT-3.5 LLM for writing the story and Stable Diffusion for generating images for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de079a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ce6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7904d5c2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"resources\">\n",
    "    \n",
    "## 8. Additional Resources\n",
    "</a>\n",
    "\n",
    "- [Improving Document Retrieval with Contextual Compression](https://blog.langchain.dev/improving-document-retrieval-with-contextual-compression/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a767818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
