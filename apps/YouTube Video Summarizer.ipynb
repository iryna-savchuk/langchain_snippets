{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9892dccc",
   "metadata": {},
   "source": [
    "#  YouTube Video Summarizer\n",
    "\n",
    "It is possible to build a tool to effectively extract key takeaways from YouTube videos. It can be done in twe main stages: 1) transcribing YouTube audio files with the help of Whisper; 2) creating summarized output with the help of the LangChain's summarization techniques (including stuff, refine, and map_reduce).\n",
    "\n",
    "The **stuff** approach for summarization is the simplest and most naive one: all the text from the documents is used in a single prompt. This method may raise exceptions if all text is longer than the available context size of the LLM. The **map-reduce** and **refine** approaches offer more sophisticated ways to process and extract information from longer documents. The \"map-reduce\" method can be parallelized, hence it is faster. The \"refine\" approach is sequential, so slower if compared to the \"map-reduce\" method, but it produces better results. The most suitable approach should be selected by considering the trade-offs between speed and quality.\n",
    "\n",
    "**Whisper** is a cutting-edge, automatic speech recognition system developed by OpenAI. It has been trained on an impressive 680,000 hours of multilingual and multitasking supervised data sourced from the web.\n",
    "\n",
    "STEPS to implement:\n",
    "- Download the desired YouTube audio file;\n",
    "- Transcribe the audio with the help of Whisper;\n",
    "- Summarize the transcribed text using LangChain (either stuff, or refine, or map_reduce);\n",
    "- Add multiple URLs to DeepLake database, and retrieve information from database to do sematic search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e986ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "\n",
    "# !pip install langchain==0.0.208 deeplake openai tiktoken\n",
    "# !pip install -q yt_dlp\n",
    "# !pip install -q git+https://github.com/openai/whisper.git\n",
    "\n",
    "######################################\n",
    "\n",
    "# MacOS (requires https://brew.sh/)\n",
    "#brew install ffmpeg\n",
    "\n",
    "# Ubuntu\n",
    "#sudo apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a455747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "from keys import OPENAI_API_KEY, ACTIVELOOP_TOKEN\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c1c42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "# Define function to download video from YouTube to a local file\n",
    "def download_mp4_from_youtube(url, filename):\n",
    "    # Set the options for the download\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "        'outtmpl': filename,\n",
    "        'quiet': True,\n",
    "    }\n",
    "    # Download the video file\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "\n",
    "        \n",
    "url = \"https://www.youtube.com/watch?v=mBjPyte2ZZo\"\n",
    "filename = '../data/lecuninterview.mp4'\n",
    "\n",
    "download_mp4_from_youtube(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d08a5",
   "metadata": {},
   "source": [
    "The Whisper package that we installed provides the `.load_model()` method to download the model and transcribe a video file. Multiple different models are available: `tiny`, `base`, `small`, `medium`, and `large` (each of them has tradeoffs between accuracy and speed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda8b948",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iryna/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "filename = '../data/lecuninterview.mp4'\n",
    "result = model.transcribe(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777cef9",
   "metadata": {},
   "source": [
    "**Note**: If an error about SSL certificate is raised while running the code above, have a look at the solution [here](https://stackoverflow.com/questions/68275857/urllib-error-urlerror-urlopen-error-ssl-certificate-verify-failed-certifica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f4b9711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi, I'm Craig Smith and this is I on A On. This week I talked to Jan LeCoon, one of the seminal figures in deep learning development and a long time proponent of self-supervised learning. Jan spoke about what's missing in large language models and about his new joint embedding predictive architecture which may be a step toward filling that gap. He also talked about his theory of consciousness and the potential for AI systems to someday exhibit the features of consciousness. It's a fascinating c\n"
     ]
    }
   ],
   "source": [
    "# Print out a chunk of the result\n",
    "print(result['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02caf9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result to a text file\n",
    "with open ('../output/text.txt', 'w') as file:  \n",
    "    file.write(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c41fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load utilities from the LangChain library that are necessary to perform Summarization Step\n",
    "from langchain import OpenAI, LLMChain  # to handle large texts\n",
    "from langchain.chains.mapreduce import MapReduceChain  # to optimize\n",
    "from langchain.prompts import PromptTemplate   # to construct prompt\n",
    "from langchain.chains.summarize import load_summarize_chain  # to run summarization\n",
    "\n",
    "# Initialize an instance of OpenAI LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad128f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split input text into smaller chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    ")\n",
    "\n",
    "with open('../output/text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts[:5]]  # only the 5 first chunks will be used in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da3072eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jan Le Ka is a professor at New York University and Chief AI Scientist at Fair, a fundamental AI\n",
      "research lab. He has been researching self-supervised learning, which has revolutionized natural\n",
      "language processing by using transformer architectures for pre-training. His latest paper is on\n",
      "joint embedding predictive architecture and how it relates to large language models. Self-supervised\n",
      "learning is a technique used to train large neural networks to predict missing words in a piece of\n",
      "text, and has been used to train large language models to predict the next word. However, attempts\n",
      "to transfer self-supervised learning methods from language processing to images have not been\n",
      "successful, and the only successful approach has been to generate representations of images instead\n",
      "of predicting the image itself.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap   \n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "# Format and print the output\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "065e031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "# To see the prompt template that is used with the map_reduce technique\n",
    "print( chain.llm_chain.prompt.template )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb90431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with the prompt\n",
    "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "167c9c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Jan LeCoon is a seminal figure in deep learning development and a long time proponent of self-supervised learning\n",
      "- Discussed what's missing in large language models and his new joint embedding predictive architecture\n",
      "- Theory of consciousness and potential for AI systems to exhibit features of consciousness\n",
      "- Self-supervised learning revolutionized natural language processing\n",
      "- Large language models lack a world model and generative models are difficult to represent uncertain predictions\n",
      "- Successful in audio but not images, so need to predict a representation of the image\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"stuff\", \n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "wrapped_text = textwrap.fill(output_summary, \n",
    "                             width=1000,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eafb4dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Craig Smith interviews Jan LeCoon, a deep learning developer and proponent of self-supervised\n",
      "learning, about his new joint embedding predictive architecture and his theory of consciousness. Jan\n",
      "discusses the gap in large language models and the potential for AI systems to exhibit features of\n",
      "consciousness. He explains how self-supervised learning has revolutionized natural language\n",
      "processing through the use of transformer architectures for pre-training, such as taking a piece of\n",
      "text, removing some of the words, and replacing them with black markers to train a large neural net\n",
      "to predict the words that are missing. This technique has been used in practical applications such\n",
      "as contact moderation systems on Facebook, Google, YouTube, and more. Jan also explains how this\n",
      "technique can be used to represent uncertain predictions in generative models, such as predicting\n",
      "the missing words in a text, or predicting the missing frames in a video. He further explains that\n",
      "while this technique has been successful in language processing, it has not been successful in the\n",
      "domain of images, and that the only successful technique is one that predicts a representation of\n",
      "the image instead of the image itself.\n"
     ]
    }
   ],
   "source": [
    "# Generating more accurate and context-aware summaries with 'refine'\n",
    "# It generates the summary of the first chunk; \n",
    "# Then, for each successive chunk, the summary is integrated with new info from the new chunk.\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92e0cf",
   "metadata": {},
   "source": [
    "**Working with multiple video URLs. Adding Transcripts to DeepLake.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22fa733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading video files from multiple URLs\n",
    "import yt_dlp\n",
    "\n",
    "def download_mp4_from_youtube(urls, job_id):\n",
    "    # This will hold the titles and authors of each downloaded video\n",
    "    video_info = []\n",
    "\n",
    "    for i, url in enumerate(urls):\n",
    "        # Set the options for the download\n",
    "        file_temp = f'../data/{job_id}_{i}.mp4\n",
    "        ydl_opts = {\n",
    "            'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "            'outtmpl': file_temp,\n",
    "            'quiet': True,\n",
    "        }\n",
    "\n",
    "        # Download the video file\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            result = ydl.extract_info(url, download=True)\n",
    "            title = result.get('title', \"\")\n",
    "            author = result.get('uploader', \"\")\n",
    "\n",
    "        # Add the title and author to our list\n",
    "        video_info.append((file_temp, title, author))\n",
    "\n",
    "    return video_info\n",
    "\n",
    "\n",
    "urls=[\"https://www.youtube.com/watch?v=mBjPyte2ZZo&t=78s\",\n",
    "    \"https://www.youtube.com/watch?v=cjs7QKJNVYM\",]\n",
    "\n",
    "videos_details = download_mp4_from_youtube(urls, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27ffa5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing ./data/1_0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iryna/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing ./data/1_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iryna/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# Load the transcription model \n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Iterate through each video and transcribe\n",
    "results = []\n",
    "\n",
    "for video in videos_details:\n",
    "    print(f\"Transcribing {video[0]}\")\n",
    "    result = model.transcribe(video[0])\n",
    "    results.append( result['text'] )\n",
    "    # print(f\"Transcription for {video[0]}:\\n{result['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74456b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save obtained transcriptions to .txt file\n",
    "with open ('../output/mult_text.txt', 'w') as file:\n",
    "    for r in results:\n",
    "        file.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9d423f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the texts from the file and split the text to chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the texts\n",
    "with open('../output/mult_text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n",
    "                                               chunk_overlap=0, \n",
    "                                               separators=[\" \", \",\", \"\\n\"])\n",
    "\n",
    "texts = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d927ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack all the chunks into a Documents\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=t) for t in texts[:10]] # will save the first 100 chunks to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a99063be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://iryna/youtube_summarizer_db', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (10, 1536)  float32   None   \n",
      "    id        text      (10, 1)      str     None   \n",
      " metadata     json      (10, 1)      str     None   \n",
      "   text       text      (10, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['493d4494-3b9d-11ee-8b7c-12ee7aa5dbdc',\n",
       " '493d457a-3b9d-11ee-8b7c-12ee7aa5dbdc',\n",
       " '493d45c0-3b9d-11ee-8b7c-12ee7aa5dbdc',\n",
       " '493d45f2-3b9d-11ee-8b7c-12ee7aa5dbdc',\n",
       " '493d4624-3b9d-11ee-8b7c-12ee7aa5dbdc',\n",
       " '493d4656-3b9d-11ee-8b7c-12ee7aa5dbdc',\n",
       " '493d467e-3b9d-11ee-8b7c-12ee7aa5dbdc',\n",
       " '493d46b0-3b9d-11ee-8b7c-12ee7aa5dbdc',\n",
       " '493d46e2-3b9d-11ee-8b7c-12ee7aa5dbdc',\n",
       " '493d470a-3b9d-11ee-8b7c-12ee7aa5dbdc']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a DeepLake database with embedded documents\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "my_activeloop_org_id = \"iryna\"\n",
    "my_activeloop_dataset_name = \"youtube_summarizer_db\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6cacd699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a retriever object\n",
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['k'] = 4 #search for 4 the most relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cda7b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constract prompt template with the QA chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of transcripts from a video to answer the question in a summarized manner. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Summarized answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "693e0ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Google company is mentioned as an example of a practical application of self-supervised learning, which is used for contact moderation systems.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever,\n",
    "                                 chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "\n",
    "print(qa.run(\"What is said about Google company?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f7c3d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"resources\">\n",
    "    \n",
    "## Additional Resources\n",
    "    \n",
    "</a>\n",
    "\n",
    "- [Textwrap Package](https://docs.python.org/3/library/textwrap.html)\n",
    "- [Introducing Whisper](https://openai.com/research/whisper)\n",
    "- [Deep Lake Vector Store in LangChain](https://docs.activeloop.ai/tutorials/vector-store/deep-lake-vector-store-in-langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83bc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
