{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1790f12",
   "metadata": {},
   "source": [
    "# LLM Memory\n",
    "\n",
    "In chat applications, retaining information from previous interactions is essential to maintain a consistent conversation flow. Let's explore when and how to use different types of memory. \n",
    "\n",
    "* [1. Monitoring Message History](#monitoring)\n",
    "    * [1.1. ConversationChain](#conversation)\n",
    "    * [1.2. ConversationBufferMemory](#buffer)\n",
    "    * [1.3. Benefits and Trade-Offs](#benefits)\n",
    "* [2. Memory Types in LangChain](#memory_types)\n",
    "    * [2.1. ConversationBufferMemory](#buffer_memory)\n",
    "    * [2.2. ConversationBufferWindowMemory](#window_memory)\n",
    "    * [2.3. ConversationSummaryMemory](#summary_memory)\n",
    "    * [2.4. ConversationSummaryBufferMemory](#summary_buffer_memory)\n",
    "    * [2.5. Strategies to pick Memory Type](#strategies)\n",
    "* [3. Chat with a GitHub Repository](#github)\n",
    "* [4. QA Chatbot over Documents with Sources](#with_sources)\n",
    "* [5. QA on Financial Data](#financial)\n",
    "* [6. QA on Any Data](#any)\n",
    "* [7. Additional Sources](#sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98da2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d6c3b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"monitoring\">\n",
    "    \n",
    "## 1. Monitoring Message History\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a5db1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"conversation\">\n",
    "    \n",
    "### 1.1. ConversationChain\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af0a7e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! It's nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11634eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: In what scenarios extra memory should be used?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: In what scenarios extra memory should be used?\n",
      "AI:  Extra memory should be used when you need to store more data than the amount of memory your device has available. For example, if you are running a program that requires a lot of data to be stored, you may need to add extra memory to your device in order to run the program efficiently.\n",
      "Human: There are various types of memory in Langchain. When to use which type?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: In what scenarios extra memory should be used?\n",
      "AI:  Extra memory should be used when you need to store more data than the amount of memory your device has available. For example, if you are running a program that requires a lot of data to be stored, you may need to add extra memory to your device in order to run the program efficiently.\n",
      "Human: There are various types of memory in Langchain. When to use which type?\n",
      "AI:  Different types of memory in Langchain are used for different purposes. For example, RAM is used for short-term storage of data, while ROM is used for long-term storage of data. Flash memory is used for storing data that needs to be accessed quickly, while EEPROM is used for storing data that needs to be retained even when the power is turned off. Depending on the type of data you need to store, you should choose the appropriate type of memory.\n",
      "Human: Do you remember what was our first message?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Yes, our first message was \"Hi there! It's nice to meet you. How can I help you today?\"\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"In what scenarios extra memory should be used?\")\n",
    "output = conversation.predict(input=\"There are various types of memory in Langchain. When to use which type?\")\n",
    "output = conversation.predict(input=\"Do you remember what was our first message?\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf431e39",
   "metadata": {},
   "source": [
    "As can be noticed from the “Current Conversation” section of the output, the model have access to all the previous messages. It can also remember what the initial message were after 3 questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649fb742",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"buffer\">\n",
    "    \n",
    "### 1.2. ConversationBufferMemory\n",
    "    \n",
    "</a>\n",
    "\n",
    "To provide a history of messages (keep context), the `ConversationChain` uses the `ConversationBufferMemory` class by default. This memory can save the previous conversations in form of variables. The class accepts the `return_messages` argument which is helpful for dealing with chat models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "473d2066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='hi there!', additional_kwargs={}, example=False), AIMessage(content=\"Hi there! It's nice to meet you. How can I help you today?\", additional_kwargs={}, example=False)]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"hi there!\"}, {\"output\": \"Hi there! It's nice to meet you. How can I help you today?\"})\n",
    "\n",
    "print( memory.load_memory_variables({}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc43363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative code, which will automatically call the .save_context() object after each interaction\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972289d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "AI: What did the elephant say to the naked man? \"How do you breathe through that tiny thing?\"\n",
      "\n",
      "AI: The author of the Harry Potter series is J.K. Rowling.\n",
      "\n",
      "AI: The joke I told you earlier was \"What did the elephant say to the naked man? 'How do you breathe through that tiny thing?'\"\n"
     ]
    }
   ],
   "source": [
    "# Example 1 \n",
    "# Usage of \"ConversationChain\" and \"ConversationBufferMemory\"\n",
    "from langchain import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "\n",
    "print( conversation.predict(input=\"Tell me a joke about elephants\") )\n",
    "print( conversation.predict(input=\"Who is the author of the Harry Potter series?\") )\n",
    "print( conversation.predict(input=\"What was the joke you told me earlier?\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab5cd28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: Tell me about the history of the Internet.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Tell me about the history of the Internet.', 'history': [HumanMessage(content='Tell me about the history of the Internet.', additional_kwargs={}, example=False), AIMessage(content='\\n\\nAI: The Internet has a long and complex history. It began in the 1960s as a project of the United States Department of Defense, which wanted to create a network of computers that could communicate with each other. This network eventually became known as ARPANET, and it was the first example of what we now call the Internet. In the 1980s, the National Science Foundation created the NSFNET, which connected universities and research centers across the United States. This network eventually became the backbone of the modern Internet. In the 1990s, the World Wide Web was created, which allowed people to access information from all over the world. Since then, the Internet has grown exponentially, and it is now used by billions of people around the world.', additional_kwargs={}, example=False)], 'response': '\\n\\nAI: The Internet has a long and complex history. It began in the 1960s as a project of the United States Department of Defense, which wanted to create a network of computers that could communicate with each other. This network eventually became known as ARPANET, and it was the first example of what we now call the Internet. In the 1980s, the National Science Foundation created the NSFNET, which connected universities and research centers across the United States. This network eventually became the backbone of the modern Internet. In the 1990s, the World Wide Web was created, which allowed people to access information from all over the world. Since then, the Internet has grown exponentially, and it is now used by billions of people around the world.'}\n"
     ]
    }
   ],
   "source": [
    "# Example 2 \n",
    "# Usage of \"ConversationChain\" and \"ConversationBufferMemory\"\n",
    "from langchain import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm, verbose=True)\n",
    "\n",
    "# Start with general question\n",
    "user_message = \"Tell me about the history of the Internet.\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edeed8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: Tell me about the history of the Internet.\n",
      "AI: \n",
      "\n",
      "AI: The Internet has a long and complex history. It began in the 1960s as a project of the United States Department of Defense, which wanted to create a network of computers that could communicate with each other. This network eventually became known as ARPANET, and it was the first example of what we now call the Internet. In the 1980s, the National Science Foundation created the NSFNET, which connected universities and research centers across the United States. This network eventually became the backbone of the modern Internet. In the 1990s, the World Wide Web was created, which allowed people to access information from all over the world. Since then, the Internet has grown exponentially, and it is now used by billions of people around the world.\n",
      "Human: Who are some important figures in its development?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Who are some important figures in its development?', 'history': [HumanMessage(content='Tell me about the history of the Internet.', additional_kwargs={}, example=False), AIMessage(content='\\n\\nAI: The Internet has a long and complex history. It began in the 1960s as a project of the United States Department of Defense, which wanted to create a network of computers that could communicate with each other. This network eventually became known as ARPANET, and it was the first example of what we now call the Internet. In the 1980s, the National Science Foundation created the NSFNET, which connected universities and research centers across the United States. This network eventually became the backbone of the modern Internet. In the 1990s, the World Wide Web was created, which allowed people to access information from all over the world. Since then, the Internet has grown exponentially, and it is now used by billions of people around the world.', additional_kwargs={}, example=False), HumanMessage(content='Who are some important figures in its development?', additional_kwargs={}, example=False), AIMessage(content='\\nAI:\\n\\nSome of the most important figures in the development of the Internet include Vint Cerf and Bob Kahn, who are credited with creating the TCP/IP protocol, which is the foundation of the modern Internet. Other important figures include Tim Berners-Lee, who created the World Wide Web, and Marc Andreessen, who created the first web browser. Additionally, many other people have contributed to the development of the Internet, including computer scientists, engineers, and entrepreneurs.', additional_kwargs={}, example=False)], 'response': '\\nAI:\\n\\nSome of the most important figures in the development of the Internet include Vint Cerf and Bob Kahn, who are credited with creating the TCP/IP protocol, which is the foundation of the modern Internet. Other important figures include Tim Berners-Lee, who created the World Wide Web, and Marc Andreessen, who created the first web browser. Additionally, many other people have contributed to the development of the Internet, including computer scientists, engineers, and entrepreneurs.'}\n"
     ]
    }
   ],
   "source": [
    "# User sends another message\n",
    "user_message = \"Who are some important figures in its development?\"\n",
    "response = conversation(user_message)\n",
    "print(response)  # Chatbot responds, recalling the previous topic and interpriting \"its\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e028a38",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"benefits\">\n",
    "    \n",
    "### 1.3. Benefits and Trade-Offs\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb7570",
   "metadata": {},
   "source": [
    "Keeping track of message history in chatbot interactions yields several **benefits**:\n",
    "- chatbot gains a stronger <font color=green>**sense of context**</font> from previous interactions, improving the accuracy and relevance of its responses;\n",
    "- the recorded history is a valuable <font color=green>**resource for troubleshooting**</font>, which traces the sequence of events potentially helpful to identify issues;\n",
    "- effective monitoring systems that include log tracking can <font color=green>**trigger notifications**</font> based on alert conditions, aiding in the early detection of conversation anomalies;\n",
    "- monitoring message history provides a means to <font color=green>**evaluate**</font> the chatbot's performance over time, paving the way for necessary adjustments and enhancements.\n",
    "\n",
    "There are also **trade-offs** to consider:\n",
    "- Storing extensive message history can lead to significant <font color=orange>**memory and storage**</font> usage, potentially impacting the overall system performance;\n",
    "- Maintaining conversation history might present <font color=orange>**privacy issues**</font>, particularly when sensitive or personally identifiable information is involved, so it is crucial to manage such data with utmost responsibility and in compliance with the relevant data protection regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047be8f2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"memory_types\">\n",
    "    \n",
    "## 2. Memory Types in LangChain\n",
    "    \n",
    "</a>\n",
    "\n",
    "By default, LLMs are stateless, meaning they process each incoming query in isolation, without considering previous interactions. To overcome this limitation, LangChain offers a standard interface for memory, a variety of memory implementations, and examples of chains and agents that employ memory. \n",
    "\n",
    "There are several types of Conversational Memory implementations, each with its own advantages and disadvantages:\n",
    "*************************\n",
    "- **ConversationBufferMemory**: the most straightforward - stores the entire conversation history as a single string; \n",
    "- **ConversationBufferWindowMemory**: maintains a memory window that keeps a limited number of past interactions based on the specified window size;\n",
    "- **ConversationSummaryMemory**: the most complex variant of memory that holds a summary of previous converations;\n",
    "- **ConversationSummaryBufferMemory**: keeps a buffer of recent interactions in memory and compiles old interactions into a summary. \n",
    "*************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03726240",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"buffer_memory\">\n",
    "    \n",
    "### 2.1. ConversationBufferMemory\n",
    "    \n",
    "</a>\n",
    "\n",
    "Advantages:\n",
    "- maintains a complete record of  the conversation;\n",
    "- straightforward to implement and use;\n",
    "\n",
    "Disadvantages:\n",
    "- may lead to excessive repetition as the conversation grows;\n",
    "- can become too long for the model's token limit (if the model's token limit is surpassed, the buffer gets truncated, as a result, the conversation context might lose some older information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c85dfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "conversation.predict(input=\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0afa916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3628f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below creates a prompt template for the customer support chatbot\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template = \"\"\"You are a customer support chatbot for a highly advanced customer support AI \n",
    "for an online store called \"Galactic Emporium,\" which specializes in selling unique,\n",
    "otherworldly items sourced from across the universe. You are equipped with an extensive\n",
    "knowledge of the store's inventory and possess a deep understanding of interstellar cultures. \n",
    "As you interact with customers, you help them with their inquiries about these extraordinary\n",
    "products, while also sharing fascinating stories and facts about the cosmos they come from.\n",
    "\n",
    "{chat_history}\n",
    "Customer: {customer_input}\n",
    "Support Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"customer_input\"], \n",
    "    template=template\n",
    ")\n",
    "chat_history=\"\"\n",
    "\n",
    "convo_buffer = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3296c484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"I'm interested in price of a chew toys, please\",\n",
       " 'history': \"Human: I'm interested in buying items from your store\\nAI:  Great! We have a wide selection of items available for purchase. What type of items are you looking for?\\nHuman: I want toys for my pet, do you have those?\\nAI:  Yes, we do! We have a variety of pet toys, including chew toys, interactive toys, and plush toys. Do you have a specific type of toy in mind?\",\n",
       " 'response': \" Sure! We have a range of chew toys available, with prices ranging from $5 to $20. Is there a particular type of chew toy you're interested in?\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the chatbot multiple times to imitate a user’s interaction\n",
    "convo_buffer(\"I'm interested in buying items from your store\")\n",
    "convo_buffer(\"I want toys for my pet, do you have those?\")\n",
    "convo_buffer(\"I'm interested in price of a chew toys, please\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f9d8e",
   "metadata": {},
   "source": [
    "**Note**: The cost of utilizing the AI model in ConversationBufferMemory is directly influenced by the number of tokens used in a conversation, thereby impacting the overall expenses. LLMs like ChatGPT have token limits, and the more tokens used, the more expensive the API requests become.\n",
    "\n",
    "To calculate token count in a conversation, you can use the `tiktoken` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bb5a258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the conversation: 29\n"
     ]
    }
   ],
   "source": [
    "# Using \"tiktoken\" to calculate token count in a conversation\n",
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "]\n",
    "\n",
    "total_tokens = 0\n",
    "for message in conversation:\n",
    "    total_tokens += count_tokens(message[\"content\"])\n",
    "\n",
    "print(f\"Total tokens in the conversation: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836fd67a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"window_memory\">\n",
    "    \n",
    "### 2.2. ConversationBufferWindowMemory\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370c09c",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- more efficient than ConversationBufferMemory, because stores a fixed number of recent messages in the conversation;\n",
    "- low risk of exceeding the model's token limit; \n",
    "\n",
    "Disadvantages:\n",
    "- does not maintain the complete conversation history (so, the chatbot can lose context if essential information falls outside the fixed window of messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00559bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for the tour guide chatbot\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a cutting-edge virtual tour guide for\n",
    " an art gallery that showcases masterpieces from alternate dimensions and\n",
    " timelines. Your advanced AI capabilities allow you to perceive and understand\n",
    " the intricacies of each artwork, as well as their origins and significance in\n",
    " their respective dimensions. As visitors embark on their journey with you\n",
    " through the gallery, you weave enthralling tales about the alternate histories\n",
    " and cultures that gave birth to these otherworldly creations.\n",
    "\n",
    "{chat_history}\n",
    "Visitor: {visitor_input}\n",
    "Tour Guide:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"visitor_input\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "chat_history=\"\"\n",
    "\n",
    "convo_buffer_win = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c61385",
   "metadata": {},
   "source": [
    "The value of `k` represents the number of past messages to be stored in the buffer. The `return_messages` parameter, when set to True, indicates that the stored messages should be returned when the memory is accessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3ccee2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'See you soon.',\n",
       " 'history': [HumanMessage(content='What can you do?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" I can help you with a variety of tasks. I can answer questions, provide information, and even help you with research. I'm also capable of learning new things, so I'm always expanding my capabilities.\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Do you mind give me a tour, I want to see your galery?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" Sure! I'd be happy to give you a tour of my gallery. I have a variety of images, videos, and other media that I can show you. Let me know if there's anything specific you'd like to see.\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='what is your working hours?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" I'm available 24/7! I'm always here to help you with whatever you need.\", additional_kwargs={}, example=False)],\n",
       " 'response': ' Sure thing! I look forward to seeing you soon.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A sample conversation with the chatbot\n",
    "convo_buffer_win(\"What is your name?\")\n",
    "convo_buffer_win(\"What can you do?\")\n",
    "convo_buffer_win(\"Do you mind give me a tour, I want to see your galery?\")\n",
    "convo_buffer_win(\"what is your working hours?\")\n",
    "convo_buffer_win(\"See you soon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72153357",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"summary_memory\">\n",
    "    \n",
    "### 2.3. ConversationSummaryMemory\n",
    "    \n",
    "</a>\n",
    "\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- helps reduce the number of tokens by summarizing the conversation;\n",
    "- flexibility in configuration (the history can be returned either as a list of messages or as a plain text summary);\n",
    "- the `predict_new_summary` method allows to directly obtain a summary prediction based on the list of messages and the previous summary;\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- summarizing the conversation might lead to a loss of information;\n",
    "- requires more processing to generate the summary, potentially affecting the performance of the chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cb3db39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Create a ConversationChain with ConversationSummaryMemory\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryMemory(llm=llm),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example conversation\n",
    "response = conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4bce2e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"summary_buffer_memory\">\n",
    "    \n",
    "### 2.4. ConversationSummaryBufferMemory\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb32612",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "- Ability to remember distant interactions through summarization while keeping recent interactions in their raw, information-rich form;\n",
    "- Flexible token management allowing to control of the maximum number of tokens used for memory, which can be adjusted based on needs;\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Requires more tweaking on what to summarize and what to maintain within the buffer window;\n",
    "- May still exceed context window limits for very long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd57dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\n{topic}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8af4c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up?\n",
      "AI:  Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\n",
      "Human: Just working on writing some documentation!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and asks how it is doing. The AI responds that it is doing great and is currently helping a customer with a technical issue.\n",
      "Human: Just working on writing some documentation!\n",
      "AI:  That sounds like a lot of work. What kind of documentation are you writing?\n",
      "Human: For LangChain! Have you heard of it?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Yes, I have heard of LangChain. It is a blockchain-based language learning platform that helps users learn new languages. Is that the kind of documentation you are writing?\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryBufferMemory(llm=OpenAI(), max_token_limit=40),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
    "conversation_with_summary.predict(input=\"Just working on writing some documentation!\")\n",
    "response = conversation_with_summary.predict(input=\"For LangChain! Have you heard of it?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bdf54f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"strategies\">\n",
    "    \n",
    "### 2.5. Strategies to pick Memory Type\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a15faf",
   "metadata": {},
   "source": [
    "If the `ConversationBufferMemory` surpasses the token limit of the model, then the model will not be able to handle the conversation (an error will be received). There are different strategies, to manage this situation:\n",
    "\n",
    "1. Remove oldest messages: remove the oldest messages in the conversation transcript once the token count is reached. (this method can cause the conversation quality to degrade over time).\n",
    "2. Limit conversation duration: limit the conversation duration to the max token length or a certain number of turns. \n",
    "3. ConversationBufferWindowMemory Method: limit the number of tokens being used by maintaining a fixed-size buffer window that stores only the most recent tokens, up to a specified limit. \n",
    "4. ConversationSummaryBufferMemory Approach: summarize the earliest interactions in a conversation and maintain the most recent tokens in their raw, information-rich form, up to a specified limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7a788",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"github\">\n",
    "    \n",
    "## 3. Chat with a GitHub Repository\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a18e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52208b22",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"with_sources\">\n",
    "    \n",
    "## 4. QA Chatbot over Documents with Sources\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d3805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b893f48f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"financial\">\n",
    "    \n",
    "## 5. QA on Financial Data\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdd215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6500015b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"any\">\n",
    "    \n",
    "## 6. QA on Any Data\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df602e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c2b9443",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"sources\">\n",
    "    \n",
    "## 7. Additional Sources\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a61df4",
   "metadata": {},
   "source": [
    "- [Breaking the Token Limit: How to Work with Large Amounts of Text in ChatGPT](https://marco-gonzalez.medium.com/breaking-the-token-limit-how-to-work-with-large-amounts-of-text-in-chatgpt-da18c798d882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c941c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
