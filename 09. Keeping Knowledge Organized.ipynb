{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1790f12",
   "metadata": {},
   "source": [
    "# Keeping Knowledge Organized\n",
    "\n",
    "* [1. Utilizing Deep Lake](#deeplake)\n",
    "    * [1.1. Using Text Loaders and Text Splitters](#loaders_splitters) \n",
    "    * [1.2. Exploring DeepLake - adding and retrieving data](#deeplake_explore) \n",
    "    * [1.3. Question-Answering Example](#q_a) \n",
    "    * [1.4. Using Document Compressors](#compressors) \n",
    "* [2. Streamlined Data Ingestion](#ingestion)\n",
    "    * [2.1. TextLoader](#TextLoader)\n",
    "    * [2.2. PyPDFLoader](#PyPDFLoader) \n",
    "    * [2.3. SeleniumURLLoader](#SeleniumURLLoader) \n",
    "    * [2.4. GoogleDriveLoader](#GoogleDriveLoader) \n",
    "* [3. Text Splitters](#splitters)\n",
    "    * [3.1. Character Text Splitter](#Character_Splitter)\n",
    "    * [3.2. Recursive Character Text Splitter](#Recursive_Splitter) \n",
    "    * [3.3. NLTK Text Splitter](#NLTK_Splitter) \n",
    "    * [3.4. Spacy Text Splitter](#Spacy_Splitter) \n",
    "    * [3.5. Markdown Text Splitter](#Markdown_Splitter) \n",
    "    * [3.6. Token Text Splitter](#Token_Splitter) \n",
    "* [4. Embeddings](#embeddings)\n",
    "    * [4.1. Similarity search and vector embeddings](#sim_search)\n",
    "    * [4.2. Embedding Models](#emb_models)\n",
    "    * [4.3. Cohere embeddings](#cohere)\n",
    "    * [4.4. DeepLake Vector Store Embeddings](#vector_store)\n",
    "* [5. Customer Support Question Answering Chatbot](#cs)\n",
    "* [6. Additional Resources (including sales assistant and book generator)](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98da2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d6c3b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"deeplake\">\n",
    "    \n",
    "## 1. Utilizing Deep Lake\n",
    "    \n",
    "</a>\n",
    "\n",
    "In LangChain, a crucial role in structuring documents and fetching relevant data for LLMs belongs to **indexes and retrievers**. An `index` is a data structure that organizes and stores documents to enable efficient searching, while a `retriever` uses the index to find and return relevant documents in response to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f2214",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"loaders_splitters\">\n",
    "    \n",
    "### 1.1. Using Text Loaders and Text Splitters\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db8d62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09ad800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of text, taken from https://www.theverge.com/2023/3/14/23639313/google-ai-language-model-palm-api-challenge-openai\n",
    "text = \"\"\"Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\n",
    "Google is offering developers access to one of its most advanced AI language models: PaLM.\n",
    "The search giant is launching an API for PaLM alongside a number of AI enterprise tools\n",
    "it says will help businesses “generate text, images, code, videos, audio, and more from\n",
    "simple natural language prompts.”\n",
    "\n",
    "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\n",
    "Meta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\n",
    "PaLM is a flexible system that can potentially carry out all sorts of text generation and\n",
    "editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\n",
    "example, or you could use it for tasks like summarizing text or even writing code.\n",
    "(It’s similar to features Google also announced today for its Workspace apps like Google\n",
    "Docs and Gmail.)\n",
    "\"\"\"\n",
    "\n",
    "# Write text to local file\n",
    "with open(\"output/my_file.txt\", \"w\") as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05aa7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Use TextLoader to load text from the local file\n",
    "loader = TextLoader(\"output/my_file.txt\")\n",
    "docs_from_file = loader.load()\n",
    "\n",
    "print(len(docs_from_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac7df4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses “generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.”\\n\\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summarizing text or even writing code.\\n(It’s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)\\n', metadata={'source': 'output/my_file.txt'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71dc35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 373, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Use CharacterTextSplitter to split the docs into texts\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20) # create a text splitter\n",
    "docs = text_splitter.split_documents(docs_from_file) # split documents into chunks\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966041eb",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"deeplake_explore\">\n",
    "    \n",
    "### 1.2. Exploring DeepLake - adding and retrieving data\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f67d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an embedder model\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781ff88",
   "metadata": {},
   "source": [
    "Let's explore Deep Lake and how it can be utilized to retrieve pertinent documents for contextual use. \n",
    "\n",
    "**Deep Lake** is a vector store that provides several advantages:\n",
    "\n",
    "- It’s **multimodal**, which means that it can be used to store items of diverse modalities (texts, images, audio, and video, along with their vector representations).\n",
    "- It’s **serverless**, which means that we can create and manage cloud datasets without the need to create and manage a database instance. \n",
    "- It’s possible to create a *streaming data loader* out of the data loaded into a Deep Lake dataset, which is convenient for fine-tuning machine learning models using common frameworks like PyTorch and TensorFlow.\n",
    "- Data can be **queried and visualized** from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea9bbf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Activeloop key \n",
    "from keys import ACTIVELOOP_TOKEN\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN\n",
    "\n",
    "# Import DeepLake\n",
    "from langchain.vectorstores import DeepLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c38e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "# Create DeepLake dataset\n",
    "my_activeloop_org_id = \"iryna\"\n",
    "my_activeloop_dataset_name = \"langchain_course_knowledge_organized\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad829e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://iryna/langchain_course_knowledge_organized', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (2, 1536)  float32   None   \n",
      "    id        text      (2, 1)      str     None   \n",
      " metadata     json      (2, 1)      str     None   \n",
      "   text       text      (2, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['d0c2411c-38b9-11ee-a540-12ee7aa5dbdc',\n",
       " 'd0c24234-38b9-11ee-a540-12ee7aa5dbdc']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add documents to the DeepLake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7e95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from db\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e6788",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"q_a\">\n",
    "    \n",
    "### 1.3. Question-Answering Example\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c87f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Create a retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(model=\"text-davinci-003\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3f40354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Google is offering developers access to its PaLM AI language model, which is similar to the GPT series created by OpenAI. It can be used for tasks like summarizing text or writing code, which could potentially challenge OpenAI's models.\n"
     ]
    }
   ],
   "source": [
    "query = \"How Google plans to challenge OpenAI?\"\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40220781",
   "metadata": {},
   "source": [
    "What happened under the hood in the question-answering example above is a similarity search. It was conducted using the embeddings to identify matching documents to be used as context for the LLM. Preselecting the most suitable documents based on semantic similarity enables us to provide the model with meaningful knowledge through the prompt while remaining within the allowed context size.\n",
    "\n",
    "Also, \"stuff chain\" was used to supply information to the LLM. In this technique, we \"stuff\" all the information into the LLM's prompt. \n",
    "\n",
    "**Note:** Stuffing is effective only with shorter documents because of context length limit that most LLMs have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871dd825",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"compressors\">\n",
    "    \n",
    "### 1.4. Using Document Compressors\n",
    "    \n",
    "</a>\n",
    "\n",
    "Including unrelated information in the LLM prompt is detrimental, because it can divert the LLM's focus from important details and occupies valuable prompt space.\n",
    "\n",
    "To address this issue and improve the retrieval process, let's use a wrapper named `ContextualCompressionRetriever` that will wrap the base retriever with an `LLMChainExtractor`. The `LLMChainExtractor` iterates over the initially returned documents and extracts only the content relevant to the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e41e1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how to use ContextualCompressionRetriever with LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Create GPT3 wrapper\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Create compressor for the retriever\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9322bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iryna/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/langchain/chains/llm.py:279: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google is offering developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses “generate text, images, code, videos, audio, and more from simple natural language prompts.”\n"
     ]
    }
   ],
   "source": [
    "# Retrieve compressed (relevant) documents \n",
    "retrieved_docs = compression_retriever.get_relevant_documents(\"How Google plans to challenge OpenAI?\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a06b13",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"ingestion\">\n",
    "    \n",
    "## 2. Streamlined Data Ingestion\n",
    "    \n",
    "</a>\n",
    "\n",
    "The LangChain library offers a variety of helpers designed to facilitate data loading and extraction from diverse sources: \n",
    "- TextLoader (handling plain text files);\n",
    "- PyPDFLoader (dealing with PDF files);\n",
    "- SeleniumURLLoaders (loading HTML documents from URLs that require JavaScript rendering);\n",
    "- GoogleDriveLoader (importing data from Google Drive docs or folders).\n",
    "\n",
    "Regardless of whether the information originates from a PDF file or website content, these classes streamline the process of handling different data formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d049ea88",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"TextLoader\">\n",
    "    \n",
    "### 2.1. TextLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea72d06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses “generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.”\\n\\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summarizing text or even writing code.\\n(It’s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)\\n', metadata={'source': 'data/my_file.txt'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "file_path = 'data/my_file.txt'\n",
    "loader = TextLoader(file_path) # optional argument: encoding=\"ISO-8859-1\"\n",
    "documents = loader.load()\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891b676",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"PyPDFLoader\">\n",
    "    \n",
    "### 2.2. PyPDFLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d94fbe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ff2e764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Simulated Annealing in Early Layers Leads to Better Generalization\\nAmir M. Sarfi1,2Zahra Karimpour1Muawiz Chaudhary1,2Nasir M. Khalid1,2\\nMirco Ravanelli1,2Sudhir Mudur1Eugene Belilovsky1,2\\n1Concordia University2Mila – Quebec AI Institute\\nAbstract\\nRecently, a number of iterative learning methods have\\nbeen introduced to improve generalization. These typically\\nrely on training for longer periods of time in exchange for\\nimproved generalization. LLF (later-layer-forgetting) is a\\nstate-of-the-art method in this category. It strengthens learn-\\ning in early layers by periodically re-initializing the last\\nfew layers of the network. Our principal innovation in this\\nwork is to use Simulated annealing in EArly Layers (SEAL)\\nof the network in place of re-initialization of later layers.\\nEssentially, later layers go through the normal gradient de-\\nscent process, while the early layers go through short stints\\nof gradient ascent followed by gradient descent. Extensive\\nexperiments on the popular Tiny-ImageNet dataset bench-\\nmark and a series of transfer learning and few-shot learning\\ntasks show that we outperform LLF by a significant margin.\\nWe further show that, compared to normal training, LLF\\nfeatures, although improving on the target task, degrade\\nthe transfer learning performance across all datasets we ex-\\nplored. In comparison, our method outperforms LLF across\\nthe same target datasets by a large margin. We also show\\nthat the prediction depth of our method is significantly lower\\nthan that of LLF and normal training, indicating on average\\nbetter prediction performance.1\\n1. Introduction\\nOverfitting is a crucial challenge in supervised deep\\nlearning, which prevents a neural network from performing\\nwell on unseen data. This problem is particularly perva-\\nsive in smaller dataset regimes. Classical machine learn-\\ning approaches to address this problem typically rely on\\nexplicit regularization added as part of the optimization ob-\\nThis research was partially funded by NSERC Discovery\\nGrant RGPIN2021-04104 and RGPIN-2019-05729. We ac-\\nknowledge the resources provided by Compute Canada and Cal-\\ncul Quebec. Correspondence to: a.m.sarfi@gmail.com,\\neugene.belilovsky@concordia.ca,\\nsudhir.mudur@concordia.ca\\n1The code to reproduce our results is publicly available at:\\nhttps://github.com/amiiir-sarfi/SEALjective [39]. A number of implicit approaches are often\\napplied in training deep networks. These implicit methods\\nare usually easier to design than explicit terms added to the\\nobjective function. For example, early stopping [44] and\\ndropout [34] are classical examples of implicit regulariza-\\ntion methods. It can be shown that these techniques can be\\nshown to be linked directly to explicit regularization terms.\\nConsider the case of linear regression, early stopping can be\\ndirectly seen as Tikhonov regularization [44]. Indeed the use\\nof modified optimization strategies to improve generalization\\nis becoming pervasive in deep learning [7].\\nRecently, iterative training methods have been introduced\\nto improve generalization in deep networks. These allow\\nneural networks to be trained for many epochs while pro-\\ngressively improving generalization [9, 10,37, 43,48]. These\\nworks typically rely on a notion of a generation , in which\\na model is optimized towards a local minimum in a single\\ngeneration. Subsequently, in a new generation, the objective\\nfunction is modified, or the network is perturbed towards\\na high loss, requiring a new generation of optimization to-\\nwards a minimum. The earliest work on this topic focused\\non self-distillation [10, 43], where, in each successive gen-\\neration, a student network with the same architecture was\\ninitialized and trained to mimic the softmax output distribu-\\ntion of the previous model. This procedure was theoretically\\nstudied by [23] and provided the formal link to its explicit\\nregularization effects.\\nA number of other techniques were subsequently pro-\\nposed. These have been characterized by Zhou et al. [48] as' metadata={'source': 'data/article.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF file has been taken here:\n",
    "# https://arxiv.org/pdf/2304.04858.pdf\n",
    "loader = PyPDFLoader(\"data/article.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4f4db8",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"SeleniumURLLoader\">\n",
    "    \n",
    "### 2.3. SeleniumURLLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc1bad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q unstructured selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6771b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.boost.ai/blog/llms-large-language-models\",\n",
    "    \"https://www.youtube.com/watch?v=6Zv6A_9urh4&t=112s\"\n",
    "]\n",
    "\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "data = loader.load() # .load() returns the list of document instances containing 'page_content' and 'metadata'\n",
    "\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b72fa",
   "metadata": {},
   "source": [
    "The `SeleniumURLLoader` class has the following attributes:\n",
    "- `urls` (List[str]): List of URLs to load from;\n",
    "- `continue_on_failure` (bool, default=True): If set to True, continues loading other URLs on failure;\n",
    "- `browser` (str, default=\"chrome\"): Browser selection, either 'Chrome' or 'Firefox';\n",
    "- `executable_path` (Optional[str], default=None): Browser executable path;\n",
    "- `headless` (bool, default=True): Browser runs in headless mode if True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13414b6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"GoogleDriveLoader\">\n",
    "    \n",
    "### 2.4. GoogleDriveLoader\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1947651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GoogleDriveLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d07a9d4",
   "metadata": {},
   "source": [
    "By default, the GoogleDriveLoader searches for the \"credentials.json\" file in \"~/.credentials/credentials.json\". Use the `credentials_file` keyword argument to modify this path.\n",
    "The \"token.json\" file follows the same principle and will be created automatically upon the loader's first use.\n",
    "\n",
    "Steps to set up the `credentials_file`:\n",
    "\n",
    "1. Create a new Google Cloud Platform project (or use an existing one) by visiting the Google Cloud Console. Ensure that billing is enabled for your project.\n",
    "2. Enable the Google Drive API by navigating to its dashboard in the Google Cloud Console and clicking \"Enable.\"\n",
    "3. Create a service account by going to the Service Accounts page in the Google Cloud Console. Follow the prompts to set up a new service account.\n",
    "4. Assign necessary roles to the service account, such as \"Google Drive API - Drive File Access\" and \"Google Drive API - Drive Metadata Read/Write Access,\" depending on your needs.\n",
    "5. After creating the service account, access the \"Actions\" menu next to it, select \"Manage keys,\" click \"Add Key,\" and choose \"JSON\" as the key type. This generates a JSON key file and downloads it to your computer, which serves as your credentials_file.\n",
    "\n",
    "To retrieve the folder or document ID from the URL:\n",
    "- Folder: https://drive.google.com/drive/u/0/folders/{folder_id}\n",
    "- Document: https://docs.google.com/document/d/{document_id}/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a33117cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GoogleDriveLoader(\n",
    "    folder_id=\"the_folder_id\",\n",
    "    recursive=False  # default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14b728d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45728d03",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"splitters\">\n",
    "    \n",
    "## 3. Text Splitters\n",
    "    \n",
    "</a>\n",
    "\n",
    "\n",
    "The length of the contents may vary depending on their source and may exceed the input window size of the model. Splitting the large text into smaller segments allows to use the most relevant chunk as the context instead of expecting the model to comprehend entire documents. \n",
    "\n",
    "Using a Text Splitter can also improve vector store search results: smaller segments might be more likely to match a query. That is why experimenting with different chunk sizes and overlaps can be beneficial.\n",
    "\n",
    "Two primary dimensions to consider:\n",
    "- the way the text is split\n",
    "- the way the chunk size (and overlap size) is measured\n",
    "\n",
    "No universal approach for chunking text will fit all scenarios - what's effective for one case might not be suitable for another. \n",
    "\n",
    "Main steps when using text splitters:\n",
    "\n",
    "1. Clean up data, get rid of anything unnecessary, like HTML tags.\n",
    "2. Divide the text into small, semantically meaningful chunks (often sentences).\n",
    "3. Combine these small chunks into a larger one until a specific size is reached (determined by a particular function).\n",
    "4. Once the desired size is attained, separate that chunk as an individual piece of text, then start forming a new chunk with some overlap to maintain context between segments.\n",
    "5. Run some queries to test the chosen chunk size.\n",
    "5. Repeat steps 2-5 to test a few different chunk sizes\n",
    "6. Compare results (the best size will depend on what kind of data and model are used). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891bd66",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"Character_Splitter\">\n",
    "    \n",
    "### 3.1. Character Text Splitter\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d2bbe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF file has been taken here: \n",
    "# https://assets.super.so/0d43acc6-3340-4118-9744-d03a54c41788/files/d3bbdb7c-5122-4066-800c-f44e043ecf43.pdf\n",
    "loader = PyPDFLoader(\"data/linux_manual.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4837e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 2 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "#print(texts[0])\n",
    "\n",
    "print (f\"You have {len(texts)} documents\")\n",
    "\n",
    "#print (\"Preview:\", texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77332d91",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"Recursive_Splitter\">\n",
    "    \n",
    "### 3.2. Recursive Character Text Splitter\n",
    "    \n",
    "</a>\n",
    "\n",
    "The Recursive Splitter allows to split the text into chunks based on a list of characters provided. It attempts to split text using the characters from a list in that particular order until the resulting chunks are small enough. \n",
    "\n",
    "The default list of characters is **`[\"\\n\\n\", \"\\n\", \" \", \"]`**, meaning the splitter tries to keep paragraphs, sentences, and words together as long as possible, as they are generally the most semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "955e1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"data/article.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "228fb75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the RecursiveCharacterTextSplitter class with the desired parameters,\n",
    "# the default list of characters to split by is [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e15681db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Simulated Annealing in Early Layers Leads to' metadata={}\n",
      "page_content='Leads to Better Generalization' metadata={}\n",
      "page_content='Amir M. Sarfi1,2Zahra Karimpour1Muawiz' metadata={}\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([pages[0].page_content])\n",
    "\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "print(texts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df40012",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"NLTK_Splitter\">\n",
    "    \n",
    "### 3.3. NLTK Text Splitter\n",
    "    \n",
    "</a>\n",
    "\n",
    "The NLTKTextSplitter in LangChain uses the Natural Language Toolkit (NLTK) library to split text based on tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "733f9d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n",
      "Simulated Annealing in Early Layers Leads to Better Generalization\n",
      "Amir M. Sarfi1,2Zahra Karimpour1Muawiz Chaudhary1,2Nasir M. Khalid1,2\n",
      "Mirco Ravanelli1,2Sudhir Mudur1Eugene Belilovsky1,2\n",
      "1Concordia University2Mila – Quebec AI Institute\n",
      "Abstract\n",
      "Recently, a number of iterative learning methods have\n",
      "been introduced to improve generalization.\n",
      "\n",
      "These typically\n",
      "rely on training for longer periods of time in exchange for\n",
      "improved generalization.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500)\n",
    "texts = text_splitter.split_text(pages[0].page_content)\n",
    "\n",
    "\n",
    "# Print the length of the first chunk and the chunk\n",
    "print(len(texts[0]))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84e19c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"Spacy_Splitter\">\n",
    "    \n",
    "### 3.4. Spacy Text Splitter\n",
    "    \n",
    "</a>\n",
    "\n",
    "SpacyTextSplitter leverages the popular SpaCy library to split texts based on linguistic features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b644c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import en_core_web_sm\n",
    "#en_core_web_sm.load(disable=[\"tagger\", \"ner\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91409efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n",
      "Simulated Annealing in Early Layers Leads to Better Generalization\n",
      "Amir M. Sarfi1,2Zahra Karimpour1Muawiz Chaudhary1,2Nasir M. Khalid1,2\n",
      "Mirco Ravanelli1,2Sudhir Mudur1Eugene Belilovsky1,2\n",
      "1Concordia University2Mila – Quebec AI Institute\n",
      "Abstract\n",
      "Recently, a number of iterative learning methods have\n",
      "been introduced to improve generalization.\n",
      "\n",
      "These typically\n",
      "rely on training for longer periods of time in exchange for\n",
      "improved generalization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iryna/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "# Instantiate the SpacyTextSplitter with the desired chunk size\n",
    "text_splitter = SpacyTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "# Split the text using SpacyTextSplitter\n",
    "texts = text_splitter.split_text(pages[0].page_content)\n",
    "\n",
    "# Print the length of the first chunk and the chunk\n",
    "print(len(texts[0]))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc3b215",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"Markdown_Splitter\">\n",
    "    \n",
    "### 3.5. Markdown Text Splitter\n",
    "    \n",
    "</a>\n",
    "\n",
    "The `MarkdownTextSplitter` is used to split text written using Markdown languages like headers, code blocks, or dividers. It is implemented as a simple subclass of `RecursiveCharacterSplitter` with Markdown-specific separators and helps divide text while preserving the structure and meaning provided by Markdown formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55809a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# \n",
    "\n",
    "# Welcome to My Blog!\n",
    "\n",
    "## Introduction\n",
    "Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python, Java, and JavaScript.\n",
    "\n",
    "Here's a list of my favorite programming languages:\n",
    "\n",
    "1. Python\n",
    "2. JavaScript\n",
    "3. Java\n",
    "\n",
    "You can check out some of my projects on [GitHub](https://github.com).\n",
    "\n",
    "## About this Blog\n",
    "In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n",
    "\n",
    "Here's a small piece of Python code to say hello:\n",
    "\n",
    "\\``` python\n",
    "def say_hello(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "say_hello(\"John\")\n",
    "\\```\n",
    "\n",
    "Stay tuned for more updates!\n",
    "\n",
    "## Contact Me\n",
    "Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at johndoe@email.com.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c24747a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='# \\n\\n# Welcome to My Blog!' metadata={}\n",
      "page_content='## Introduction' metadata={}\n",
      "page_content='Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python,' metadata={}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = markdown_splitter.create_documents([markdown_text])\n",
    "\n",
    "print(docs[0])\n",
    "print(docs[1])\n",
    "print(docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b11de",
   "metadata": {},
   "source": [
    "**Note**: The default separators that are determined by the Markdown syntax can be customized when MarkdownTextSplitter instance is initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73ed98",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"Token_Splitter\">\n",
    "    \n",
    "### 3.6. Token Text Splitter\n",
    "    \n",
    "</a>\n",
    "\n",
    "TokenTextSplitter respects the token boundaries, ensuring that the chunks do not split tokens in the middle. This type of splitter breaks down raw text strings into smaller pieces by initially converting the text into BPE (Byte Pair Encoding) tokens, and subsequently dividing these tokens into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c48a78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Annealing in Early Layers Leads to Better Generalization\n",
      "Amir M. Sarfi1,2Zahra Karimpour1Muawiz Chaudhary1,2Nasir M. Khalid1,2\n",
      "Mirco Ravanelli1,2Sudhir Mudur1Eugene Belilovsky1,2\n",
      "1Concordia University2Mila – Quebec AI Institute\n",
      "Abstract\n",
      "Recently, a number of iterative learning\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Initialize the TokenTextSplitter with desired chunk size and overlap\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "\n",
    "# Split into smaller chunks\n",
    "texts = text_splitter.split_text(pages[0].page_content)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afce34e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"embeddings\">\n",
    "    \n",
    "## 4. Embeddings\n",
    "    \n",
    "</a>\n",
    "\n",
    "\n",
    "LLMs can transform textual data into embedding space, allowing for versatile representations across languages.  \n",
    "\n",
    "Embeddings are high-dimensional vectors that capture semantic information. Embeddings also serve to identify relevant information by quantifying the distance between data points (by indicating closer semantic meaning for points being closer together).\n",
    "\n",
    "The LangChain integration provides necessary functions for both transforming and calculating similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8485d4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"sim_search\">\n",
    "    \n",
    "### 4.1. Similarity search and vector embeddings \n",
    "    \n",
    "</a>\n",
    "\n",
    "GPT-3 is a powerful language model offered by OpenAI. It can be used for various tasks, such as generating embeddings and performing similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ae50eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1336958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat on the mat.\",\n",
    "    \"The dog is in the yard.\",\n",
    "    \"There is a dog in the yard.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3152809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAIEmbeddings instance\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = embeddings.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e3b7eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar document to the query 'A cat is sitting on a mat.':\n",
      "The cat is on the mat.\n"
     ]
    }
   ],
   "source": [
    "# Perform a similarity search for a given query\n",
    "query = \"A cat is sitting on a mat.\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "# Find the most similar document\n",
    "most_similar_index = np.argmax(similarity_scores)\n",
    "most_similar_document = documents[most_similar_index]\n",
    "\n",
    "print(f\"Most similar document to the query '{query}':\")\n",
    "print(most_similar_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f0f7bd",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"emb_models\">\n",
    "    \n",
    "### 4.2. Embedding Models\n",
    "    \n",
    "</a>\n",
    "\n",
    "Embedding models are ML models that convert discrete data into continuous vectors. In the context of NLP, these discrete data points can be words, sentences, or even entire documents. The generated vectors (embeddings) are supposed to capture the semantic meaning of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2da43f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16998107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "63d4be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, \n",
    "                           model_kwargs=model_kwargs)\n",
    "\n",
    "documents = [\"It is really hot in Portugal during August\", \n",
    "             \"Summertime is too hot in Lisbon\", \n",
    "             \"Something else here\"]\n",
    "\n",
    "doc_embeddings = hf.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48bc50",
   "metadata": {},
   "source": [
    "The obtained embeddings (doc_embeddings) are ready for any downstream tasks: classification, clustering, or similarity analysis. They represent our original documents in a form that machines can understand and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eef4ed9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70272776]]\n",
      "[[0.0454873]]\n",
      "[[0.04992708]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([doc_embeddings[0]], [doc_embeddings[1]]))\n",
    "print(cosine_similarity([doc_embeddings[0]], [doc_embeddings[2]]))\n",
    "print(cosine_similarity([doc_embeddings[1]], [doc_embeddings[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8cb179",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"cohere\">\n",
    "    \n",
    "### 4.3. Cohere embeddings\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e624c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import COHERE_API_KEY ## API key obtained from https://dashboard.cohere.ai/api-keys\n",
    "\n",
    "# !pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80adc24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from langchain.embeddings import CohereEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0e0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CohereEmbeddings object\n",
    "cohere = CohereEmbeddings(\n",
    "    model=\"embed-multilingual-v2.0\",\n",
    "    cohere_api_key=COHERE_API_KEY\n",
    ")\n",
    "\n",
    "# Define a list of texts\n",
    "texts = [\n",
    "    \"Hello from Cohere!\", \n",
    "    \"مرحبًا من كوهير!\", \n",
    "    \"Hallo von Cohere!\",  \n",
    "    \"Bonjour de Cohere!\", \n",
    "    \"¡Hola desde Cohere!\", \n",
    "    \"Olá do Cohere!\",  \n",
    "    \"Ciao da Cohere!\", \n",
    "    \"您好，来自 Cohere！\", \n",
    "    \"कोहेरे से नमस्ते!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc76d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello from Cohere!\n",
      "Embedding: [0.23449707, 0.50097656, -0.04876709, 0.14001465, -0.1796875, 0.39575195, 0.2836914]\n",
      "Text: مرحبًا من كوهير!\n",
      "Embedding: [0.25341797, 0.30004883, 0.01083374, 0.12573242, -0.1821289, 0.39160156, 0.31201172]\n",
      "Text: Hallo von Cohere!\n",
      "Embedding: [0.10205078, 0.28320312, -0.0496521, 0.2364502, -0.0715332, 0.34643555, 0.34179688]\n",
      "Text: Bonjour de Cohere!\n",
      "Embedding: [0.15161133, 0.28222656, -0.057281494, 0.11743164, -0.044189453, 0.29467773, 0.29125977]\n",
      "Text: ¡Hola desde Cohere!\n",
      "Embedding: [0.25146484, 0.43139648, -0.08642578, 0.24682617, -0.117004395, 0.29956055, 0.33691406]\n",
      "Text: Olá do Cohere!\n",
      "Embedding: [0.18676758, 0.390625, -0.04550171, 0.14562988, -0.11230469, 0.25732422, 0.34716797]\n",
      "Text: Ciao da Cohere!\n",
      "Embedding: [0.11590576, 0.4333496, -0.025772095, 0.14538574, 0.0703125, 0.11468506, 0.31713867]\n",
      "Text: 您好，来自 Cohere！\n",
      "Embedding: [0.24645996, 0.3083496, -0.111816406, 0.26586914, -0.05102539, 0.17785645, 0.36328125]\n",
      "Text: कोहेरे से नमस्ते!\n",
      "Embedding: [0.19274902, 0.6352539, 0.031951904, 0.117370605, -0.26098633, 0.18774414, 0.18652344]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for the texts\n",
    "document_embeddings = cohere.embed_documents(texts)\n",
    "\n",
    "# Print the embeddings\n",
    "for text, embedding in zip(texts, document_embeddings):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Embedding: {embedding[:7]}\")  # print first 7 dimensions of each embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a086f4ec",
   "metadata": {},
   "source": [
    "Given a list of multilingual texts, the embed_documents() method in LangChain's CohereEmbeddings class, connected to Cohere’s embedding endpoint and generated unique semantic embeddings for each text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8a530",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"vector_store\">\n",
    "    \n",
    "### 4.4. DeepLake Vector Store Embeddings\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24065059",
   "metadata": {},
   "source": [
    "DeepLake is a Vector Store for creating, storing, and querying vector representations (also known as embeddings) of data.\n",
    "\n",
    "The pipeline below demonstrates how to leverage the power of the LangChain, OpenAI, and Deep Lake libraries and products to create a conversational AI model capable of retrieving and answering questions based on the content of a given repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "178cc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import OPENAI_API_KEY, ACTIVELOOP_TOKEN\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db045258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3db4dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d5b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\",\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "441bf279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://iryna/course_embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['9b4f40c6-3a34-11ee-aa34-12ee7aa5dbdc',\n",
       " '9b4f4486-3a34-11ee-aa34-12ee7aa5dbdc',\n",
       " '9b4f4558-3a34-11ee-aa34-12ee7aa5dbdc',\n",
       " '9b4f45d0-3a34-11ee-aa34-12ee7aa5dbdc']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Create DeepLake dataset\n",
    "my_activeloop_org_id = \"iryna\"\n",
    "my_activeloop_dataset_name = \"course_embeddings\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding=embeddings)\n",
    "\n",
    "# Add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "910745b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from db\n",
    "retriever = db.as_retriever() # transforming the DeepLake dataset into a LangChain retriever object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fabef0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael Jordan was born on 17 February 1963.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a RetrievalQA chain and run it\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo')  ## istantiate the LLM wrapper \n",
    "qa_chain = RetrievalQA.from_llm(model, retriever=retriever) ## create the question-answering chain\n",
    "qa_chain.run(\"When was Michael Jordan born?\") ## ask a question to the chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ead2c",
   "metadata": {},
   "source": [
    "**Note**: The underlying OpenAI model in this chain is responsible for both the question embedding and the answer generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10b3ca",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"cs\">\n",
    "    \n",
    "## 5. Customer Support Question Answering Chatbot\n",
    "    \n",
    "</a>\n",
    "\n",
    "When users interact with the chatbot, their queries are matched to the most similar intent, generating the associated response. As LLMs continue to evolve, chatbot development is shifting toward more sophisticated and dynamic solutions capable of handling a broader range of user inquiries with greater precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587c329",
   "metadata": {},
   "source": [
    "Let's demonstrate how to use a **website's content as supplementary context for a chatbot** to respond to user queries effectively. \n",
    "\n",
    "The code implementation below involves:\n",
    "- employing data loaders to scrape some content from online articles;\n",
    "- employing data splitters to split content into small chunks;\n",
    "- computting and storing the corresponding embeddings in the Deep Lake dataset; \n",
    "- retrieving the most relevant documents corresponding to the user's question.\n",
    "\n",
    "**Note**: There is always a risk of generating hallucinations or false information when using LLMs. It might not be acceptable for many customers support use cases, however the chatbot can still be helpful for assisting operators in drafting answers that they can double-check before sending them to the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96c69fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unstructured selenium\n",
    "#!pip install langchain==0.0.208 deeplake openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e58c76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83cd8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our chatbot will use information from the following articles\n",
    "urls = ['https://beebom.com/what-is-nft-explained/',\n",
    "        'https://beebom.com/how-delete-spotify-account/',\n",
    "        'https://beebom.com/how-download-gif-twitter/',\n",
    "        'https://beebom.com/how-delete-spotify-account/',\n",
    "        'https://beebom.com/how-save-instagram-story-with-music/',\n",
    "        'https://beebom.com/how-install-pip-windows/',\n",
    "        'https://beebom.com/how-check-disk-usage-linux/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb898572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape and split the documents into chunks\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "docs_not_splitted = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(docs_not_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdd3bb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://iryna/langchain_course_customer_support', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (102, 1536)  float32   None   \n",
      "    id        text      (102, 1)      str     None   \n",
      " metadata     json      (102, 1)      str     None   \n",
      "   text       text      (102, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c3726fb6-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37271fa-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372724a-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727290-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37272cc-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727308-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727344-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727380-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37273b2-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37273ee-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372742a-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727466-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37274a2-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37274d4-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727510-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727556-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727592-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37275c4-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372760a-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372763c-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727678-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37276b4-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37276e6-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727722-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372775e-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372779a-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37277d6-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727812-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372784e-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727880-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37278bc-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37278ee-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727920-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372795c-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727998-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37279ca-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727a06-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727a42-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727a7e-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727aba-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727af6-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727b32-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727b6e-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727baa-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727bdc-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727c18-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727c4a-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727c86-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727cb8-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727cf4-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727d26-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727d62-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727d9e-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727dda-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727e16-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727e5c-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727e98-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727eca-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727f06-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727f38-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727f74-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727fa6-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3727fe2-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728014-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728046-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728186-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37281cc-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728208-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372823a-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728276-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37282a8-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37282e4-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728320-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728352-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728398-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37283d4-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728410-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728460-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372849c-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37284d8-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728514-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728550-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372858c-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37285c8-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728604-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728636-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372867c-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37286b8-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37286f4-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728730-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c372876c-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37287a8-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37288fc-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728992-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c37289e2-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728a28-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728a64-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728aa0-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728ad2-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728b0e-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728b4a-3a47-11ee-aa34-12ee7aa5dbdc',\n",
       " 'c3728b7c-3a47-11ee-aa34-12ee7aa5dbdc']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the embeddings using OpenAIEmbeddings \n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Create DeepLake dataset (store embeddings in a DeepLake vector store on the cloud)\n",
    "my_activeloop_org_id = \"iryna\"\n",
    "my_activeloop_dataset_name = \"langchain_course_customer_support\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b0f3707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home  Tech  How to Check Disk Usage in Linux (4 Methods)\n",
      "\n",
      "How to Check Disk Usage in Linux (4 Methods)\n",
      "\n",
      "Beebom Staff\n",
      "\n",
      "Last Updated: June 19, 2023 5:14 pm\n",
      "\n",
      "There may be times when you need to download some important files or transfer some photos to your Linux system, but face a problem of insufficient disk space. You head over to your file manager to delete the large files which you no longer require, but you have no clue which of them are occupying most of your disk space. In this article, we will show some easy methods to check disk usage in Linux from both the terminal and the GUI application.\n",
      "\n",
      "Monitor Disk Usage in Linux (2023)\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Check Disk Space Using the df Command\n",
      "\t\t\n",
      "Display Disk Usage in Human Readable FormatDisplay Disk Occupancy of a Particular Type\n",
      "\n",
      "Check Disk Usage using the du Command\n",
      "\t\t\n",
      "Display Disk Usage in Human Readable FormatDisplay Disk Usage for a Particular DirectoryCompare Disk Usage of Two Directories\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the most relevant documents to a specific query\n",
    "query = \"how to check disk usage in linux?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be59d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt for a customer support chatbot\n",
    "template = \"\"\"You are an exceptional customer support chatbot that gently answer questions.\n",
    "\n",
    "You know the following context information.\n",
    "\n",
    "{chunks_formatted}\n",
    "\n",
    "Answer to the following question from a customer. Use only information from the previous context information. Do not invent stuff.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chunks_formatted\", \"query\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29d32012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You can check disk usage in Linux using the df command or by using a GUI tool such as the GDU Disk Usage Analyzer or the Gnome Disks Tool. The df command is used to check the current disk usage and the available disk space in Linux. The syntax for the df command is: df <options> <file_system>. The options to use with the df command are: a, h, t, and x. To install the GDU Disk Usage Analyzer, use the command: sudo snap install gdu-disk-usage-analyzer. To install the Gnome Disks Tool, use the command: sudo apt-get -y install gnome-disk-utility.\n"
     ]
    }
   ],
   "source": [
    "# FULL PIPELINE\n",
    "\n",
    "query = \"How to check disk usage in linux?\"  # user question\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "docs = db.similarity_search(query)\n",
    "retrieved_chunks = [doc.page_content for doc in docs]\n",
    "\n",
    "# Format the prompt\n",
    "chunks_formatted = \"\\n\\n\".join(retrieved_chunks)\n",
    "prompt_formatted = prompt.format(chunks_formatted=chunks_formatted, query=query)\n",
    "\n",
    "# Generate answer\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "answer = llm(prompt_formatted)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6c63b",
   "metadata": {},
   "source": [
    "**Note**: it can be challenging to ensure that the model generates answers solely based on the context, as it has a tendency to generate new, potentially false information. Generating false information might have various severity levels depending on the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ae7c4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"resources\">\n",
    "    \n",
    "## 6. Additional Resources\n",
    "</a>\n",
    "\n",
    "- [Improving Document Retrieval with Contextual Compression](https://blog.langchain.dev/improving-document-retrieval-with-contextual-compression/)\n",
    "- [Split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter)\n",
    "- [Split code](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter)\n",
    "- [Recursively split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)\n",
    "- [Chatbot + Knowledge Base](https://learnprompting.org/docs/applied_prompting/build_chatbot_from_kb)\n",
    "- [Conversation Intelligence: Gong.io Open-Source Alternative AI Sales Assistant](https://www.activeloop.ai/resources/conversation-intelligence-gong-io-open-source-alternative-ai-sales-assistant/)\n",
    "- [AI Story Generator: OpenAI Function Calling, LangChain, & Stable Diffusion](https://www.activeloop.ai/resources/ai-story-generator-open-ai-function-calling-lang-chain-stable-diffusion/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
