{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1790f12",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "Prompt engineering is a powerful technique that can help to optimize language models for various applications and research topics. By creating good prompts, the model can be guided to deliver accurate, contextually relevant, and insightful responses.\n",
    "\n",
    "* [1. Intro to Prompting](#intro)\n",
    "    * [1.1. Role Prompting](#role)\n",
    "    * [1.2. Few Shot Prompting](#few_shot)\n",
    "    * [1.3. Bad Prompt Practices](#bad) \n",
    "    * [1.4. Chain Prompting](#chain)\n",
    "    * [1.5. Well-Structured Prompt Example](#well)\n",
    "* [2. Using Prompt Templates](#templates)\n",
    "    * [2.1. PromptTemplate](#prompttemp)\n",
    "    * [2.2. FewShotPromptTemplate](#fewshottemp)\n",
    "    * [2.3. Saving PromptTemplate and Loading it back](#savetemp)\n",
    "    * [2.4. Example: Teach LLM to Respond Sarcastically](#sarcastic)\n",
    "    * [2.5. LengthBasedExampleSelector](#example_selector)\n",
    "    * [2.6. Alternating Human/AI messages](#human_ai)\n",
    "    * [2.7. SemanticSimilarityExampleSelector](#semantic)\n",
    "* [3. Managing Outputs with Output Parsers](#outputs)    \n",
    "* [4. Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98da2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271229c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"intro\">\n",
    "    \n",
    "## 1. Intro to Prompting\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d6c3b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"role\">\n",
    "    \n",
    "### 1.1 Role Prompting\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7741d",
   "metadata": {},
   "source": [
    "Role prompting involves asking the LLM to assume a specific role or identity before performing a given task, such as acting as a copywriter. This can help guide the model's response by providing a context or perspective for the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d680bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "template = \"\"\"\n",
    "As a futuristic robot band conductor, I need you to help me come up with a song title.\n",
    "What's a cool song title for a song about {theme} in the year {year}?\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"theme\", \"year\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Create LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f63999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data for the prompt\n",
    "input_data = {\"theme\": \"interstellar travel\", \"year\": \"3030\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0913f2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: interstellar travel\n",
      "Year: 3030\n",
      "AI-generated song title: \n",
      "\"Journey to the Stars: 3030\"\n"
     ]
    }
   ],
   "source": [
    "# Run the LLMChain to get the AI-generated song title\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Theme:\", input_data[\"theme\"])\n",
    "print(\"Year:\", input_data[\"year\"])\n",
    "print(\"AI-generated song title:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0351c2ee",
   "metadata": {},
   "source": [
    "The provided prompt is a good example of role prompting  for several reasons:\n",
    "\n",
    "- Clear instructions\n",
    "- Specificity\n",
    "- Open-ended creativity\n",
    "- Focus on the task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892dccc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"few_shot\">\n",
    "    \n",
    "### 1.2. Few Shot Prompting\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16312c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e986ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
    "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
    "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
    "]\n",
    "\n",
    "example_formatter_template = \"\"\"\n",
    "Color: {color}\n",
    "Emotion: {emotion}\\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"color\", \"emotion\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Here are some examples of colors and the emotions associated with them:\\n\\n\",\n",
    "    suffix=\"\\n\\nNow, given a new color, identify the emotion associated with it:\\n\\nColor: {input}\\nEmotion:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ded862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: purple\n",
      "Emotion:  creativity\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = few_shot_prompt.format(input=\"purple\")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate(template=formatted_prompt, input_variables=[]))\n",
    "\n",
    "# Run the LLMChain to get the AI-generated emotion associated with the input color\n",
    "response = chain.run({})\n",
    "\n",
    "print(\"Color: purple\")\n",
    "print(\"Emotion:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33dd6411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: black\n",
      "Emotion:  power\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt2 = few_shot_prompt.format(input=\"black\")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate(template=formatted_prompt2, input_variables=[]))\n",
    "\n",
    "# Run the LLMChain to get the AI-generated emotion associated with the input color\n",
    "response = chain.run({})\n",
    "\n",
    "print(\"Color: black\")\n",
    "print(\"Emotion:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c293cea",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"bad\">\n",
    "    \n",
    "### 1.3. Bad Prompt Practices\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a285fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me something about Europe.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of a too-vague prompt that provides little context or guidance \n",
    "# for the model to generate a meaningful response\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"Tell me something about {topic}.\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=template,\n",
    ")\n",
    "prompt.format(topic=\"Europe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca42556",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"chain\">\n",
    "    \n",
    "### 1.4. Chain Prompting\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff5d97a",
   "metadata": {},
   "source": [
    "Chain Prompting refers to the practice of chaining consecutive prompts, where the output of a previous prompt becomes the input of the successive prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e54900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "227f5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1\n",
    "template_question = \"\"\"What is the name of the famous scientist who developed the theory of general relativity?\n",
    "Answer: \"\"\"\n",
    "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
    "\n",
    "# Prompt 2\n",
    "template_fact = \"\"\"Provide a brief description of {scientist}'s theory of general relativity.\n",
    "Answer: \"\"\"\n",
    "prompt_fact = PromptTemplate(input_variables=[\"scientist\"], template=template_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b787bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientist: Albert Einstein\n",
      "Fact: \n",
      "Albert Einstein's theory of general relativity is a theory of gravitation that states that the gravitational force between two objects is a result of the curvature of spacetime caused by the presence of mass and energy. It explains the phenomenon of gravity as a result of the warping of space and time by matter and energy.\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0) \n",
    "\n",
    "# Create the LLMChain for the first prompt\n",
    "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
    "\n",
    "# Run the LLMChain for the first prompt with an empty dictionary\n",
    "response_question = chain_question.run({})\n",
    "\n",
    "# Extract the scientist's name from the response\n",
    "scientist = response_question.strip()\n",
    "\n",
    "# Create the LLMChain for the second prompt\n",
    "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
    "\n",
    "# Input data for the second prompt\n",
    "input_data = {\"scientist\": scientist}\n",
    "\n",
    "# Run the LLMChain for the second prompt\n",
    "response_fact = chain_fact.run(input_data)\n",
    "\n",
    "print(\"Scientist:\", scientist)\n",
    "print(\"Fact:\", response_fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1097e43",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"well\">\n",
    "    \n",
    "### 1.5. Well-Structured Prompt Example\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9d0b9",
   "metadata": {},
   "source": [
    "Tips to engineer effective prompts:\n",
    "- Be specific (provide detail and enough context to guide the LLM toward the desired output);\n",
    "- Force conciseness when needed;\n",
    "- Encourage the model to explain its reasoning (this way the results can be more accurate, especially for complex tasks);\n",
    "- Iterate if needed (several refinements may be required to obtain the best possible answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bcd821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What are some tips for improving communication skills?\n",
      "AI Response:  Practice active listening, be mindful of your body language, and be open to constructive feedback.\n"
     ]
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate, PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the secret to happiness?\",\n",
    "        \"answer\": \"Finding balance in life and learning to enjoy the small moments.\"\n",
    "    }, {\n",
    "        \"query\": \"How can I become more productive?\",\n",
    "        \"answer\": \"Try prioritizing tasks, setting goals, and maintaining a healthy work-life balance.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "life coach. The assistant provides insightful and practical advice to the users' questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the few-shot prompt template\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "\n",
    "# Define the user query\n",
    "user_query = \"What are some tips for improving communication skills?\"\n",
    "\n",
    "# Run the LLMChain for the user query\n",
    "response = chain.run({\"query\": user_query})\n",
    "\n",
    "print(\"User Query:\", user_query)\n",
    "print(\"AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe05cbc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"templates\">\n",
    "    \n",
    "## 2. Using Prompt Templates\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df310c31",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"prompttemp\">\n",
    "    \n",
    "### 2.1. PromptTemplate\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c78246f",
   "metadata": {},
   "source": [
    "A PromptTemplate is a predefined structure or pattern used to construct effective and consistent prompts for large language models. It is a guideline to ensure the input text or prompt is properly formatted.\n",
    "\n",
    "To create a `PromptTemplate` object, two arguments are required:\n",
    "\n",
    "1. `input_variables`: A list of variable names in the template;\n",
    "2. `template`: The template string containing formatted text and placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bd653e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main advantage of quantum computing over classical computing?\n",
      "Answer:  The main advantage of quantum computing over classical computing is its ability to solve complex problems faster.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided, answer\n",
    "with \"I don't know\".\n",
    "Context: Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\n",
    "...\n",
    "Question: {query}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Set the query you want to ask\n",
    "input_data = {\"query\": \"What is the main advantage of quantum computing over classical computing?\"}\n",
    "\n",
    "# Run the LLMChain to get the AI-generated answer\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Question:\", input_data[\"query\"])\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49022e",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"fewshottemp\">\n",
    "    \n",
    "### 2.2. FewShotPromptTemplate\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bb6bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tropical forests and mangrove swamps\n"
     ]
    }
   ],
   "source": [
    "# For more advanced usage, it is possible to create a FewShotPromptTemplate \n",
    "# with an ExampleSelector to select a subset of examples that will be most informative for the language model.\n",
    "\n",
    "from langchain import LLMChain, FewShotPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\"animal\": \"lion\", \"habitat\": \"savanna\"},\n",
    "    {\"animal\": \"polar bear\", \"habitat\": \"Arctic ice\"},\n",
    "    {\"animal\": \"elephant\", \"habitat\": \"African grasslands\"}\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Animal: {animal}\n",
    "Habitat: {habitat}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"animal\", \"habitat\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Identify the habitat of the given animal\",\n",
    "    suffix=\"Animal: {input}\\nHabitat:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the dynamic_prompt\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"input\": \"tiger\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4256dd",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"savetemp\">\n",
    "    \n",
    "### 2.3.  Saving PromptTemplate and Loading it back\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82c7060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PromptTemplate to a JSON file (or YAML file)\n",
    "prompt_template.save(\"output/my_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14483a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] output_parser=None partial_variables={} template='Answer the question based on the context below. If the\\nquestion cannot be answered using the information provided, answer\\nwith \"I don\\'t know\".\\nContext: Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\\n...\\nQuestion: {query}\\nAnswer: ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# Loading back\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "loaded_prompt = load_prompt(\"output/my_prompt.json\")\n",
    "print(loaded_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091495f1",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sarcastic\">\n",
    "    \n",
    "### 2.4. Example: Teach LLM to Respond Sarcastically\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "666a2e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Start by learning the language of the birds. Once you've mastered that, you'll be well on your way to mastering natural language processing.\n"
     ]
    }
   ],
   "source": [
    "# Teach the LLM by providing examples to respond sarcastically to questions\n",
    "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do I become a better programmer?\",\n",
    "        \"answer\": \"Try talking to a rubber duck; it works wonders.\"\n",
    "    }, {\n",
    "        \"query\": \"Why is the sky blue?\",\n",
    "        \"answer\": \"It's nature's way of preventing eye strain.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI assistant. \n",
    "The assistant is typically sarcastic and witty, producing creative and funny \n",
    "responses to users' questions. Here are some examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the few_shot_prompt_template\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"query\": \"How can I learn natural language processing?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29737b63",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"example_selector\">\n",
    "    \n",
    "### 2.5. LengthBasedExampleSelector\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91948284",
   "metadata": {},
   "source": [
    "It is essential to optimize the performance of few-shot learning, providing the model with as many relevant examples as possible while staying within the limits of the maximum context window and avoiding excessive processing times. The dynamic inclusion or exclusion of examples allows us to achieve a balance between providing sufficient context and maintaining efficiency in the model's operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c8c1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do you feel today?\",\n",
    "        \"answer\": \"As an AI, I don't have feelings, but I've got jokes!\"\n",
    "    }, {\n",
    "        \"query\": \"What is the speed of light?\",\n",
    "        \"answer\": \"Fast enough to make a round trip around Earth 7.5 times in one second!\"\n",
    "    }, {\n",
    "        \"query\": \"What is a quantum computer?\",\n",
    "        \"answer\": \"A magical box that harnesses the power of subatomic particles to solve complex problems.\"\n",
    "    }, {\n",
    "        \"query\": \"Who invented the telephone?\",\n",
    "        \"answer\": \"Alexander Graham Bell, the original 'ringmaster'.\"\n",
    "    }, {\n",
    "        \"query\": \"What programming language is best for AI development?\",\n",
    "        \"answer\": \"Python, because it's the only snake that won't bite.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris, the city of love and baguettes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is photosynthesis?\",\n",
    "        \"answer\": \"A plant's way of saying 'I'll turn this sunlight into food. You're welcome, Earth.'\"\n",
    "    }, {\n",
    "        \"query\": \"What is the tallest mountain on Earth?\",\n",
    "        \"answer\": \"Mount Everest, Earth's most impressive bump.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the most abundant element in the universe?\",\n",
    "        \"answer\": \"Hydrogen, the basic building block of cosmic smoothies.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the largest mammal on Earth?\",\n",
    "        \"answer\": \"The blue whale, the original heavyweight champion of the world.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the fastest land animal?\",\n",
    "        \"answer\": \"The cheetah, the ultimate sprinter of the animal kingdom.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the square root of 144?\",\n",
    "        \"answer\": \"12, the number of eggs you need for a really big omelette.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the average temperature on Mars?\",\n",
    "        \"answer\": \"Cold enough to make a Martian wish for a sweater and a hot cocoa.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a147109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of utilizing the examples list of dictionaries directly, let's implement a LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9c2b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To dynamically select and include examples based on their length, \n",
    "# (ensuring that the final prompt stays within the desired token limit)\n",
    "\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, \n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a2d8d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander Graham Bell, the person who wanted to make sure nobody would ever be able to have a peaceful dinner again.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create the LLMChain for the dynamic_prompt_template\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt_template)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"query\": \"Who invented the telephone?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cef20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example of using LengthBasedExampleSelector\n",
    "\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\n",
    "    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\n",
    "    {\"word\": \"windy\", \"antonym\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Create an instance of LengthBasedExampleSelector\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25,\n",
    ")\n",
    "\n",
    "# Create a FewShotPromptTemplate\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b83efc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "\n",
      "\n",
      "Word: energetic\n",
      "Antonym: lethargic\n",
      "\n",
      "\n",
      "\n",
      "Word: sunny\n",
      "Antonym: gloomy\n",
      "\n",
      "\n",
      "Word: big\n",
      "Antonym:\n",
      "small\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt.format(input=\"big\"))\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
    "response = chain.run(input=\"big\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50798b2",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"human_ai\">\n",
    "    \n",
    "### 2.6. Alternating Human/AI messages\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d2206bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I be lovin' the art of code plunderin'.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "template=\"You are a helpful assistant that translates english to pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
    "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, \n",
    "                                                example_human, \n",
    "                                                example_ai, \n",
    "                                                human_message_prompt])\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(\"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341ae7f",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"semantic\">\n",
    "    \n",
    "### 2.7. SemanticSimilarityExampleSelector\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c110e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Create a PromptTemplate\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Define some examples\n",
    "examples = [\n",
    "    {\"input\": \"0°C\", \"output\": \"32°F\"},\n",
    "    {\"input\": \"10°C\", \"output\": \"50°F\"},\n",
    "    {\"input\": \"20°C\", \"output\": \"68°F\"},\n",
    "    {\"input\": \"30°C\", \"output\": \"86°F\"},\n",
    "    {\"input\": \"40°C\", \"output\": \"104°F\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4673d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding function\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "757d517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://iryna/langchain_course_fewshot_selector already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "# Create Deep Lake dataset\n",
    "\n",
    "from keys import ACTIVELOOP_TOKEN\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN\n",
    "\n",
    "my_activeloop_org_id = \"iryna\" \n",
    "my_activeloop_dataset_name = \"langchain_course_fewshot_selector\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34bdbb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in ./deeplake/ already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (27, 1536)  float32   None   \n",
      "    id        text      (27, 1)      str     None   \n",
      " metadata     json      (27, 1)      str     None   \n",
      "   text       text      (27, 1)      str     None   \n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 10°C\n",
      "Output: 50°F\n",
      "\n",
      "Input: 10°C\n",
      "Output:\n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 30°C\n",
      "Output: 86°F\n",
      "\n",
      "Input: 30°C\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Instantiate SemanticSimilarityExampleSelector using the examples\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples, embeddings, db, k=1\n",
    ")\n",
    "\n",
    "# Create a FewShotPromptTemplate using the example_selector\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Convert the temperature from Celsius to Fahrenheit\",\n",
    "    suffix=\"Input: {temperature}\\nOutput:\", \n",
    "    input_variables=[\"temperature\"],\n",
    ")\n",
    "\n",
    "# Test the similar_prompt with different inputs\n",
    "print(similar_prompt.format(temperature=\"10°C\"))  # Test with an input\n",
    "print(similar_prompt.format(temperature=\"30°C\"))  # Test with another input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7f72b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (28, 1536)  float32   None   \n",
      "    id        text      (28, 1)      str     None   \n",
      " metadata     json      (28, 1)      str     None   \n",
      "   text       text      (28, 1)      str     None   \n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 40°C\n",
      "Output: 104°F\n",
      "\n",
      "Input: 40°C\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Add a new example to the SemanticSimilarityExampleSelector\n",
    "similar_prompt.example_selector.add_example({\"input\": \"50°C\", \"output\": \"122°F\"})\n",
    "print(similar_prompt.format(temperature=\"40°C\")) # Test with a new input after adding the example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c85f6b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"outputs\">\n",
    "    \n",
    "## 3. Output Parsers\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f8b80d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"pydandic\">\n",
    "    \n",
    "### 3.1. PydanticOutputParser\n",
    "    \n",
    "</a>\n",
    "\n",
    "The main idea: the model is instructed to generate its output in a JSON format. This way we make it possible to retrieve the information from the model response (to treat the parser’s output as a list). \n",
    "\n",
    "**Note**: not all models have the same capability in terms of generating JSON outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30971406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.0.208 deeplake openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38ac6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Define the desired data structure\n",
    "# (We need a variable that can store multiple suggestions)\n",
    "\n",
    "class Suggestions(BaseModel):  # class that inherits from the Pydantic’s BaseModel class\n",
    "    \n",
    "    # 1. Expected Outputs\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "\n",
    "    # 2. Validators\n",
    "    \n",
    "    # Throw error in case of receiving a numbered-list from API\n",
    "    @validator('words') # @validator decorator must receive the same name as the variable to approve\n",
    "    def not_start_with_number(cls, field):\n",
    "        for item in field: # field is a list\n",
    "            if item[0].isnumeric():\n",
    "                raise ValueError(\"The word can not start with numbers!\")\n",
    "        return field\n",
    "\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=Suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb64af",
   "metadata": {},
   "source": [
    "Two essential parts to the class defined above:\n",
    "\n",
    "- **Expected Outputs**: Each output is defined by declaring a variable with desired type, like a list of strings (: List[str]) or a single string (: str). The Field function’s description attribute contains simple explanation to help the model during inference.\n",
    "\n",
    "- **Validators**: Functions to validate the formatting. The functions’ name are unimportant, but the @validator decorator must receive the same name as the variable to approve - like @validator(’words’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fca001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the prompt\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# The template outlines our expectations for the model, \n",
    "# including the expected formatting from the parser and the inputs. \n",
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitue the specified target_word based the presented context.\n",
    "{format_instructions}\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"], # are initialized later using .format_prompt() \n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()} # to be initialized instantly\n",
    ")\n",
    "\n",
    "model_input = prompt.format_prompt(\n",
    "    target_word=\"behaviour\",\n",
    "    context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b45da0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner', 'action', 'demeanor', 'attitude', 'activity'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Davinci model\n",
    "# setting the temperature value to 0 to make results be reproducible\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "model = OpenAI(model_name='text-davinci-003', temperature=0.0)\n",
    "output = model(model_input.to_string())\n",
    "parser.parse(output) # convert the model’s string response to the format we specified earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "684b7e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['specialist', 'expert', 'consultant', 'analyst', 'practitioner'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing another target word and another context\n",
    "model_input_2 = prompt.format_prompt(\n",
    "    target_word=\"professional\",\n",
    "    context=\"I am a  Data Science professional with 5 years of enterprise experience in Data Analysis and Machine Learning.\"\n",
    ")\n",
    "\n",
    "output_2 = model(model_input_2.to_string())\n",
    "\n",
    "parser.parse(output_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9dc583e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Outputs Example\n",
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitute the specified target_word based on the presented context and the reasoning for each word.\n",
    "{format_instructions}\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\"\n",
    "\n",
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
    "    \n",
    "    @validator('words')\n",
    "    def not_start_with_number(cls, field):\n",
    "        for item in field:\n",
    "            if item[0].isnumeric():\n",
    "                raise ValueError(\"The word can not start with numbers!\")\n",
    "        return field\n",
    "    \n",
    "    @validator('reasons')\n",
    "    def end_with_dot(cls, field):\n",
    "        for idx, item in enumerate( field ):\n",
    "            if item[-1] != \".\":\n",
    "                field[idx] += \".\"\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3afd4f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner', 'demeanor', 'comportment'], reasons=['refers to the way someone acts in a particular situation.', 'refers to the way someone behaves in a particular situation.', 'refers to the way someone behaves in a particular situation.', 'refers to the way someone behaves in a particular situation.'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Suggestions(\n",
    "    words=['conduct', 'manner', 'demeanor', 'comportment'], \n",
    "    reasons=['refers to the way someone acts in a particular situation.', \n",
    "             'refers to the way someone behaves in a particular situation.', \n",
    "             'refers to the way someone behaves in a particular situation.', \n",
    "             'refers to the way someone behaves in a particular situation.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad42f51",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"comma_separated\">\n",
    "    \n",
    "### 3.2. CommaSeparatedOutputParser\n",
    "    \n",
    "</a>\n",
    "\n",
    "It handles one specific case: to receive a list of outputs from the model. This parser does not require a setting up step. Therefore it is less flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5f06157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5271f4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Conduct',\n",
       " 'Actions',\n",
       " 'Demeanor',\n",
       " 'Mannerisms',\n",
       " 'Attitude',\n",
       " 'Performance',\n",
       " 'Reactions',\n",
       " 'Interactions',\n",
       " 'Habits',\n",
       " 'Repertoire',\n",
       " 'Disposition',\n",
       " 'Bearing',\n",
       " 'Posture',\n",
       " 'Deportment',\n",
       " 'Comportment']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Prepare the Prompt\n",
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitute the word '{target_word}' based the presented the following text: {context}.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# notice '.format()' instead of 'format_prompt.()' here\n",
    "model_input = prompt.format( \n",
    "  target_word=\"behaviour\",\n",
    "  context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
    ")\n",
    "\n",
    "# Loading OpenAI API\n",
    "model = OpenAI(model_name='text-davinci-003', temperature=0.0)\n",
    "\n",
    "# Send the Request\n",
    "output = model(model_input)\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4243ba",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"structured\">\n",
    "    \n",
    "### 3.3. StructuredOutputParser\n",
    "    \n",
    "</a>\n",
    "\n",
    "It can be used when you want to receive one response from the model (for example, only one substitute word in the thesaurus application in the previous example). This parser can process multiple outputs, but only supports texts and does not provide options for other data types, such as lists or integers.\n",
    "\n",
    "The code snippet below shows how to define a schema, but we are probably not going to use it, because this class has no advantage over PydanticOutputParser class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "259b0f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"words\", description=\"A substitue word based on context\"),\n",
    "    ResponseSchema(name=\"reasons\", description=\"the reasoning of why this word fits the context.\")\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76517dbb",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"fixing_errors\">\n",
    "    \n",
    "## 4. Fixing Errors\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fdfa22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f95ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fc3cab3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"resources\">\n",
    "    \n",
    "## 5. Aditional Resources\n",
    "    \n",
    "</a>\n",
    "\n",
    "- [A Hands-on Guide to Prompt Engineering with ChatGPT and GPT-3](https://dev.to/mmz001/a-hands-on-guide-to-prompt-engineering-with-chatgpt-and-gpt-3-4127)\n",
    "- [Prompt Engineering Tips and Tricks with GPT-3](https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/)\n",
    "- [Prompt Engineering LLMs with LangChain and W&B](https://wandb.ai/a-sh0ts/langchain_callback_demo/reports/Prompt-Engineering-LLMs-with-LangChain-and-W-B--VmlldzozNjk1NTUw)\n",
    "- [Few-Shot Prompting](https://www.promptingguide.ai/techniques/fewshot)\n",
    "- [ChatGPT Prompt Engineering Tips: Zero, One and Few Shot Prompting](https://www.allabtai.com/prompt-engineering-tips-zero-one-and-few-shot-prompting/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ebc20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
