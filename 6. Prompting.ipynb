{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1790f12",
   "metadata": {},
   "source": [
    "# Building Applications Powered by LLMs with LangChain\n",
    "\n",
    "Prompt engineering is a powerful technique that can help to optimize language models for various applications and research topics. By creating good prompts, the model can be guided to deliver accurate, contextually relevant, and insightful responses.\n",
    "\n",
    "* [1. Role Prompting](#role)\n",
    "* [2. Few Shot Prompting](#few_shot)\n",
    "* [3. Bad Prompt Practices](#bad) \n",
    "* [4. Chain Prompting](#chain)\n",
    "* [5. Well-Structured Prompt Example](#well)\n",
    "* [99. Resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98da2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d6c3b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"role\">\n",
    "    \n",
    "## 1. Role Prompting\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5b17d",
   "metadata": {},
   "source": [
    "Role prompting involves asking the LLM to assume a specific role or identity before performing a given task, such as acting as a copywriter. This can help guide the model's response by providing a context or perspective for the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d680bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "template = \"\"\"\n",
    "As a futuristic robot band conductor, I need you to help me come up with a song title.\n",
    "What's a cool song title for a song about {theme} in the year {year}?\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"theme\", \"year\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Create LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63fa00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data for the prompt\n",
    "input_data = {\"theme\": \"interstellar travel\", \"year\": \"3030\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c24842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: interstellar travel\n",
      "Year: 3030\n",
      "AI-generated song title: \n",
      "\"Journey to the Stars: 3030\"\n"
     ]
    }
   ],
   "source": [
    "# Run the LLMChain to get the AI-generated song title\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Theme:\", input_data[\"theme\"])\n",
    "print(\"Year:\", input_data[\"year\"])\n",
    "print(\"AI-generated song title:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff036483",
   "metadata": {},
   "source": [
    "The provided prompt is a good example of role prompting  for several reasons:\n",
    "\n",
    "- Clear instructions\n",
    "- Specificity\n",
    "- Open-ended creativity\n",
    "- Focus on the task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892dccc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"few_shot\">\n",
    "    \n",
    "## 2. Few Shot Prompting\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e5b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e986ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
    "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
    "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
    "]\n",
    "\n",
    "example_formatter_template = \"\"\"\n",
    "Color: {color}\n",
    "Emotion: {emotion}\\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"color\", \"emotion\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Here are some examples of colors and the emotions associated with them:\\n\\n\",\n",
    "    suffix=\"\\n\\nNow, given a new color, identify the emotion associated with it:\\n\\nColor: {input}\\nEmotion:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e46d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: purple\n",
      "Emotion:  creativity\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = few_shot_prompt.format(input=\"purple\")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate(template=formatted_prompt, input_variables=[]))\n",
    "\n",
    "# Run the LLMChain to get the AI-generated emotion associated with the input color\n",
    "response = chain.run({})\n",
    "\n",
    "print(\"Color: purple\")\n",
    "print(\"Emotion:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c293cea",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"bad\">\n",
    "    \n",
    "## 3. Bad Prompt Practices\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e74ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me something about Europe.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of a too-vague prompt that provides little context or guidance \n",
    "# for the model to generate a meaningful response\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"Tell me something about {topic}.\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=template,\n",
    ")\n",
    "prompt.format(topic=\"Europe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb9f96",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"chain\">\n",
    "    \n",
    "## 4. Chain Prompting\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d68fb",
   "metadata": {},
   "source": [
    "Chain Prompting refers to the practice of chaining consecutive prompts, where the output of a previous prompt becomes the input of the successive prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a08bd43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "626e41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1\n",
    "template_question = \"\"\"What is the name of the famous scientist who developed the theory of general relativity?\n",
    "Answer: \"\"\"\n",
    "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
    "\n",
    "# Prompt 2\n",
    "template_fact = \"\"\"Provide a brief description of {scientist}'s theory of general relativity.\n",
    "Answer: \"\"\"\n",
    "prompt_fact = PromptTemplate(input_variables=[\"scientist\"], template=template_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9220c3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientist: Albert Einstein\n",
      "Fact: \n",
      "Albert Einstein's theory of general relativity is a theory of gravitation that states that the gravitational force between two objects is a result of the curvature of spacetime caused by the presence of mass and energy. It explains the phenomenon of gravity as a result of the warping of space and time by matter and energy.\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0) \n",
    "\n",
    "# Create the LLMChain for the first prompt\n",
    "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
    "\n",
    "# Run the LLMChain for the first prompt with an empty dictionary\n",
    "response_question = chain_question.run({})\n",
    "\n",
    "# Extract the scientist's name from the response\n",
    "scientist = response_question.strip()\n",
    "\n",
    "# Create the LLMChain for the second prompt\n",
    "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
    "\n",
    "# Input data for the second prompt\n",
    "input_data = {\"scientist\": scientist}\n",
    "\n",
    "# Run the LLMChain for the second prompt\n",
    "response_fact = chain_fact.run(input_data)\n",
    "\n",
    "print(\"Scientist:\", scientist)\n",
    "print(\"Fact:\", response_fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee54eb",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"well\">\n",
    "    \n",
    "## 5. Well-Structured Prompt Example\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0ba5c",
   "metadata": {},
   "source": [
    "Tips to engineer effective prompts:\n",
    "- Be specific (provide detail and enough context to guide the LLM toward the desired output);\n",
    "- Force conciseness when needed;\n",
    "- Encourage the model to explain its reasoning (this way the results can be more accurate, especially for complex tasks);\n",
    "- Iterate if needed (several refinements may be required to obtain the best possible answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bca4c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What are some tips for improving communication skills?\n",
      "AI Response:  Practice active listening, be mindful of your body language, and be open to constructive feedback.\n"
     ]
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate, PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the secret to happiness?\",\n",
    "        \"answer\": \"Finding balance in life and learning to enjoy the small moments.\"\n",
    "    }, {\n",
    "        \"query\": \"How can I become more productive?\",\n",
    "        \"answer\": \"Try prioritizing tasks, setting goals, and maintaining a healthy work-life balance.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "life coach. The assistant provides insightful and practical advice to the users' questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the few-shot prompt template\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "\n",
    "# Define the user query\n",
    "user_query = \"What are some tips for improving communication skills?\"\n",
    "\n",
    "# Run the LLMChain for the user query\n",
    "response = chain.run({\"query\": user_query})\n",
    "\n",
    "print(\"User Query:\", user_query)\n",
    "print(\"AI Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1defad9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a35857af",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"resources\">\n",
    "    \n",
    "## 99. Resources:\n",
    "    \n",
    "</a>\n",
    "\n",
    "- [A Hands-on Guide to Prompt Engineering with ChatGPT and GPT-3](https://dev.to/mmz001/a-hands-on-guide-to-prompt-engineering-with-chatgpt-and-gpt-3-4127)\n",
    "- [Prompt Engineering Tips and Tricks with GPT-3](https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/)\n",
    "- [Prompt Engineering LLMs with LangChain and W&B](https://wandb.ai/a-sh0ts/langchain_callback_demo/reports/Prompt-Engineering-LLMs-with-LangChain-and-W-B--VmlldzozNjk1NTUw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fed1a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
