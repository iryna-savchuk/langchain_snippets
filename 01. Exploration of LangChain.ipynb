{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac34004",
   "metadata": {},
   "source": [
    "* [1. The Power and Limitations of Large Language Models](#power_and_lim)\n",
    "* [2. The LLMs](#llms)\n",
    "* [3. The Chains](#chains)\n",
    "* [4. The Memory](#memory)\n",
    "* [5. Deep Lake VectorStore](#deeplake)\n",
    "    * [5.1. Using Deep Lake as a vector database](#db)\n",
    "    * [5.2. Creating an agent with a RetrievalQA chain as a tool to answer questions based on the given document](#RetrievalQA)\n",
    "    * [5.3. Reloading an existing vector store and adding more data](#adding_data)\n",
    "* [6. Agents in LangChain](#agents)\n",
    "* [7. Tools in LangChain](#tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f494b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"power_and_lim\">\n",
    "    \n",
    "## 1. The Power and Limitations of Large Language Models\n",
    "    \n",
    "</a>\n",
    "\n",
    "Large Language Models (LLMs) are trained to understand the distribution of words in a language, enabling them to **generate meaningful text** without memorizing specific data. However, their knowledge is limited to the training data, leading to potential fabrications when asked about information beyond their training period, known as **\"hallucination.\"**\n",
    "\n",
    "To address this issue, **retrievers** can be used alongside LLMs. Retrievers fetch information from trusted sources, and LLMs are prompted to rearrange the retrieved information without adding new details. LLMs like GPT-4 and Claude can handle large context window sizes, but an efficient retriever is needed to find the most relevant documents due to the cost of execution.\n",
    "\n",
    "Efficient retrievers are built using embedding models that map texts to vectors. These vectors are then stored in specialized databases called vector stores. This is where **Deep Lake** comes in: it provides a seamless way to store embeddings and their corresponding metadata. Deep Lake also enables hybrid searches on these embeddings and their attributes for efficient data retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6afbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain\n",
    "#!pip install deeplake\n",
    "#!pip install openai\n",
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253aa370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import ACTIVELOOP_TOKEN, OPENAI_API_KEY\n",
    "\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741e8f5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"llms\">\n",
    "    \n",
    "## 2. The LLMs\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3e8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LLM wrapper\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae214086",
   "metadata": {},
   "source": [
    "The temperature parameter in OpenAI models manages the randomness of the output.\n",
    "- 0: output is mostly predetermined and result is stable;\n",
    "- 1: output can be inconsistent and interesting, but isn't generally advised for most tasks;\n",
    "- between 0.70 and 0.90: offers a balance of reliability and creativity for creative tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5520ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GPT-3 model’s Davinci variant\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73ebf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Jogging: Aim to jog for at least 30 minutes 3-4 times per week. Start at a slow pace and gradually increase your running speed and intensity as your endurance improves.\n",
      "\n",
      "2. Trail Running: Trail running improves coordination and balance. Aim to run 2-3 times per week on off-road trails. Start at a slow pace and gradually increase your speed and distance.\n",
      "\n",
      "3. Hill Sprinting: Hill sprinting is a great way to improve cardiovascular endurance and build leg strength. Sprint up a steep hill for 30 seconds and then walk or jog back down. Aim to do this 3-4 times per week.\n",
      "\n",
      "4. Swimming: Swimming is an excellent way to improve cardiovascular endurance and build upper body strength. Aim to swim for 30 minutes 3-4 times per week.\n",
      "\n",
      "5. Cycling: Cycling is a great way to build endurance. Aim to bike for 30 minutes 3-4 times a week. Start at an easy pace and gradually increase your speed and intensity as your endurance improves.\n",
      "\n",
      "6. Rowing: Rowing is an excellent cardiovascular exercise. Aim to row for 30 minutes 3-4 times a week. Start at an easy pace and gradually increase your speed and intensity as your\n"
     ]
    }
   ],
   "source": [
    "# Example of calling the initialized LLM\n",
    "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe26d1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"chains\">\n",
    "    \n",
    "## 3. The Chains\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639973f",
   "metadata": {},
   "source": [
    "A chain is a combination of multiple individual components in a specific sequence. The most commonly used type of chain is the **LLMChain**, which consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser.\n",
    "\n",
    "The LLMChain works as follows:\n",
    "\n",
    "1. Takes input variable(s);\n",
    "2. Format the input variables into a prompt by using the PromptTemplate;\n",
    "3. Passes the formatted prompt to the model (LLM or ChatModel);\n",
    "4. Parses the output of the LLM into a final format, if an OutputParser is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e40d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a chain that generates a possible name for a company that produces a given product\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f7e4a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EcoBottles.\n"
     ]
    }
   ],
   "source": [
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"eco-friendly water bottles\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a57242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Brewed Moments.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"personalized tea cups\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092981f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"memory\">\n",
    "    \n",
    "## 4. The Memory\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47c787",
   "metadata": {},
   "source": [
    "In LangChain, Memory refers to the mechanism that stores and manages the conversation history between a user and the AI. It helps maintain context and coherency throughout the interaction, enabling the AI to generate more relevant and accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23421cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Tell me about yourself.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about yourself.\n",
      "AI:  Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\n",
      "Human: What can you do?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about yourself.\n",
      "AI:  Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\n",
      "Human: What can you do?\n",
      "AI:  I can help you with a variety of tasks, such as scheduling appointments, setting reminders, and providing information. I'm also able to answer questions about topics like current events, sports, and entertainment. I'm always learning new things, so I'm sure I can help you with whatever you need.\n",
      "Human: How can you help me with data analysis?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm not familiar with data analysis, but I'm sure I can help you find the information you need. I can search the web for articles and resources related to data analysis, and I can also provide you with links to helpful websites.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory # a wrapper around ChatMessageHistory\n",
    "\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "# Start the conversation\n",
    "conversation.predict(input=\"Tell me about yourself.\")\n",
    "\n",
    "# Continue the conversation\n",
    "conversation.predict(input=\"What can you do?\")\n",
    "conversation.predict(input=\"How can you help me with data analysis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f373330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display the conversation:\n",
    "# print(conversation)            ### Careful: it displays the openai_api_key!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd19c36",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"deeplake\">\n",
    "    \n",
    "## 5. Deep Lake VectorStore\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e431f",
   "metadata": {},
   "source": [
    "Deep Lake provides storage for embeddings and their corresponding metadata in the context of LLM apps. It enables hybrid searches on these embeddings and their attributes for efficient data retrieval. It also integrates with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3865a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"db\">\n",
    "    \n",
    "### 5.1. Using Deep Lake as a vector database\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea2bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Instantiate the LLM and embeddings models\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Create documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\"\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7036aab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://iryna/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "# Create Deep Lake dataset\n",
    "my_activeloop_org_id = \"iryna\" \n",
    "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd578175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://iryna/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (2, 1536)  float32   None   \n",
      "    id        text      (2, 1)      str     None   \n",
      " metadata     json      (2, 1)      str     None   \n",
      "   text       text      (2, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1a4a8ff6-1614-11ee-bc41-12ee7aa5dbdc',\n",
       " '1a4a9190-1614-11ee-bc41-12ee7aa5dbdc']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add documents to the Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d8997",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"RetrievalQA\">\n",
    "    \n",
    "### 5.2. Creating an agent with a RetrievalQA chain as a tool to answer questions based on the given document\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a87439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a RetrievalQA chain\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "623bbab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an agent that uses the RetrievalQA chain as a tool\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3b9ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Napoleone was born.\n",
      "Action: Retrieval QA System\n",
      "Action Input: When was Napoleone born?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Napoleon Bonaparte was born on 15 August 1769.\n"
     ]
    }
   ],
   "source": [
    "# Using the agent to ask a question\n",
    "response = agent.run(\"When was Napoleone born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2db4c0",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"adding_data\">\n",
    "    \n",
    "### 5.3. Reloading an existing vector store and adding more data\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5c5f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://iryna/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://iryna/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (5, 1536)  float32   None   \n",
      "    id        text      (5, 1)      str     None   \n",
      " metadata     json      (5, 1)      str     None   \n",
      "   text       text      (5, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['9252369a-1617-11ee-bc41-12ee7aa5dbdc',\n",
       " '92523898-1617-11ee-bc41-12ee7aa5dbdc',\n",
       " '92523906-1617-11ee-bc41-12ee7aa5dbdc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the existing Deep Lake dataset and specify the embedding function\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# Create new documents\n",
    "texts = [\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\",\n",
    "    \"Claude Monet was born on November 14, 1840.\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# Add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f4971a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "## Recreate the previous agent and to ask new questions ##\n",
    "##########################################################\n",
    "\n",
    "# Instantiate the wrapper class for GPT3\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Create a retriever from the db\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# Instantiate a tool that uses the retriever\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create an agent that uses the tool\n",
    "agent = initialize_agent(tools,\n",
    "                         llm,\n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab0dac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Michael Jordan was born.\n",
      "Action: Retrieval QA System\n",
      "Action Input: When was Michael Jordan born?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Michael Jordan was born on 17 February 1963.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Michael Jordan was born on 17 February 1963.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Michael Jordan was born on 17 February 1963.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"When was Michael Jordan born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38aaa215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Claude Monet's birthday is.\n",
      "Action: Retrieval QA System\n",
      "Action Input: When is Claude Monet's birthday?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Claude Monet's birthday is November 14, 1840.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Claude Monet's birthday is November 14, 1840.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Claude Monet's birthday is November 14, 1840.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"When is birthday of Claude Monet?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec7b1844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Iryna Savchuk's birthday is.\n",
      "Action: Retrieval QA System\n",
      "Action Input: When is Iryna Savchuk's birthday?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find a source that has the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthday of Iryna Savchuk?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: Where can I find the birthday of Iryna Savchuk?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for the answer online.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the date of Iryna Savchuk's birthday?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthdate of Iryna Savchuk?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthdate of Iryna Savchuk?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthdate of Iryna Savchuk according to Wikipedia?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthdate of Iryna Savchuk according to IMDb?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthdate of Iryna Savchuk according to her official website?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthdate of Iryna Savchuk according to her official social media accounts?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthdate of Iryna Savchuk according to reliable sources?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthdate of Iryna Savchuk according to reliable sources online?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthdate of Iryna Savchuk according to reliable sources online?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthdate of Iryna Savchuk according to reliable sources online?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to search for a reliable source with the answer.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the birthdate of Iryna Savchuk according to reliable sources online?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent stopped due to iteration limit or time limit.\n"
     ]
    }
   ],
   "source": [
    "# Trying to ask something outside of the database knowledge\n",
    "response = agent.run(\"When is birthday of Iryna Savchuk?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ecea96",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"agents\">\n",
    "    \n",
    "## 6. Agents in LangChain\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5b922",
   "metadata": {},
   "source": [
    "**Agents** are high-level components that use language models (LLMs) to determine which actions to take and in what order. An **action**: either using a tool and observing its output or returning it to the user. **Tools** are functions that perform specific duties, such as Google Search, database lookups, or Python REPL.\n",
    "\n",
    "Types of agents are available in LangChain:\n",
    "\n",
    "- The `zero-shot-react-description` agent uses the ReAct framework to decide which tool to employ based purely on the tool's description. It necessitates a description of each tool.\n",
    "- The `react-docstore` agent engages with a docstore through the ReAct framework. It needs two tools: a Search tool and a Lookup tool. The Search tool finds a document, and the Lookup tool searches for a term in the most recently discovered document.\n",
    "- The `self-ask-with-search` agent employs a single tool named Intermediate Answer, which is capable of looking up factual responses to queries. It is identical to the original self-ask with the search paper, where a Google search API was provided as the tool.\n",
    "- The `conversational-react-description` agent is designed for conversational situations. It uses the ReAct framework to select a tool and uses memory to remember past conversation interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ec31ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9d4af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import GOOGLE_API_KEY, GOOGLE_CSE_ID\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c27898c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI \n",
    "\n",
    "from langchain.agents import AgentType \n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c30a65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)  # temperature to 0 for the precise answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae631268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Google search wrapper\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ada1ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    # a tool for performing Google searches\n",
    "    Tool( \n",
    "        name = \"google-search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to search google to answer questions about current events\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2317f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True,\n",
    "                         max_iterations=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce43b17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the latest news about Ukraine counteroffensive\n",
      "Action: google-search\n",
      "Action Input: \"Ukraine counteroffensive\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4 days ago ... The Ukrainian Army is encountering an array of challenges that has complicated the early stages of its counteroffensive, especially the ... Jun 15, 2023 ... The counteroffensive is complicated by Ukraine's lack of air power. Kyiv has lobbied the West for months to supply F-16 fighter jets but they ... 2 days ago ... Ukrainian officials have said the counteroffensive is going as planned, even though it's clear, through open source accounts, that Ukrainian ... 7 days ago ... In its early phases, Ukraine's counteroffensive is having less success and Russian forces are showing more competence than western ... Jun 12, 2023 ... Ukrainian President Volodymyr Zelenskyy said Saturday that counteroffensive and defensive actions are underway against Russian forces, asserting ... 8 days ago ... While saying little, Putin has revealed that military failures are entirely at odds with his vision for the future, while several more ... Jun 5, 2023 ... Ukraine has been preparing for a counteroffensive against the Russian troops that invaded their country. \"Our goal has been to provide them with ... 3 days ago ... Ukraine “continues to move forward with a counteroffensive,” he said on NBC's “Meet the Press” on Sunday. “These are early days, ... Jun 14, 2023 ... Ukrainian forces are blunted. Like the recent Russian offensive, the Ukrainian counteroffensive stalls, taking little or no territory. Command ... Jun 8, 2023 ... Senior U.S. officials are convinced that President Joe Biden's global reputation hinges on the success of Ukraine's counteroffensive.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the latest update about Ukraine counteroffensive\n",
      "Final Answer: The latest update about Ukraine counteroffensive is that Ukrainian forces are blunted and the counteroffensive has stalled, taking little or no territory. Command and control issues have complicated the early stages of the counteroffensive, and Ukraine has been lobbying the West for months to supply F-16 fighter jets. Senior U.S. officials are convinced that President Joe Biden's global reputation hinges on the success of Ukraine's counteroffensive.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The latest update about Ukraine counteroffensive is that Ukrainian forces are blunted and the counteroffensive has stalled, taking little or no territory. Command and control issues have complicated the early stages of the counteroffensive, and Ukraine has been lobbying the West for months to supply F-16 fighter jets. Senior U.S. officials are convinced that President Joe Biden's global reputation hinges on the success of Ukraine's counteroffensive.\n"
     ]
    }
   ],
   "source": [
    "response = agent(\"What's the latest update about Ukraine counteroffensive?\")\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e72b2b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the latest update about Ukraine counteroffensive?\",\n",
       " 'output': \"The latest update about Ukraine counteroffensive is that Ukrainian forces are blunted and the counteroffensive has stalled, taking little or no territory. Command and control issues have complicated the early stages of the counteroffensive, and Ukraine has been lobbying the West for months to supply F-16 fighter jets. Senior U.S. officials are convinced that President Joe Biden's global reputation hinges on the success of Ukraine's counteroffensive.\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89a1d9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"tools\">\n",
    "    \n",
    "## 7. Tools in LangChain\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7dda95",
   "metadata": {},
   "source": [
    "LangChain provides a variety of tools for agents to interact with the outside world. These tools can be used to create custom agents that perform various tasks: web searching, answering questions, or running Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8587d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83bee94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a  LLMChain specifically for text summarization\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Write a summary of the following text: {query}\"\n",
    ")\n",
    "\n",
    "summarize_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92949707",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for finding information about recent events\"\n",
    "    ),\n",
    "    Tool(\n",
    "       name='Summarizer',\n",
    "       func=summarize_chain.run,\n",
    "       description='useful for summarizing texts'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94e81dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an agent that leverages two tools\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0337ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should search for recent news about the Mars rover and then summarize the results.\n",
      "Action: Search\n",
      "Action Input: Latest news about the Mars rover\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mMars 2020 Perseverance Rover · The largest and most capable rover ever sent to Mars. ... Curiosity Rover · Measures Mars' atmosphere to understand its climate ... Dec 15, 2021 ... Mars Sample Return is going to have great stuff to choose from!” Get the Latest JPL News. SUBSCRIBE TO THE NEWSLETTER. The mission has concluded that the solar-powered lander has run out of energy after more than four years on the Red Planet. Oct 19, 2022 ... NASA's Curiosity Mars rover used its Mast Camera, or Mastcam, to capture this panorama of a hill nicknamed ... Get the Latest JPL News. NASA's Mars 2020 Perseverance rover will look for signs of past microbial life, cache rock and soil samples, and prepare for future human exploration. Curiosity rover finds water-carved 'book' rock on Mars (photo) ... A book-shaped rock on Mars was likely carved by ancient water running through the area, ... Feb 8, 2023 ... “Mars' ancient climate had a wonderful complexity to it, much like Earth's.” Get the Latest JPL News. SUBSCRIBE TO THE NEWSLETTER ... Sep 16, 2022 ... Since July, NASA's Perseverance rover has drilled and collected four slim cores of sedimentary rock, formed in what was once a lake on Mars. Apr 13, 2023 ... NASA's Curiosity Mars rover will drive faster and reduce wear on its wheels thanks to two of the new capabilities included with a major software ... Apr 25, 2023 ... China's Mars rover, which has been in longer-than-expected hibernation on the red planet since May 2022, has likely suffered excessive ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should summarize the results of the search.\n",
      "Action: Summarizer\n",
      "Action Input: Mars 2020 Perseverance Rover is the largest and most capable rover ever sent to Mars. Curiosity Rover measures Mars' atmosphere to understand its climate. Mars Sample Return is going to have great stuff to choose from. The mission has concluded that the solar-powered lander has run out of energy after more than four years on the Red Planet. NASA's Curiosity Mars rover used its Mast Camera to capture a panorama of a hill nicknamed \"Book Rock\". NASA's Mars 2020 Perseverance rover will look for signs of past microbial life, cache rock and soil samples, and prepare for future human exploration. Curiosity rover finds water-carved 'book' rock on Mars. Mars' ancient climate had a wonderful complexity to it, much like Earth's. Since July, NASA's Perseverance rover has drilled and collected four slim cores of sedimentary rock, formed in what was once a lake on Mars. NASA's Curiosity Mars rover will drive faster and reduce wear on its wheels thanks to two of the new capabilities included with a major software update. China's Mars rover, which has been in longer-than-expected hibernation on the red planet since May 2022, has likely suffered excessive\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m cold damage.\n",
      "\n",
      "NASA's Mars 2020 Perseverance rover is the largest and most capable rover ever sent to Mars. It will look for signs of past microbial life, cache rock and soil samples, and prepare for future human exploration. Curiosity rover, which has been on the Red Planet for more than four years, has used its Mast Camera to capture a panorama of a hill nicknamed \"Book Rock\". Mars Sample Return mission has concluded that the solar-powered lander has run out of energy. Curiosity rover has also found water-carved 'book' rock on Mars, and Perseverance rover has drilled and collected four slim cores of sedimentary rock. With a major software update, Curiosity rover will drive faster and reduce wear on its wheels. China's Mars rover, which has been in hibernation since May 2022, has likely suffered excessive cold damage.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: NASA's Mars 2020 Perseverance rover is the largest and most capable rover ever sent to Mars. It will look for signs of past microbial life, cache rock and soil samples, and prepare for future human exploration. Curiosity rover has used its Mast Camera to capture a panorama of a hill nicknamed \"Book Rock\", found water-carved 'book' rock on Mars, and Perseverance rover has drilled and collected four slim cores of sedimentary rock. With a major software update, Curiosity rover will drive faster and reduce wear on its wheels. China's Mars rover has likely suffered excessive cold damage.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "NASA's Mars 2020 Perseverance rover is the largest and most capable rover ever sent to Mars. It will look for signs of past microbial life, cache rock and soil samples, and prepare for future human exploration. Curiosity rover has used its Mast Camera to capture a panorama of a hill nicknamed \"Book Rock\", found water-carved 'book' rock on Mars, and Perseverance rover has drilled and collected four slim cores of sedimentary rock. With a major software update, Curiosity rover will drive faster and reduce wear on its wheels. China's Mars rover has likely suffered excessive cold damage.\n"
     ]
    }
   ],
   "source": [
    "# Run the agent\n",
    "response = agent(\"What's the latest news about the Mars rover? Then please summarize the results.\")\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12bd98",
   "metadata": {},
   "source": [
    "LangChain provides a toolkit that integrates various functions to improve the functionality of conversational agents, for instance:\n",
    "- `SerpAPI`: an interface for the SerpAPI search engine, allowing the agent to perform robust online searches to pull in relevant data for a conversation or task.\n",
    "- `PythonREPLTool`: tool that enables the writing and execution of Python code within an agent. This opens up a wide range of possibilities for advanced computations and interactions within the conversation.\n",
    "- `custom tools`: allows adding more specialized capabilities to the LangChain conversational agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96dba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
