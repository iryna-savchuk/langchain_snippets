{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fec7179",
   "metadata": {},
   "source": [
    "**Large Language Models (LLMs)** are deep learning models with billions of parameters that excel at a wide range of natural language processing tasks. They can perform tasks like translation, sentiment analysis, and chatbot conversations without being specifically trained for them. LLMs can be used without fine-tuning by employing **prompting** techniques.\n",
    "\n",
    "**Architecture**: LLMs typically consist of multiple layers of neural networks, feedforward layers, embedding layers, and attention layers. These layers work together to process input text and generate output predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec139fe6",
   "metadata": {},
   "source": [
    "* [1. Maximum number of tokens](#max_tokens)\n",
    "* [2. Tokens Distributions and Predicting the Next Token](#distr)\n",
    "    * [2.1. Tracking Token Usage](#tracking-usage)\n",
    "* [3. Few-shot learning](#few-shot)\n",
    "* [4. Prompts Examples](#prompts)\n",
    "    * [4.1. Question-Answering Prompt Template](#qa)\n",
    "    * [4.2. Text Summarization](#summarization)\n",
    "    * [4.3. Text Translation](#translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1107cb1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"max_tokens\">\n",
    "    \n",
    "## 1. Maximum number of tokens\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e484b",
   "metadata": {},
   "source": [
    "In the LangChain library, the LLM context size, or the maximum number of tokens the model can process, is determined by the specific implementation of the LLM. For example, iin the GPT-3 model, the maximum number of tokens supported by the model is 2,049. \n",
    "\n",
    "It is important to ensure that the input text does not exceed the maximum number of tokens supported by the model. For example, it is possible to split the input text into smaller chunks, process them separately, and then combine the results as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6763090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "## PSEUDOCODE to handle text that exceeds the maximum token limit ##\n",
    "####################################################################\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"your_long_input_text\"  ## this input can be really long, exceeding the token limit of the given model\n",
    "\n",
    "# Determine the maximum number of tokens from documentation\n",
    "max_tokens = 4097\n",
    "\n",
    "# Split the input text into chunks based on the max tokens\n",
    "text_chunks = split_text_into_chunks(input_text, max_tokens)\n",
    "\n",
    "# Process each chunk separately\n",
    "results = []\n",
    "for chunk in text_chunks:\n",
    "    result = llm.process(chunk)\n",
    "    results.append(result)\n",
    "\n",
    "# Combine the results as needed\n",
    "final_result = combine_results(results)\n",
    "\n",
    "##\n",
    "## NOTE: split_text_into_chunks and combine_results \n",
    "## are custom functions that will be covered later\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0037106",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"distr\">\n",
    "    \n",
    "## 2. Tokens Distributions and Predicting the Next Token\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d42be",
   "metadata": {},
   "source": [
    "GPT-3 and GPT-4, prominent examples of large language models, undergo pretraining on vast quantities of textual data. They acquire the ability to anticipate the subsequent token in a sequence by leveraging the context derived from preceding tokens. GPT-family models use Causal Language modeling, which predicts the next token while only having access to the tokens before it, which enables them to generate contextually relevant text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed90940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Vivid Threads.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "text = \"What would be a good name for a company that produces colorful t-shirts?\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1a2102",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"tracking-usage\">\n",
    "    \n",
    "### 2.1. Tracking Token Usage\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d4b716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 50\n",
      "\tPrompt Tokens: 6\n",
      "\tCompletion Tokens: 44\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.001\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke about dogs\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e109a305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What do you call a dog magician?\\nA: A labracadabrador!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae6f681",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"few-shot\">\n",
    "    \n",
    "## 3. Few-shot learning\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070151d",
   "metadata": {},
   "source": [
    "Few-shot learning is a remarkable ability that allows LLMs to learn and generalize from limited examples. Prompts serve as the input to these models and play a crucial role in achieving this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ce8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Template \n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# Create examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt example from the above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Break the previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "# and the suffix is the user input and output indicator\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# Create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a14864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To find the perfect balance between pizza and ice cream.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "\n",
    "# Load the model\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
    "chain.run(\"What's the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cc9b1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"prompts\">\n",
    "    \n",
    "## 4. Prompts Examples\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5e85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35a5de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import HUGGINGFACEHUB_API_TOKEN\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578f9b9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"qa\">\n",
    "    \n",
    "### 4.1. Question-Answering Prompt Template\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77eca34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a simple question-answering prompt template using LangChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# User question\n",
    "question = \"What is the capital city of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82ec5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "# Using the Hugging Face model \"google/flan-t5-large\" to answer the question\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# Initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0}\n",
    ")\n",
    "\n",
    "# Create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# Ask the user question about the capital of France\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e52ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris', generation_info=None)], [Generation(text='giraffe', generation_info=None)], [Generation(text='nitrogen', generation_info=None)], [Generation(text='yellow', generation_info=None)]] llm_output=None run=RunInfo(run_id=UUID('587dabe2-2813-4f2e-9369-49e97dafa377'))\n"
     ]
    }
   ],
   "source": [
    "# Asking Multiple Questions\n",
    "\n",
    "# Approach 1\n",
    "# Iterating through all questions one at a time\n",
    "\n",
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "\n",
    "res = llm_chain.generate(qa)\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f24b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris\\nBlue whale\\nNitrogen\\nYellow'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asking Multiple Questions\n",
    "\n",
    "# Approach 2\n",
    "# Placing all questions into a single prompt\n",
    "# This method performs best on more capable models\n",
    "\n",
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "    \"What color is a ripe banana?\\n\"\n",
    ")\n",
    "\n",
    "llm_chain.run(qs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34cc37f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"summarization\">\n",
    "    \n",
    "### 4.2. Text Summarization\n",
    "   \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ef9f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the necessary imports and an instance of the OpenAI language model\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4059e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template for summarization\n",
    "summarization_template = \"Summarize the following text to one short sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e51f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the predict method with the text to be summarized\n",
    "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
    "summarized_text = summarization_chain.predict(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0eb25674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain offers various modules for building language model applications, allowing users to combine them for complex applications or use them individually for simpler ones, with the basic building block being calling an LLM on input, as demonstrated in the example of generating company names based on their products.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ebf2c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"translation\">\n",
    "    \n",
    "### 4.3. Text Translation\n",
    "   \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b52c186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same llm variable as defined before.\n",
    "# Pass a different prompt that asks for translating the query from a source_language to the target_language.\n",
    "\n",
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], \n",
    "                                    template=translation_template)\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f130c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aujourd'hui est le jour parfait pour étudier.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the predict method to use the translation chain\n",
    "source_language = \"English\"\n",
    "target_language = \"French\"\n",
    "text = \"Today is the perfect day for studing\"\n",
    "\n",
    "translated_text = translation_chain.predict(source_language=source_language, \n",
    "                                            target_language=target_language, \n",
    "                                            text=text)\n",
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837703c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
