{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1790f12",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "* [1. Chains and Why They Are Used](#chains)\n",
    "    * [1.1. LLMChain](#LLMChain) \n",
    "    * [1.2. Parsers](#parsers) \n",
    "    * [1.3. Conversational Chain (Memory)](#memory) \n",
    "    * [1.4. Sequential Chain](#seq)\n",
    "    * [1.5. Debug](#debug) \n",
    "    * [1.6. Custom](#custom) \n",
    "* [2. YouTube Video Summarizer](#video_summarizer)\n",
    "* [3. Creating Voice Assistant](#voice_assistant)\n",
    "* [4. Code Comprehension - Twitter Algorithm](#code_understanding)\n",
    "* [5. Recommendation Engine for Songs](#recommendation)\n",
    "* [6. Self-Critique Chain](#critique)\n",
    "* [7. Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98da2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d6c3b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"chains\">\n",
    "    \n",
    "## 1. Chains and the Reasons to Use them\n",
    "    \n",
    "</a>\n",
    "\n",
    "Chains are responsible for creating an end-to-end pipeline to use LLMs. Chains join the model, prompt, memory, parsing output, debugging capability. They also provide an easy-to-use interface. \n",
    "\n",
    "A chain will:\n",
    "1) receive the user’s query as an input;\n",
    "2) process the LLM’s response;\n",
    "3) return the output to the user.\n",
    "\n",
    "It is possible to design a custom pipeline by inheriting the `Chain` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c9176",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"LLMChain\">\n",
    "    \n",
    "### 1.1. LLMChain\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3da66f",
   "metadata": {},
   "source": [
    "There are several methods to use chains:\n",
    "- **`__ call __`** pass an input directly to the object while initializing it; will return the input variable and the model’s response under the text key;\n",
    "- **`.apply()`** pass multiple inputs at once and receive a list for each input;\n",
    "- **`.generate()`** returns an instance of `LLMResult`, which provides more information;\n",
    "- **`.predict()`** pass multiple (or single) inputs for a single prompt;\n",
    "- **`.run()`** the same as .predict (they can be used interchangeably)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7532f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd6f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"What is a word to replace the following: {word}?\"\n",
    "\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0884645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'artificial', 'text': '\\n\\nSynthetic'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing input directly to object while initializing it >> __call__ \n",
    "llm_chain(\"artificial\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f0508a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\\n\\nSynthetic'}, {'text': '\\n\\nWisdom'}, {'text': '\\n\\nAutomaton'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing multiple inputs  >> .apply() \n",
    "input_list = [\n",
    "    {\"word\": \"artificial\"},\n",
    "    {\"word\": \"intelligence\"},\n",
    "    {\"word\": \"robot\"}\n",
    "]\n",
    "\n",
    "llm_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02822fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nSynthetic', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWisdom', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nAutomaton', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 46, 'prompt_tokens': 33, 'completion_tokens': 13}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('12354ad4-428d-4577-afed-9bf2f543fdd4')), RunInfo(run_id=UUID('846124ec-28fe-42f9-9bf3-818e7f79afa7')), RunInfo(run_id=UUID('fe91d0d7-e93e-41b0-a123-b1ba6bbe585e'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return an instance of LLMResult with more information >> .generate()\n",
    "llm_chain.generate(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b1bc231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ngift'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass both the word and the context\n",
    "prompt_template = \"Looking at the context of '{context}'. What is an appropriate word to replace the following: {word}?\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
    "\n",
    "llm_chain.predict(word=\"present\", context=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4658b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ngift'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternative to .predict()\n",
    "llm_chain.run(word=\"present\", context=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab34c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nNow'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(word=\"present\", context=\"time\") # or llm_chain.run(word=\"present\", context=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e721955",
   "metadata": {},
   "source": [
    "**Note**: To format the output we can use either parsers (see example below and refer to [notebook section](06.%20Prompting.ipynb#outputs)) or we can directly pass a prompt as a string to a Chain and initialize it using the .from_string() function as follows:\n",
    "`LLMChain.from_string(llm=llm, template=template)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab1a17",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"parsers\">\n",
    "    \n",
    "### 1.2. Parsers\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e22fae44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Synthetic',\n",
       " 'Manufactured',\n",
       " 'Imitation',\n",
       " 'Fabricated',\n",
       " 'Fake',\n",
       " 'Mechanical',\n",
       " 'Computerized',\n",
       " 'Automated',\n",
       " 'Simulated',\n",
       " 'Artificial Intelligence',\n",
       " 'Constructed',\n",
       " 'Programmed',\n",
       " 'Processed',\n",
       " 'Algorithmic.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, output_parser=output_parser, input_variables=[]),\n",
    "    output_parser=output_parser)\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f111ca",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"memory\">\n",
    "    \n",
    "### 1.3. Conversational Chain (Memory)\n",
    "    \n",
    "</a>\n",
    "\n",
    "LangChain provides a `ConversationalChain` to track previous prompts and responses using the `ConversationalBufferMemory` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e058030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Excellent, superb, wonderful, terrific, outstanding, remarkable, splendid, grand, fabulous, magnificent, glorious, sublime.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"List all possible words as substitute for 'great' as comma separated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "912413f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Amazing, remarkable, incredible, and phenomenal.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f98947",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"seq\">\n",
    "    \n",
    "### 1.4. Sequential Chain\n",
    "    \n",
    "</a>\n",
    "\n",
    "Sequantial chain is desined to concatenate multiple chains into one. For example, the `SimpleSequentialChain` instance created below will start running each chain from the first index and pass its response to the next one in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb91d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "#overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f45e8b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"debug\">\n",
    "    \n",
    "### 1.5. Debug\n",
    "    \n",
    "</a>\n",
    "\n",
    "To trace the inner work of a chain, one should set the `verbose` argument to `True` (if so, the output will depend on a specific application of chain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6240ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'neural' as comma separated.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Brainy, Nervous, Nerve-wracking, Synaptic, Cognitive, Cerebral, Mental, Intellective, Thoughtful, Mindful, Psychogenic, Psychical, Psychosomatic.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"List all possible words as substitute for 'neural' as comma separated.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True)\n",
    "\n",
    "conversation.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04b0b7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"custom\">\n",
    "    \n",
    "### 1.6. Custom Chain\n",
    "    \n",
    "</a>\n",
    "\n",
    "\n",
    "It is possible to define your own chain for any custom task:\n",
    "1. Define a class that inherits most of its functionalities from the `Chain` class;\n",
    "2. Declare three methods: `input_keys`, `output_keys` and `_call` (declation will depend on the specifics of the task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fad71fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom chain (ConcatenateChain) that returns a word's meaning and then suggests a replacement\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        # Union of the input keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3986124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare each chain individually using the \"LLMChain\" class\n",
    "\n",
    "# 1\n",
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is the meaning of the following word '{word}'?\",\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "# 2\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is a word to replace the following: {word}?\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28366c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated output:\n",
      "\n",
      "\n",
      "Network is a term used to describe a system of interconnected components, such as computers, servers, and other devices, that are able to communicate with each other. It can also refer to the connections between people, such as social networks.\n",
      "\n",
      "System\n"
     ]
    }
   ],
   "source": [
    "# Call ConcatenateChain to merge the results of the chain_1 and chain_2\n",
    "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
    "concat_output = concat_chain.run(\"network\")\n",
    "\n",
    "print(f\"Concatenated output:\\n{concat_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892dccc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"video_summarizer\">\n",
    "    \n",
    "## 2. YouTube Video Summarizer\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e986ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d6c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c293cea",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"voice_assistant\">\n",
    "    \n",
    "## 3. Creating Voice Assistant\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de079a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ce6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10b98575",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"code_understanding\">\n",
    "    \n",
    "## 4. Code Comprehension - Twitter Algorithm\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da9132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b653d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb26264e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"recommendation\">\n",
    "    \n",
    "## 5. Recommendation Engine for Songs\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3952552a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb190e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "064e6e5a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"critique\">\n",
    "    \n",
    "## 6. Self-Critique Chain\n",
    "    \n",
    "</a>\n",
    "\n",
    "Self-critique chain acts as a mechanism to ensure model responses are appropriate in a production environment. By iterating over the model's output and checking against predefined expectations, the self-critique chain prompts the model to correct itself when necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98f57d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1f57a34",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"resources\">\n",
    "    \n",
    "## 7. Additional Resources\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d567d",
   "metadata": {},
   "source": [
    "- [Langchain documentation on Chains](https://python.langchain.com/docs/modules/chains/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc6435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
