{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1790f12",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "* [1. Chains and Why They Are Used](#chains)\n",
    "    * [1.1. LLMChain](#LLMChain) \n",
    "    * [1.2. Parsers](#parsers) \n",
    "    * [1.3. Conversational Chain (Memory)](#memory) \n",
    "    * [1.4. Sequential Chain](#seq)\n",
    "    * [1.5. Debug](#debug) \n",
    "    * [1.6. Custom](#custom) \n",
    "* [2. YouTube Video Summarizer](#video_summarizer)\n",
    "* [3. Creating Voice Assistant](#voice_assistant)\n",
    "* [4. Code Comprehension - Twitter Algorithm](#code_understanding)\n",
    "* [5. Recommendation Engine for Songs](#recommendation)\n",
    "* [6. Self-Critique Chain](#critique)\n",
    "* [7. Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98da2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d6c3b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"chains\">\n",
    "    \n",
    "## 1. Chains and the Reasons to Use them\n",
    "    \n",
    "</a>\n",
    "\n",
    "Chains are responsible for creating an end-to-end pipeline to use LLMs. Chains join the model, prompt, memory, parsing output, debugging capability. They also provide an easy-to-use interface. \n",
    "\n",
    "A chain will:\n",
    "1) receive the user’s query as an input;\n",
    "2) process the LLM’s response;\n",
    "3) return the output to the user.\n",
    "\n",
    "It is possible to design a custom pipeline by inheriting the `Chain` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493e61",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"LLMChain\">\n",
    "    \n",
    "### 1.1. LLMChain\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe4fd8",
   "metadata": {},
   "source": [
    "There are several methods to use chains:\n",
    "- **`__ call __`** pass an input directly to the object while initializing it; will return the input variable and the model’s response under the text key;\n",
    "- **`.apply()`** pass multiple inputs at once and receive a list for each input;\n",
    "- **`.generate()`** returns an instance of `LLMResult`, which provides more information;\n",
    "- **`.predict()`** pass multiple (or single) inputs for a single prompt;\n",
    "- **`.run()`** the same as .predict (they can be used interchangeably)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7532f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf2ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"What is a word to replace the following: {word}?\"\n",
    "\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67e81a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'artificial', 'text': '\\n\\nSynthetic'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing input directly to object while initializing it >> __call__ \n",
    "llm_chain(\"artificial\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ab298a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\\n\\nSynthetic'}, {'text': '\\n\\nWisdom'}, {'text': '\\n\\nAutomaton'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing multiple inputs  >> .apply() \n",
    "input_list = [\n",
    "    {\"word\": \"artificial\"},\n",
    "    {\"word\": \"intelligence\"},\n",
    "    {\"word\": \"robot\"}\n",
    "]\n",
    "\n",
    "llm_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127988d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nSynthetic', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWisdom', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nAutomaton', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 46, 'prompt_tokens': 33, 'completion_tokens': 13}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('12354ad4-428d-4577-afed-9bf2f543fdd4')), RunInfo(run_id=UUID('846124ec-28fe-42f9-9bf3-818e7f79afa7')), RunInfo(run_id=UUID('fe91d0d7-e93e-41b0-a123-b1ba6bbe585e'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return an instance of LLMResult with more information >> .generate()\n",
    "llm_chain.generate(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67de76b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ngift'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass both the word and the context\n",
    "prompt_template = \"Looking at the context of '{context}'. What is an appropriate word to replace the following: {word}?\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
    "\n",
    "llm_chain.predict(word=\"present\", context=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74e62e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ngift'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternative to .predict()\n",
    "llm_chain.run(word=\"present\", context=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6b59f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nNow'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(word=\"present\", context=\"time\") # or llm_chain.run(word=\"present\", context=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a0d2f",
   "metadata": {},
   "source": [
    "**Note**: To format the output we can use either parsers (see example below and refer to [notebook section](06.%20Prompting.ipynb#outputs)) or we can directly pass a prompt as a string to a Chain and initialize it using the .from_string() function as follows:\n",
    "`LLMChain.from_string(llm=llm, template=template)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a2442",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"parsers\">\n",
    "    \n",
    "### 1.2. Parsers\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9ac60e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Synthetic',\n",
       " 'Manufactured',\n",
       " 'Imitation',\n",
       " 'Fabricated',\n",
       " 'Fake',\n",
       " 'Mechanical',\n",
       " 'Computerized',\n",
       " 'Automated',\n",
       " 'Simulated',\n",
       " 'Artificial Intelligence',\n",
       " 'Constructed',\n",
       " 'Programmed',\n",
       " 'Processed',\n",
       " 'Algorithmic.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, output_parser=output_parser, input_variables=[]),\n",
    "    output_parser=output_parser)\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7cbf2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"memory\">\n",
    "    \n",
    "### 1.3. Conversational Chain (Memory)\n",
    "    \n",
    "</a>\n",
    "\n",
    "LangChain provides a `ConversationalChain` to track previous prompts and responses using the `ConversationalBufferMemory` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfe37dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Excellent, superb, wonderful, terrific, outstanding, remarkable, splendid, grand, fabulous, magnificent, glorious, sublime.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"List all possible words as substitute for 'great' as comma separated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d7e8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Amazing, remarkable, incredible, and phenomenal.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a894f99",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"seq\">\n",
    "    \n",
    "### 1.4. Sequential Chain\n",
    "    \n",
    "</a>\n",
    "\n",
    "Sequantial chain is desined to concatenate multiple chains into one. For example, the `SimpleSequentialChain` instance created below will start running each chain from the first index and pass its response to the next one in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05c51319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "#overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066a30f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"debug\">\n",
    "    \n",
    "### 1.5. Debug\n",
    "    \n",
    "</a>\n",
    "\n",
    "To trace the inner work of a chain, one should set the `verbose` argument to `True` (if so, the output will depend on a specific application of chain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b00a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'neural' as comma separated.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Brainy, Nervous, Nerve-wracking, Synaptic, Cognitive, Cerebral, Mental, Intellective, Thoughtful, Mindful, Psychogenic, Psychical, Psychosomatic.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"List all possible words as substitute for 'neural' as comma separated.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True)\n",
    "\n",
    "conversation.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea0e2b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"custom\">\n",
    "    \n",
    "### 1.6. Custom Chain\n",
    "    \n",
    "</a>\n",
    "\n",
    "\n",
    "It is possible to define your own chain for any custom task:\n",
    "1. Define a class that inherits most of its functionalities from the `Chain` class;\n",
    "2. Declare three methods: `input_keys`, `output_keys` and `_call` (declation will depend on the specifics of the task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b159140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom chain (ConcatenateChain) that returns a word's meaning and then suggests a replacement\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        # Union of the input keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bb452d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare each chain individually using the \"LLMChain\" class\n",
    "\n",
    "# 1\n",
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is the meaning of the following word '{word}'?\",\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "# 2\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is a word to replace the following: {word}?\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f7e042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated output:\n",
      "\n",
      "\n",
      "Intelligence is the ability to acquire and apply knowledge and skills. It is the capacity to think, reason, understand, and learn. It is also the ability to solve problems and adapt to new situations.\n",
      "\n",
      "Wisdom\n"
     ]
    }
   ],
   "source": [
    "# Call ConcatenateChain to merge the results of the chain_1 and chain_2\n",
    "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
    "concat_output = concat_chain.run(\"intelligence\")\n",
    "\n",
    "print(f\"Concatenated output:\\n{concat_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892dccc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"video_summarizer\">\n",
    "    \n",
    "## 2. YouTube Video Summarizer\n",
    "    \n",
    "</a>\n",
    "\n",
    "It is possible build a tool to effectively extract key takeaways from YouTube videos. It can be done by leveraging Whisper to transcribe YouTube audio files and then create summarized output by using LangChain's summarization techniques (including stuff, refine, and map_reduce).\n",
    "\n",
    "The **stuff** approach is the simplest and most naive one: all the text from the documents is used in a single prompt. This method may raise exceptions if all text is longer than the available context size of the LLM. The **map-reduce** and **refine** approaches offer more sophisticated ways to process and extract information from longer documents. The \"map-reduce\" method can be parallelized,so it is faster. The \"refine\" approach is sequential in nature, making it slower compared to the \"map-reduce\" method, but producing better results. The most suitable approach should be selected by considering the trade-offs between speed and quality.\n",
    "\n",
    "**Whisper** is a cutting-edge, automatic speech recognition system developed by OpenAI. It has been trained on an impressive 680,000 hours of multilingual and multitasking supervised data sourced from the web.\n",
    "\n",
    "STEPS:\n",
    "- Download the desired YouTube audio file;\n",
    "- Transcribe the audio with the help of Whisper;\n",
    "- Summarize the transcribed text using LangChain (stuff, refine, and map_reduce);\n",
    "- Adding multiple URLs to DeepLake database, and retrieving information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e986ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "\n",
    "# !pip install langchain==0.0.208 deeplake openai tiktoken\n",
    "# !pip install -q yt_dlp\n",
    "# !pip install -q git+https://github.com/openai/whisper.git\n",
    "\n",
    "######################################\n",
    "\n",
    "# MacOS (requires https://brew.sh/)\n",
    "#brew install ffmpeg\n",
    "\n",
    "# Ubuntu\n",
    "#sudo apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a455747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keys import OPENAI_API_KEY, ACTIVELOOP_TOKEN\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1c42c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "\n",
    "# Define function to download video from YouTube to a local file\n",
    "def download_mp4_from_youtube(url, filename):\n",
    "    # Set the options for the download\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "        'outtmpl': filename,\n",
    "        'quiet': True,\n",
    "    }\n",
    "    # Download the video file\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "\n",
    "        \n",
    "url = \"https://www.youtube.com/watch?v=mBjPyte2ZZo\"\n",
    "filename = 'data/lecuninterview.mp4'\n",
    "\n",
    "download_mp4_from_youtube(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d08a5",
   "metadata": {},
   "source": [
    "The whisper package that we installed earlier provides the `.load_model()` method to download the model and transcribe a video file. Multiple different models are available: `tiny`, `base`, `small`, `medium`, and `large` (each of them has tradeoffs between accuracy and speed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda8b948",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iryna/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "filename = 'data/lecuninterview.mp4'\n",
    "result = model.transcribe(filename)\n",
    "\n",
    "# To print out the obtained transcription\n",
    "# print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91269289",
   "metadata": {},
   "source": [
    "**Note**: If an eeror about SSL certificate is raised while running the code above, have a look at the solution [here](https://stackoverflow.com/questions/68275857/urllib-error-urlerror-urlopen-error-ssl-certificate-verify-failed-certifica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c207981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result to a text file\n",
    "with open ('output/text.txt', 'w') as file:  \n",
    "    file.write(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2882b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi, I'm Craig Smith and this is I on A On. This week I talked to Jan LeCoon, one of the seminal figures in deep learning development and a long time proponent of self-supervised learning. Jan spoke about what's missing in large language models and about his new joint embedding predictive architecture which may be a step toward filling that gap. He also talked about his theory of consciousness and the potential for AI systems to someday exhibit the features of consciousness. It's a fascinating conversation that I hope you'll enjoy. Okay, so Jan, it's great to see you again. I wanted to talk to\n"
     ]
    }
   ],
   "source": [
    "# Print out a chunk of the result\n",
    "print(result['text'][:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26a73a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading utilities from the LangChain library necessary to perform Summarization Step\n",
    "from langchain import OpenAI, LLMChain  # to handle large texts\n",
    "from langchain.chains.mapreduce import MapReduceChain  # to optimize\n",
    "from langchain.prompts import PromptTemplate   # to construct prompt\n",
    "from langchain.chains.summarize import load_summarize_chain  # to run summarization\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)  # initialize an instance of OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c306bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split input text into smaller chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    ")\n",
    "\n",
    "with open('output/text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts[:5]]  # only the 5 first chunks will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "525f4bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jan Le Ka is a professor at New York University and Chief AI Scientist at Fair, a fundamental AI\n",
      "research lab. His research focuses on self-supervised learning, which has revolutionized natural\n",
      "language processing. His latest paper is the Joint Embedding Predictive Architecture, which relates\n",
      "to what is lacking in large language models. Self-supervised learning is a technique used to train\n",
      "large neural networks to predict missing words in a piece of text. Generative models are used to\n",
      "predict missing words in a text, but it is difficult to represent uncertain predictions. Attempts to\n",
      "transfer self-supervised learning methods from language processing to images have not been\n",
      "successful, but this approach has been successful in audio.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap   \n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "# Format and print the output\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a802d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "# To see the prompt template that is used with the map_reduce technique\n",
    "print( chain.llm_chain.prompt.template )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11daad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with the prompt\n",
    "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5955fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Jan LeCoon is a seminal figure in deep learning development and a long time proponent of self-supervised learning\n",
      "- Discussed what's missing in large language models and his new joint embedding predictive architecture\n",
      "- Theory of consciousness and potential for AI systems to exhibit features of consciousness\n",
      "- Self-supervised learning revolutionized natural language processing\n",
      "- Large language models lack a world model and generative models are difficult to represent uncertain predictions\n",
      "- Successful in audio but not images, so need to predict a representation of the image\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"stuff\", \n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "wrapped_text = textwrap.fill(output_summary, \n",
    "                             width=1000,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "331bb4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Craig Smith interviews Jan LeCoon, a deep learning developer and proponent of self-supervised\n",
      "learning, about his new joint embedding predictive architecture and his theory of consciousness. Jan\n",
      "discusses the gap in large language models and the potential for AI systems to exhibit features of\n",
      "consciousness. He explains how self-supervised learning has revolutionized natural language\n",
      "processing through the use of transformer architectures for pre-training, such as taking a piece of\n",
      "text, removing some of the words, and replacing them with black markers to train a large neural net\n",
      "to predict the words that are missing. This technique has been used in practical applications such\n",
      "as contact moderation systems on Facebook, Google, YouTube, and more. Jan also explains how this\n",
      "technique can be used to represent uncertain predictions in generative models, such as predicting\n",
      "the missing words in a text, or predicting the missing frames in a video. He further explains that\n",
      "while this technique has been successful in language processing, it has not been successful in the\n",
      "domain of images, and that the only successful technique is one that predicts a representation of\n",
      "the image instead of the image itself.\n"
     ]
    }
   ],
   "source": [
    "# Generating more accurate and context-aware summaries with 'refine'\n",
    "# It generates the summary of the first chunk; \n",
    "# Then, for each successive chunk, the summary is integrated with new info from the new chunk.\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82dcc7",
   "metadata": {},
   "source": [
    "**Working with multiple video URLs. Adding Transcripts to DeepLake.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08110595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    }
   ],
   "source": [
    "# Loading video files from multiple URLs\n",
    "import yt_dlp\n",
    "\n",
    "def download_mp4_from_youtube(urls, job_id):\n",
    "    # This will hold the titles and authors of each downloaded video\n",
    "    video_info = []\n",
    "\n",
    "    for i, url in enumerate(urls):\n",
    "        # Set the options for the download\n",
    "        file_temp = f'./data/{job_id}_{i}.mp4'\n",
    "        ydl_opts = {\n",
    "            'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "            'outtmpl': file_temp,\n",
    "            'quiet': True,\n",
    "        }\n",
    "\n",
    "        # Download the video file\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            result = ydl.extract_info(url, download=True)\n",
    "            title = result.get('title', \"\")\n",
    "            author = result.get('uploader', \"\")\n",
    "\n",
    "        # Add the title and author to our list\n",
    "        video_info.append((file_temp, title, author))\n",
    "\n",
    "    return video_info\n",
    "\n",
    "\n",
    "urls=[\"https://www.youtube.com/watch?v=mBjPyte2ZZo&t=78s\",\n",
    "    \"https://www.youtube.com/watch?v=cjs7QKJNVYM\",]\n",
    "\n",
    "vides_details = download_mp4_from_youtube(urls, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29329361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing ./data/1_0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iryna/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing ./data/1_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iryna/Documents/projects/langchain_snippets/langenv/lib/python3.10/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# Load the model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# iterate through each video and transcribe\n",
    "results = []\n",
    "for video in vides_details:\n",
    "    print(f\"Transcribing {video[0]}\")\n",
    "    result = model.transcribe(video[0])\n",
    "    results.append( result['text'] )\n",
    "    # print(f\"Transcription for {video[0]}:\\n{result['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d279cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('output/mult_text.txt', 'w') as file:\n",
    "    for r in results:\n",
    "        file.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "509e3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the texts from the file and split the text to chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the texts\n",
    "with open('output/mult_text.txt') as f:\n",
    "    text = f.read()\n",
    "texts = text_splitter.split_text(text)\n",
    "\n",
    "# Split \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    "    )\n",
    "texts = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23c7dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack all the chunks into a Documents\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=t) for t in texts[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f1bd741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://iryna/youtube_summarizer already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://iryna/youtube_summarizer', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['b14c8faa-3b0b-11ee-9ddc-12ee7aa5dbdc',\n",
       " 'b14c9248-3b0b-11ee-9ddc-12ee7aa5dbdc',\n",
       " 'b14c92d4-3b0b-11ee-9ddc-12ee7aa5dbdc',\n",
       " 'b14c9338-3b0b-11ee-9ddc-12ee7aa5dbdc']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a DeepLake database with embedded documents\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "my_activeloop_org_id = \"iryna\"\n",
    "my_activeloop_dataset_name = \"youtube_summarizer\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7842a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a retriever object\n",
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['k'] = 4 #search for k the most relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae47793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constract prompt template with the QA chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of transcripts from a video to answer the question in bullet points and summarized. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Summarized answer in bullter points:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "701dd131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "• Google uses self-supervised learning to train AI systems.\n",
      "• This involves taking a piece of text, removing some of the words, and replacing them with black markers.\n",
      "• The system then learns good representations of text that can be used for downstream tasks, such as translation or hitchbitch detection.\n",
      "• This technique is used in contact moderation systems on Google, YouTube, and other applications.\n",
      "• Large language models are partially based on this technique, which is used to predict the next word in a text.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever,\n",
    "                                 chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "\n",
    "print(qa.run(\"Summarize the mentions of google according to their AI program\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c293cea",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"voice_assistant\">\n",
    "    \n",
    "## 3. Creating Voice Assistant\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de079a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ce6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8b82590",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"code_understanding\">\n",
    "    \n",
    "## 4. Code Comprehension - Twitter Algorithm\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd33818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b594a3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "514dd985",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"recommendation\">\n",
    "    \n",
    "## 5. Recommendation Engine for Songs\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2cb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a077b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "835ea31b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"critique\">\n",
    "    \n",
    "## 6. Self-Critique Chain\n",
    "    \n",
    "</a>\n",
    "\n",
    "Self-critique chain acts as a mechanism to ensure model responses are appropriate in a production environment. By iterating over the model's output and checking against predefined expectations, the self-critique chain prompts the model to correct itself when necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a034b80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52e78152",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"resources\">\n",
    "    \n",
    "## 7. Additional Resources\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a489c1",
   "metadata": {},
   "source": [
    "- [Langchain documentation on Chains](https://python.langchain.com/docs/modules/chains/)\n",
    "- [Textwrap Package](https://docs.python.org/3/library/textwrap.html)\n",
    "- [Introducing Whisper](https://openai.com/research/whisper)\n",
    "- [Deep Lake Vector Store in LangChain](https://docs.activeloop.ai/tutorials/vector-store/deep-lake-vector-store-in-langchain)\n",
    "- [Voice Assitant - ‘JarvisBase’ repository on GitHub](https://github.com/peterw/JarvisBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa8ed92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
